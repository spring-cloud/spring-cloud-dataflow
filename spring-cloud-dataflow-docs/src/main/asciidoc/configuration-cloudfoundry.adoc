[[configuration-cloudfoundry]]
== Configuration - Cloud Foundry

This section describes how to configure Spring Cloud Data Flow server's features, such as security and which relational database to use.
It also describes how to configure Spring Cloud Data Flow shell's features.

[[configuration-cloudfoundry-enable-disable-specific-features]]
=== Feature Toggles

Data Flow server offers a specific set of features that you can enable or disable when launching. These features include all the lifecycle operations and REST endpoints (server, client implementations including Shell and the UI) for:

* Streams
* Tasks

You can enable or disable these features by setting the following boolean properties when you launch the Data Flow server:

* `spring.cloud.dataflow.features.streams-enabled`
* `spring.cloud.dataflow.features.tasks-enabled`

By default, all features are enabled.

The REST endpoint (`/features`) provides information on the enabled and disabled features.

[[configuration-cloudfoundry-deployer]]
=== Deployer Properties
You can use the following configuration properties of the Data Flow server's https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry[Cloud Foundry deployer] to customize how applications are deployed.
When deploying with the Data Flow shell, you can use the syntax `deployer.<appName>.cloudfoundry.<deployerPropertyName>`. See below for an example shell usage.
These properties are also used when configuring the <<configuration-cloudfoundry-tasks,Cloud Foundry Task platforms>> in the Data Flow server and and Kubernetes platforms in Skipper for deploying Streams.


[width="100%",frame="topbot",options="header"]
|===
|Deployer Property Name | Description | Default Value

|services
|The names of services to bind to the deployed application.
|<none>

|host
|The host name to use as part of the route.
|hostname derived by Cloud Foundry

|domain
|The domain to use when mapping routes for the application.
|<none>

|routes
|The list of routes that the application should be bound to.  Mutually exclusive with host and domain.
|<none>

|buildpack
|The buildpack to use for deploying the application. Deprecated use buildpacks.
|https://github.com/cloudfoundry/java-buildpack.git#v4.29.1

|buildpacks
|The list of buildpacks to use for deploying the application.
|https://github.com/cloudfoundry/java-buildpack.git#v4.29.1

|memory
|The amount of memory to allocate. Default unit is mebibytes, 'M' and 'G" suffixes supported
|1024m

|disk
|The amount of disk space to allocate. Default unit is mebibytes, 'M' and 'G" suffixes supported.
|1024m

|healthCheck
|The type of health check to perform on deployed application.  Values can be HTTP, NONE, PROCESS, and PORT
|PORT

|healthCheckHttpEndpoint
|The path that the http health check will use,
|/health

|healthCheckTimeout
|The timeout value for health checks in seconds.
|120

|instances
|The number of instances to run.
|1

|enableRandomAppNamePrefix
|Flag to enable prefixing the app name with a random prefix.
|true

|apiTimeout
|Timeout for blocking API calls, in seconds.
|360

|statusTimeout
|Timeout for status API operations in milliseconds
|5000

|useSpringApplicationJson
|Flag to indicate whether application properties are fed into `SPRING_APPLICATION_JSON` or as separate environment variables.
|true

|stagingTimeout
|Timeout allocated for staging the application.
|15 minutes

|startupTimeout
|Timeout allocated for starting the application.
|5 minutes

|appNamePrefix
|String to use as prefix for name of deployed application
|The Spring Boot property `spring.application.name` of the application that is using the deployer library.

|deleteRoutes
|Whether to also delete routes when un-deploying an application.
|true

|javaOpts
|The Java Options to pass to the JVM, e.g -Dtest=foo
|<none>

|pushTasksEnabled
|Whether to push task applications or assume that the application already exists when launched.
|true

|autoDeleteMavenArtifacts
|Whether to automatically delete Maven artifacts from the local repository when deployed.
|true

|env.<key>
|Defines a top level environment variable. This is useful for customizing https://github.com/cloudfoundry/java-buildpack#configuration-and-extension[Java build pack configuration] which must be included as top level environment variables in the application manifest, as the Java build pack does not recognize `SPRING_APPLICATION_JSON`.

|The deployer determines if the app has https://github.com/pivotal-cf/java-cfenv[Java CfEnv] in its classpath. If so, it applies the required https://github.com/pivotal-cf/java-cfenv#pushing-your-application-to-cloud-foundry[configuration].

|===

Here are some examples using the Cloud Foundry deployment properties:

* You can set the buildpack that is used to deploy each application. For example, to use the Java offline buildback,
set the following environment variable:


[source,bash,subs=attributes]
----
cf set-env dataflow-server SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_BUILDPACKS java_buildpack_offline
----

* Setting `buildpack` is now deprecated in favour of `buildpacks` which allows you to pass on more than one if needed. More about this can be found from https://docs.cloudfoundry.org/buildpacks/understand-buildpacks.html[How Buildpacks Work].

* You can customize the health check mechanism used by Cloud Foundry to assert whether apps are running by using the `SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_HEALTH_CHECK` environment variable. The current supported options
are `http` (the default), `port`, and `none`.

You can also set environment variables that specify the HTTP-based health check endpoint and timeout: `SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_HEALTH_CHECK_ENDPOINT` and `SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_HEALTH_CHECK_TIMEOUT`, respectively. These default to `/health` (the Spring Boot default location) and `120` seconds.

* You can also specify deployment properties by using the DSL. For instance, if you want to set the allocated memory for the `http` application to 512m and also bind a mysql service to the `jdbc` application, you can run the following commands:


[source,bash,subs=attributes]
----
dataflow:> stream create --name mysqlstream --definition "http | jdbc --tableName=names --columns=name"
dataflow:> stream deploy --name mysqlstream --properties "deployer.http.memory=512, deployer.jdbc.cloudfoundry.services=mysql"
----


[NOTE]
=====
You can configure these settings separately for stream and task apps. To alter settings for tasks,
substitute `TASK` for `STREAM` in the property name, as the following example shows:


[source,bash,subs=attributes]
----
cf set-env dataflow-server SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_MEMORY 512
----

=====

[[configuration-cloudfoundry-tasks]]
=== Tasks
The Data Flow server is responsible for deploying Tasks.
Tasks that are launched by Data Flow write their state to the same database that is used by the Data Flow server.
For Tasks which are Spring Batch Jobs, the job and step execution data is also stored in this database.
As with Skipper, Tasks can be launched to multiple platforms.
When Data Flow is running on Cloud Foundry, a Task platfom must be defined.
To configure new platform accounts that target Cloud Foundry, provide an entry under the `spring.cloud.dataflow.task.platform.cloudfoundry` section in your `application.yaml` file for via another Spring Boot supported mechanism.
In the following example, two Cloud Foundry platform accounts named `dev` and  `qa` are created.
The keys such as `memory` and `disk` are <<configuration-cloudfoundry-deployer, Cloud Foundry Deployer Properties>>.


[source,yaml]
----
spring:
  cloud:
    dataflow:
      task:
        platform:
          cloudfoundry:
            accounts:
              dev:
                connection:
                  url: https://api.run.pivotal.io
                  org: myOrg
                  space: mySpace
                  domain: cfapps.io
                  username: user@example.com
                  password: drowssap
                  skipSslValidation: false
                deployment:
                  memory: 512m
                  disk: 2048m
                  instances: 4
                  services: rabbit,mysql
                  appNamePrefix: dev1
              qa:
                connection:
                  url: https://api.run.pivotal.io
                  org: myOrgQA
                  space: mySpaceQA
                  domain: cfapps.io
                  username: user@example.com
                  password: drowssap
                  skipSslValidation: true
                deployment:
                  memory: 756m
                  disk: 724m
                  instances: 2
                  services: rabbitQA,mysqlQA
                  appNamePrefix: qa1
----

TIP: By defining one platform as `default` allows you to skip using `platformName` where its use would otherwise be required.

When launching a task, pass the value of the platform account name using the task launch option `--platformName`  If you do not pass a value for `platformName`, the value `default` will be used.

NOTE: When deploying a task to multiple platforms, the configuration of the task needs to connect to the same database as the Data Flow Server.

You can configure the Data Flow server that is on Cloud Foundry to deploy tasks to Cloud Foundry or Kubernetes.  See the section on <<configuration-kubernetes-tasks,Kubernetes Task Platform Configuration>> for more information.

Detailed examples for launching and scheduling tasks across multiple platforms, are available in this section https://dataflow.spring.io/docs/recipes/multi-platform-deployment/[Multiple Platform Support for Tasks] on http://dataflow.spring.io.

[[configuration-app-names-cloud-foundry]]
=== Application Names and Prefixes

To help avoid clashes with routes across spaces in Cloud Foundry, a naming strategy that provides a random prefix to a
deployed application is available and is enabled by default. You can override the https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry#application-name-settings-and-deployments[default configurations]
and set the respective properties by using `cf set-env` commands.

For instance, if you want to disable the randomization, you can override it by using the following command:


[source,bash,subs=attributes]
----
cf set-env dataflow-server SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_ENABLE_RANDOM_APP_NAME_PREFIX false
----


[[configuration-cloudfoundry-custom-routes]]
=== Custom Routes

As an alternative to a random name or to get even more control over the hostname used by the deployed apps, you can use
custom deployment properties, as the following example shows:


[source,bash,subs=attributes]
----
dataflow:>stream create foo --definition "http | log"

sdataflow:>stream deploy foo --properties "deployer.http.cloudfoundry.domain=mydomain.com,
                                          deployer.http.cloudfoundry.host=myhost,
                                          deployer.http.cloudfoundry.route-path=my-path"
----


The preceding example binds the `http` app to the `https://myhost.mydomain.com/my-path` URL. Note that this
example shows *all* of the available customization options. In practice, you can use only one or two out of the three.

[[configuration-cloudfoundry-docker-apps]]
=== Docker Applications

Starting with version 1.2, it is possible to register and deploy Docker based apps as part of streams and tasks by using
Data Flow for Cloud Foundry.

If you use Spring Boot and RabbitMQ-based Docker images, you can provide a common deployment property
to facilitate binding the apps to the RabbitMQ service. Assuming your RabbitMQ service is named `rabbit`, you can provide the following:


[source,bash,subs=attributes]
----
cf set-env dataflow-server SPRING_APPLICATION_JSON '{"spring.cloud.dataflow.applicationProperties.stream.spring.rabbitmq.addresses": "${vcap.services.rabbit.credentials.protocols.amqp.uris}"}'
----


For Spring Cloud Task apps, you can use something similar to the following, if you use a database service instance named `mysql`:


[source,bash,subs=attributes]
----
cf set-env SPRING_DATASOURCE_URL '${vcap.services.mysql.credentials.jdbcUrl}'
cf set-env SPRING_DATASOURCE_USERNAME '${vcap.services.mysql.credentials.username}'
cf set-env SPRING_DATASOURCE_PASSWORD '${vcap.services.mysql.credentials.password}'
cf set-env SPRING_DATASOURCE_DRIVER_CLASS_NAME 'org.mariadb.jdbc.Driver'
----


For non-Java or non-Boot applications, your Docker app must parse the `VCAP_SERVICES` variable in order to bind to any available services.

[NOTE]
.Passing application properties
=====
When using non-Boot applications, chances are that you want to pass the application properties by using traditional
environment variables, as opposed to using the special `SPRING_APPLICATION_JSON` variable. To do so, set the
following variables for streams and tasks, respectively:


[source, properties]
----
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_USE_SPRING_APPLICATION_JSON=false
----

=====

[[configuration-cloudfoundry-service-binding-at-application-level]]
=== Application-level Service Bindings

When deploying streams in Cloud Foundry, you can take advantage of application-specific service bindings, so not all
services are globally configured for all the apps orchestrated by Spring Cloud Data Flow.

For instance, if you want to provide a `mysql` service binding only for the `jdbc` application in the following stream
definition, you can pass the service binding as a deployment property:


[source,bash,subs=attributes]
----
dataflow:>stream create --name httptojdbc --definition "http | jdbc"
dataflow:>stream deploy --name httptojdbc --properties "deployer.jdbc.cloudfoundry.services=mysqlService"
----


where `mysqlService` is the name of the service specifically bound only to the `jdbc` application and the `http`
application does not get the binding by this method.


If you have more than one service to bind, they can be passed as comma-separated items
(for example: `deployer.jdbc.cloudfoundry.services=mysqlService,someService`).

[[configure-service-binding-parameters]]
=== Configuring Service binding parameters

The CloudFoundry API supports providing configuration parameters when binding a service instance. Some service brokers require or
recommend binding configuration.
For example, binding the https://docs.pivotal.io/partners/gcp-sb/using.html[Google Cloud Platform service] using the CF CLI looks something like:

[source]
----
cf bind-service my-app my-google-bigquery-example -c '{"role":"bigquery.user"}'
----

Likewise the https://docs.cloudfoundry.org/devguide/services/using-vol-services.html[NFS Volume Service] supports binding configuration such as:

[source]
----
cf bind-service my-app nfs_service_instance -c '{"uid":"1000","gid":"1000","mount":"/var/volume1","readonly":true}'
----

Starting with version 2.0, Data Flow for Cloud Foundry allows you to provide binding configuration parameters may be provided in the app level or server level `cloudfoundry.services` deployment property. For example, to bind to the nfs service, as above :

[source]
----
dataflow:> stream deploy --name mystream --properties "deployer.<app>.cloudfoundry.services='nfs_service_instance uid:1000,gid:1000,mount:/var/volume1,readonly:true'"
----

The format is intended to be compatible with the Data Flow DSL parser.
Generally, the `cloudfoundry.services` deployment property accepts a comma delimited value.
Since a comma is also used to separate configuration parameters, and to avoid white space issues, any item including configuration parameters must be enclosed in singe quotes. Valid values incude things like:

[source]
----
rabbitmq,'nfs_service_instance uid:1000,gid:1000,mount:/var/volume1,readonly:true',mysql,'my-google-bigquery-example role:bigquery.user'
----

[TIP]
Spaces are permitted within single quotes and  `=` may be used instead of `:` to delimit key-value pairs.


[[configuration-cloudfoundry-ups]]
=== User-provided Services
In addition to marketplace services, Cloud Foundry supports
https://docs.cloudfoundry.org/devguide/services/user-provided.html[User-provided Services] (UPS). Throughout this reference manual,
regular services have been mentioned, but there is nothing precluding the use of User-provided Services as well, whether for use as the
messaging middleware (for example, if you want to use an external Apache Kafka installation) or for use by some
of the stream applications (for example, an Oracle Database).

Now we review an example of extracting and supplying the connection credentials from a UPS.

The following example shows a sample UPS setup for Apache Kafka:


[source,bash]
----
cf create-user-provided-service kafkacups -p '{”brokers":"HOST:PORT","zkNodes":"HOST:PORT"}'
----


The UPS credentials are wrapped within `VCAP_SERVICES`, and they can be supplied directly in the stream definition, as
the following example shows.


[source,bash,subs=attributes]
----
stream create fooz --definition "time | log"
stream deploy fooz --properties "app.time.spring.cloud.stream.kafka.binder.brokers=${vcap.services.kafkacups.credentials.brokers},app.time.spring.cloud.stream.kafka.binder.zkNodes=${vcap.services.kafkacups.credentials.zkNodes},app.log.spring.cloud.stream.kafka.binder.brokers=${vcap.services.kafkacups.credentials.brokers},app.log.spring.cloud.stream.kafka.binder.zkNodes=${vcap.services.kafkacups.credentials.zkNodes}"
----


[[configuration-cloudfoundry-db-connection-pool]]
=== Database Connection Pool
As of Data Flow 2.0, the Spring Cloud Connector library is no longer used to create the DataSource.
The library  https://github.com/pivotal-cf/java-cfenv[java-cfenv] is now used which allows you to set https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#boot-features-connect-to-production-database[Spring Boot properties] to configure the connection pool.

[[configuration-cloudfoundry-maximum-disk-quota-configuration]]
=== Maximum Disk Quota
By default, every application in Cloud Foundry starts with 1G disk quota and this can be adjusted to a default maximum of
2G. The default maximum can also be overridden up to 10G by using Pivotal Cloud Foundry's (PCF) Ops Manager GUI.

This configuration is relevant for Spring Cloud Data Flow because every task deployment is composed of applications
(typically Spring Boot uber-jar's), and those applications are resolved from a remote maven repository. After resolution,
the application artifacts are downloaded to the local Maven Repository for caching and reuse. With this happening in the background,
the default disk quota (1G) can fill up rapidly, especially when we experiment with streams that
are made up of unique applications. In order to overcome this disk limitation and depending
on your scaling requirements, you may want to change the default maximum from 2G to 10G. Let's review the
steps to change the default maximum disk quota allocation.

==== PCF's Operations Manager

From PCF's Ops Manager, select the "`Pivotal Elastic Runtime`" tile and navigate to the "`Application Developer Controls`" tab.
Change the "`Maximum Disk Quota per App (MB)`" setting from 2048 (2G) to 10240 (10G). Save the disk quota update and click
"`Apply Changes`" to complete the configuration override.

[[configuration-cloudfoundry-scaling]]
=== Scale Application

Once the disk quota change has been successfully applied and assuming you have a xref:running-on-cloudfoundry[running application],
you can scale the application with a new `disk_limit` through the CF CLI, as the following example shows:


[source,bash]
----
→ cf scale dataflow-server -k 10GB

Scaling app dataflow-server in org ORG / space SPACE as user...
OK

....
....
....
....

     state     since                    cpu      memory           disk           details
#0   running   2016-10-31 03:07:23 PM   1.8%     497.9M of 1.1G   193.9M of 10G
----


You can then list the applications and see the new maximum disk space, as the following example shows:


[source,bash]
----
→ cf apps
Getting apps in org ORG / space SPACE as user...
OK

name              requested state   instances   memory   disk   urls
dataflow-server   started           1/1         1.1G     10G    dataflow-server.apps.io
----


[[configuration-cloudfoundry-managing-disk-utilization]]
=== Managing Disk Use

Even when configuring the Data Flow server to use 10G of space, there is the possibility of exhausting
the available space on the local disk. To prevent this, `jar` artifacts downloaded from external sources, i.e., apps registered as `http` or `maven` resources, are automatically deleted whenever the application is deployed, whether or not the deployment request succeeds.
This behavior is optimal for production environments in which container runtime stability is more critical than I/O latency incurred during deployment.
In development environments deployment happens more frequently. Additionally, the `jar` artifact (or a lighter `metadata` jar) contains metadata describing application configuration properties
which is used by various operations related to application configuration, more frequently performed during pre-production activities.
To provide a more responsive interactive developer experience at the expense of more disk usage in pre-production environments, you can set the CloudFoundry deployer property `autoDeleteMavenArtifacts` to `false`.

If you deploy the Data Flow server by using the default `port` health check type, you must explicitly monitor the disk space on the server in order to avoid running out space.
If you deploy the server by using the `http` health check type (see the next example), the Data Flow server is restarted if there is low disk space.
This is due to Spring Boot's link:https://github.com/spring-projects/spring-boot/blob/v1.5.14.RELEASE/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/health/DiskSpaceHealthIndicator.java[Disk Space Health Indicator].
You can link:https://docs.spring.io/spring-boot/docs/1.5.14.RELEASE/reference/htmlsingle/#common-application-properties[configure] the settings of the Disk Space Health Indicator by using the properties that have the `management.health.diskspace` prefix.

For version 1.7, we are investigating the use of link:https://docs.cloudfoundry.org/devguide/services/using-vol-services.html[Volume Services] for the Data Flow server to store `.jar` artifacts before pushing them to Cloud Foundry.

The following example shows how to deploy the `http` health check type to an endpoint called `/management/health`:


[source]
----
---
  ...
  health-check-type: http
  health-check-http-endpoint: /management/health
----

[[configuration-cloudfoundry-app-resolution-options]]
=== Application Resolution Alternatives

Though we recommend using a Maven Artifactory for application <<spring-cloud-dataflow-register-stream-apps>>,
there might be situations where one of the following alternative approaches would make sense.

* We have custom-built and maintain a link:https://github.com/spring-cloud-stream-app-starters/scdf-app-tool[SCDF APP Tool]
that can run as a regular Spring Boot application in Cloud Foundry, but it will in turn host and serve the application
JARs for SCDF at runtime.

* With the help of Spring Boot, we can serve link:https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-developing-web-applications.html#boot-features-spring-mvc-static-content[static content]
in Cloud Foundry. A simple Spring Boot application can bundle all the required stream and task applications. By having it
run on Cloud Foundry, the static application can then serve the über-jar's. From the shell, you can, for example, register the
application with the name `http-source.jar` by using `--uri=http://<Route-To-StaticApp>/http-source.jar`.

* The über-jar's can be hosted on any external server that's reachable over HTTP. They can be resolved from raw GitHub URIs
as well. From the shell, you can, for example, register the app with the name `http-source.jar` by using `--uri=http://<Raw_GitHub_URI>/http-source.jar`.

* link:https://docs.cloudfoundry.org/buildpacks/staticfile/index.html[Static Buildpack] support in Cloud Foundry is another
option. A similar HTTP resolution works on this model, too.

* link:https://docs.cloudfoundry.org/devguide/services/using-vol-services.html[Volume Services] is another great option.
The required über-jars can be hosted in an external file system. With the help of volume-services, you can, for
example, register the application with the name `http-source.jar` by using `--uri=file://<Path-To-FileSystem>/http-source.jar`.

[[configuration-cloudfoundry-security]]
=== Security

By default, the Data Flow server is unsecured and runs on an unencrypted HTTP connection. You can secure your REST endpoints
(as well as the Data Flow Dashboard) by enabling HTTPS and requiring clients to authenticate.
For more details about securing the
REST endpoints and configuring to authenticate against an OAUTH backend (UAA and SSO running on Cloud Foundry),
see the security section from the core <<configuration-local-security>>. You can configure the security details in `dataflow-server.yml` or pass them as environment variables through `cf set-env` commands.

[[configuration-cloudfoundry-authentication]]
==== Authentication

Spring Cloud Data Flow can either integrate with Pivotal Single Sign-On Service
(for example, on PWS) or Cloud Foundry User Account and Authentication (UAA) Server.

[[configuration-cloudfoundry-security-sso]]
===== Pivotal Single Sign-On Service

When deploying Spring Cloud Data Flow to Cloud Foundry, you can bind the
application to the Pivotal Single Sign-On Service. By doing so, Spring Cloud
Data Flow takes advantage of the
https://github.com/pivotal-cf/java-cfenv[Java CFEnv],
which provides Cloud Foundry-specific auto-configuration support for OAuth 2.0.

To do so, bind the Pivotal Single Sign-On Service to your Data Flow Server application and
provide the following properties:

[source,yaml]
----
SPRING_CLOUD_DATAFLOW_SECURITY_CFUSEUAA: false                                                 # <1>
SECURITY_OAUTH2_CLIENT_CLIENTID: "${security.oauth2.client.clientId}"
SECURITY_OAUTH2_CLIENT_CLIENTSECRET: "${security.oauth2.client.clientSecret}"
SECURITY_OAUTH2_CLIENT_ACCESSTOKENURI: "${security.oauth2.client.accessTokenUri}"
SECURITY_OAUTH2_CLIENT_USERAUTHORIZATIONURI: "${security.oauth2.client.userAuthorizationUri}"
SECURITY_OAUTH2_RESOURCE_USERINFOURI: "${security.oauth2.resource.userInfoUri}"
----

<1> It is important that the property `spring.cloud.dataflow.security.cf-use-uaa` is set to `false`

Authorization is similarly supported for non-Cloud Foundry security scenarios.
See the security section from the core Data Flow <<configuration-local-security>>.

As the provisioning of roles can vary widely across environments, we by
default assign all Spring Cloud Data Flow roles to users.

You can customize this behavior by providing your own https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/autoconfigure/security/oauth2/resource/AuthoritiesExtractor.html[`AuthoritiesExtractor`].

The following example shows one possible approach to set the custom `AuthoritiesExtractor` on the `UserInfoTokenServices`:


[source,java]
----
public class MyUserInfoTokenServicesPostProcessor
	implements BeanPostProcessor {

	@Override
	public Object postProcessBeforeInitialization(Object bean, String beanName) {
		if (bean instanceof UserInfoTokenServices) {
			final UserInfoTokenServices userInfoTokenServices == (UserInfoTokenServices) bean;
			userInfoTokenServices.setAuthoritiesExtractor(ctx.getBean(AuthoritiesExtractor.class));
		}
		return bean;
	}

	@Override
	public Object postProcessAfterInitialization(Object bean, String beanName) {
		return bean;
	}
}
----


Then you can declare it in your configuration class as follows:


[source,java]
----
@Bean
public BeanPostProcessor myUserInfoTokenServicesPostProcessor() {
	BeanPostProcessor postProcessor == new MyUserInfoTokenServicesPostProcessor();
	return postProcessor;
}
----


[[configuration-cloudfoundry-security-uaa]]
===== Cloud Foundry UAA

The availability of Cloud Foundry User Account and Authentication (UAA) depends on the Cloud Foundry environment.
In order to provide UAA integration, you have to provide the necessary
OAuth2 configuration properties (for example, by setting the `SPRING_APPLICATION_JSON`
property).

The following JSON example shows how to create a security configuration:


[source,json]
----
{
  "security.oauth2.client.client-id": "scdf",
  "security.oauth2.client.client-secret": "scdf-secret",
  "security.oauth2.client.access-token-uri": "https://login.cf.myhost.com/oauth/token",
  "security.oauth2.client.user-authorization-uri": "https://login.cf.myhost.com/oauth/authorize",
  "security.oauth2.resource.user-info-uri": "https://login.cf.myhost.com/userinfo"
}
----


By default, the `spring.cloud.dataflow.security.cf-use-uaa`  property is set to `true`. This property activates a special
https://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/autoconfigure/security/oauth2/resource/AuthoritiesExtractor.html[`AuthoritiesExtractor`] called `CloudFoundryDataflowAuthoritiesExtractor`.

If you do not use CloudFoundry UAA, you should set `spring.cloud.dataflow.security.cf-use-uaa` to `false`.

Under the covers, this `AuthoritiesExtractor` calls out to the
https://apidocs.cloudfoundry.org/253/apps/retrieving_permissions_on_a_app.html[Cloud Foundry
Apps API] and ensure that users are in fact Space Developers.

If the authenticated user is verified as a Space Developer, all roles are assigned.

=== Configuration Reference

You must provide several pieces of configuration. These are Spring Boot `@ConfigurationProperties`, so you can set
them as environment variables or by any other means that Spring Boot supports. The following listing is in environment
variable format, as that is an easy way to get started configuring Boot applications in Cloud Foundry.
Note that in the future, you will be able to deploy tasks to multiple platforms, but for 2.0.0.M1 you can deploy only to a single platform and the name must be `default`.


[source,bash]
----
# Default values appear after the equal signs.
# Example values, typical for Pivotal Web Services, are included as comments.

# URL of the CF API (used when using cf login -a for example) - for example, https://api.run.pivotal.io
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_URL=

# The name of the organization that owns the space above - for example, youruser-org
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_ORG=

# The name of the space into which modules will be deployed - for example, development
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SPACE=

# The root domain to use when mapping routes - for example, cfapps.io
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_DOMAIN=

# The user name and password of the user to use to create applications
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_USERNAME=
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_PASSWORD

# The identity provider to be used when accessing the Cloud Foundry API (optional).
# The passed string has to be a URL-Encoded JSON Object, containing the field origin with value as origin_key of an identity provider - for example, {"origin":"uaa"}
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_LOGIN_HINT=

# Whether to allow self-signed certificates during SSL validation (you should NOT do so in production)
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SKIP_SSL_VALIDATION

# A comma-separated set of service instance names to bind to every deployed task application.
# Among other things, this should include an RDBMS service that is used
# for Spring Cloud Task execution reporting, such as my_postgres
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_SERVICES
spring.cloud.deployer.cloudfoundry.task.services=

# Timeout, in seconds, to use when doing blocking API calls to Cloud Foundry
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_API_TIMEOUT=

# Timeout, in milliseconds, to use when querying the Cloud Foundry API to compute app status
SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_STATUS_TIMEOUT
----


Note that you can set `spring.cloud.deployer.cloudfoundry.services`,
`spring.cloud.deployer.cloudfoundry.buildpacks`, or the Spring Cloud Deployer-standard
`spring.cloud.deployer.memory` and `spring.cloud.deployer.disk`
as part of an individual deployment request by using the `deployer.<app-name>` shortcut, as the following example shows:


[source,bash,subs=attributes]
----
stream create --name ticktock --definition "time | log"
stream deploy --name ticktock --properties "deployer.time.memory=2g"
----


The commands in the preceding example deploy the time source with 2048MB of memory, while the log sink uses the default 1024MB.

When you deploy a stream, you can also pass `JAVA_OPTS` as a deployment property, as the following example shows:


[source,bash]
----
stream deploy --name ticktock --properties "deployer.time.cloudfoundry.javaOpts=-Duser.timezone=America/New_York"
----


=== Debugging
If you want to get better insights into what is happening when your streams and tasks are being deployed, you may want
to turn on the following features:

* Reactor "`stacktraces`", showing which operators were involved before an error occurred. This feature is helpful, as the deployer
relies on project reactor and regular stacktraces may not always allow understanding the flow before an error happened.
Note that this comes with a performance penalty, so it is disabled by default.


[source,bash]
----
spring.cloud.dataflow.server.cloudfoundry.debugReactor == true
----

* Deployer and Cloud Foundry client library request and response logs. This feature allows seeing a detailed conversation between
the Data Flow server and the Cloud Foundry Cloud Controller.


[source,data]
----
logging.level.cloudfoundry-client == DEBUG
----


=== Spring Cloud Config Server
You can use Spring Cloud Config Server to centralize configuration properties for Spring Boot applications. Likewise,
both Spring Cloud Data Flow and the applications orchestrated by Spring Cloud Data Flow can be integrated with
a configuration server to use the same capabilities.

==== Stream, Task, and Spring Cloud Config Server
Similar to Spring Cloud Data Flow server, you can configure both the stream and task applications to resolve the centralized properties from the configuration server.
Setting the `spring.cloud.config.uri` property for the deployed applications is a common way to bind to the configuration server.
See the link:https://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_spring_cloud_config_client[Spring Cloud Config Client] reference guide for more information.
Since this property is likely to be used across all applications deployed by the Data Flow server, the Data Flow server's `spring.cloud.dataflow.applicationProperties.stream` property for stream applications and `spring.cloud.dataflow.applicationProperties.task` property for task applications can be used to pass the `uri` of the Config Server to each deployed stream or task application. See the section on <<spring-cloud-dataflow-global-properties>> for more information.

Note that, if you use applications from the link:https://cloud.spring.io/spring-cloud-stream-app-starters/[App Starters project], these applications already embed the `spring-cloud-services-starter-config-client` dependency.
If you build your application from scratch and want to add the client side support for config server, you can add a dependency reference to the config server client library. The following snippet shows a Maven example:


[source,xml]
----
...
<dependency>
  <groupId>io.pivotal.spring.cloud</groupId>
  <artifactId>spring-cloud-services-starter-config-client</artifactId>
  <version>CONFIG_CLIENT_VERSION</version>
</dependency>
...
----

where `CONFIG_CLIENT_VERSION` can be the latest release of the https://github.com/pivotal-cf/spring-cloud-services-connector/releases[Spring Cloud Config Server]
client for Pivotal Cloud Foundry.


NOTE: You may see a `WARN` logging message if the application that uses this library cannot connect to the configuration
server when the application starts and whenever the `/health` endpoint is accessed.
If you know that you are not using config server functionality, you can disable the client library by setting the
`SPRING_CLOUD_CONFIG_ENABLED` environment variable to `false`.

==== Sample Manifest Template

The following SCDF and Skipper `manifest.yml` templates includes the required environment variables for the Skipper and Spring Cloud Data Flow server and deployed applications and tasks to successfully run on Cloud Foundry and automatically resolve centralized properties from `my-config-server` at runtime:


[source,yml]
----
---
applications:
- name: data-flow-server
  host: data-flow-server
  memory: 2G
  disk_quota: 2G
  instances: 1
  path: {PATH TO SERVER UBER-JAR}
  env:
    SPRING_APPLICATION_NAME: data-flow-server
    MAVEN_REMOTE_REPOSITORIES_REPO1_URL: https://repo.spring.io/libs-snapshot
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_URL: https://api.sys.huron.cf-app.com
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_ORG: sabby20
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SPACE: sabby20
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_DOMAIN: apps.huron.cf-app.com
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_USERNAME: admin
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_PASSWORD: ***
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SKIP_SSL_VALIDATION: true
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_SERVICES: mysql
    SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI: https://<skipper-host-name>/api
services:
- mysql
- my-config-server

---
applications:
- name: skipper-server
  host: skipper-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  timeout: 180
  buildpack: java_buildpack
  path: <PATH TO THE DOWNLOADED SKIPPER SERVER UBER-JAR>
  env:
    SPRING_APPLICATION_NAME: skipper-server
    SPRING_CLOUD_SKIPPER_SERVER_ENABLE_LOCAL_PLATFORM: false
    SPRING_CLOUD_SKIPPER_SERVER_STRATEGIES_HEALTHCHECK_TIMEOUTINMILLIS: 300000
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_URL: https://api.local.pcfdev.io
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_ORG: pcfdev-org
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SPACE: pcfdev-space
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_DOMAIN: cfapps.io
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_USERNAME: admin
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_PASSWORD: admin
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SKIP_SSL_VALIDATION: false
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_DELETE_ROUTES: false
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_SERVICES: rabbit, my-config-server
services:
- mysql
  my-config-server

----

where `my-config-server` is the name of the Spring Cloud Config Service instance running on Cloud Foundry.


By binding the service to Spring Cloud Data Flow server, Spring Cloud Task and via Skipper to all the Spring Cloud Stream applications respectively, we can now resolve centralized properties backed by this service.

==== Self-signed SSL Certificate and Spring Cloud Config Server

Often, in a development environment, we may not have a valid certificate to enable SSL communication between clients and the backend services.
However, the configuration server for Pivotal Cloud Foundry uses HTTPS for all client-to-service communication, so we need to add a self-signed SSL certificate in environments with no valid certificates.

By using the same `manifest.yml` templates listed in the previous section for the server, we can provide the self-signed SSL certificate by setting `TRUST_CERTS: <API_ENDPOINT>`.

However, the deployed applications also require `TRUST_CERTS` as a flat environment variable (as opposed to being wrapped inside `SPRING_APPLICATION_JSON`), so we must instruct the server with yet another set of tokens (`SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_USE_SPRING_APPLICATION_JSON: false`) for tasks.
With this setup, the applications receive their application properties as regular environment variables.

The following listing shows the updated `manifest.yml` with the required changes. Both the Data Flow server and deployed applications
get their configuration from the `my-config-server` Cloud Config server (deployed as a Cloud Foundry service).


[source,yml,options="wrap"]
----
---
applications:
- name: test-server
  host: test-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  path: spring-cloud-dataflow-server-VERSION.jar
  env:
    SPRING_APPLICATION_NAME: test-server
    MAVEN_REMOTE_REPOSITORIES_REPO1_URL: https://repo.spring.io/libs-snapshot
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_URL: https://api.sys.huron.cf-app.com
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_ORG: sabby20
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SPACE: sabby20
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_DOMAIN: apps.huron.cf-app.com
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_USERNAME: admin
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_PASSWORD: ***
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SKIP_SSL_VALIDATION: true
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_SERVICES: mysql, config-server
    SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI: https://<skipper-host-name>/api
    TRUST_CERTS: <API_ENDPOINT> #this is for the server
    SPRING_CLOUD_DATAFLOW_APPLICATION_PROPERTIES_TASK_TRUST_CERTS: <API_ENDPOINT>   #this propagates to all tasks
services:
- mysql
- my-config-server #this is for the server
----


Also add the `my-config-server` service to the Skipper's manifest environment


[source,yml]
----
---
applications:
- name: skipper-server
  host: skipper-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  timeout: 180
  buildpack: java_buildpack
  path: <PATH TO THE DOWNLOADED SKIPPER SERVER UBER-JAR>
  env:
    SPRING_APPLICATION_NAME: skipper-server
    SPRING_CLOUD_SKIPPER_SERVER_ENABLE_LOCAL_PLATFORM: false
    SPRING_CLOUD_SKIPPER_SERVER_STRATEGIES_HEALTHCHECK_TIMEOUTINMILLIS: 300000
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_URL: <URL>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_ORG: <ORG>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SPACE: <SPACE>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_DOMAIN: <DOMAIN>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_USERNAME: <USER>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_PASSWORD: <PASSWORD>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_SERVICES: rabbit, my-config-server #this is so all stream applications bind to my-config-server
services:
- mysql
  my-config-server

----


[[configuration-cloudfoundry-scheduling]]
=== Configure Scheduling
This section discusses how to configure Spring Cloud Data Flow to connect to the https://www.cloudfoundry.org/the-foundry/scheduler/[PCF-Scheduler] as its agent to execute tasks.

[NOTE]
====
Before following these instructions, be sure to have an instance of the PCF-Scheduler service running in your Cloud Foundry space.
To create a PCF-Scheduler in your space (assuming it is in your Market Place) execute the following from the CF CLI: `cf create-service scheduler-for-pcf standard <name of service>`.
Name of a service is later used to bound running application in _PCF_.
====

For scheduling, you must add (or update) the following environment variables in your environment:

* Enable scheduling for Spring Cloud Data Flow by setting `spring.cloud.dataflow.features.schedules-enabled` to `true`.
* Bind the task deployer to your instance of PCF-Scheduler by adding the PCF-Scheduler service name to the `SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_SERVICES` environment variable.
* Establish the URL to the PCF-Scheduler by setting the `SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_SCHEDULER_SCHEDULER_URL` environment variable.

[NOTE]
====
After creating the preceding configurations, you must create any task definitions that need to be scheduled.
====

The following sample manifest shows both environment properties configured (assuming you have a PCF-Scheduler service available with the name `myscheduler`):


[source,yml]
----
---
applications:
- name: data-flow-server
  host: data-flow-server
  memory: 2G
  disk_quota: 2G
  instances: 1
  path: {PATH TO SERVER UBER-JAR}
  env:
    SPRING_APPLICATION_NAME: data-flow-server
    SPRING_CLOUD_SKIPPER_SERVER_ENABLE_LOCAL_PLATFORM: false
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_URL: <URL>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_ORG: <ORG>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_SPACE: <SPACE>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_DOMAIN: <DOMAIN>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_USERNAME: <USER>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_CONNECTION_PASSWORD: <PASSWORD>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_DEPLOYMENT_SERVICES: rabbit, myscheduler
    SPRING_CLOUD_DATAFLOW_FEATURES_SCHEDULES_ENABLED: true
    SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI: https://<skipper-host-name>/api
    SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_SCHEDULER_SCHEDULER_URL: https://scheduler.local.pcfdev.io
services:
- mysql
----

Where the `SPRING_CLOUD_DATAFLOW_TASK_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[default]_SCHEDULER_SCHEDULER_URL` has the following format: `scheduler.<Domain-Name>` (for
example, `https://scheduler.local.pcfdev.io`). Check the actual address from your _PCF_ environment.

NOTE: Detailed examples for launching and scheduling tasks across multiple platforms, are available in this section https://dataflow.spring.io/docs/recipes/multi-platform-deployment/[Multiple Platform Support for Tasks] on http://dataflow.spring.io.
