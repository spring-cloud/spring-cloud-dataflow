[[getting-started]]
= Getting started

[partintro]
--
If you're just getting started with Spring Cloud Data Flow, this is the section
for you! Here we answer the basic "`what?`", "`how?`" and "`why?`" questions. You'll
find a gentle introduction to Spring Cloud Data Flow along with installation instructions.
We'll then build our first Spring Cloud Data Flow application, discussing some core principles as
we go.
--

[[getting-started-system-requirements]]
== System Requirements

You need Java installed (Java 7 or better, we recommend Java 8), and to build, you need to have Maven installed as well.

You need to have an RDBMS for storing stream, task and app states in the database. The `local` Data Flow server by default uses embedded H2 database for this.

You also need to have link:https://redis.io[Redis] running if you are running any streams that involve analytics applications. Redis may also be required run the unit/integration tests.

For the deployed streams and tasks to communicate, either link:https://rabbitmq.com[RabbitMQ] or link:https://kafka.apache.org[Kafka] needs to be installed.  The local server registers sources, sink, processors and tasks the are published from the link:https://github.com/spring-cloud/spring-cloud-stream-app-starters[Spring Cloud Stream App Starters] and link:https://github.com/spring-cloud/spring-cloud-task-app-starters[Spring Cloud Task App Starters] repository.  By default the server registers these applications that use Kafka, but setting the property `binding` to `rabbit` will register a list of applications that use RabbitMQ as the message broker.

[[enable-disable-specific-features]]
== Controlling features with Data Flow server

Data Flow server offers specific set of features that can be enabled/disabled when launching. These features include all the lifecycle operations, REST endpoints (server, client implementations including Shell and the UI) for:

. Streams
. Tasks
. Analytics

One can enable, disable these features by setting the following boolean properties when launching the Data Flow server:

* `spring.cloud.dataflow.features.streams-enabled`
* `spring.cloud.dataflow.features.tasks-enabled`
* `spring.cloud.dataflow.features.analytics-enabled`

By default, all these features are enabled for `local` dataflow server.

The REST endpoint `/features` provides information on the features enabled/disabled.

[[getting-started-deploying-spring-cloud-dataflow]]
== Deploying Spring Cloud Data Flow

=== Deploying 'local'
. Download the Spring Cloud Data Flow Server and Shell apps:
+
[source,bash,subs=attributes]
----
wget https://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-server-local/{project-version}/spring-cloud-dataflow-server-local-{project-version}.jar

wget https://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-shell/{project-version}/spring-cloud-dataflow-shell-{project-version}.jar
----
+
. Launch the Data Flow Server
+
.. Since the Data Flow Server is a Spring Boot application, you can run it just by using `java -jar`.
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar
----
+
.. Running with Custom Maven Settings and/or Behind a Proxy
If you want to override specific maven configuration properties (remote repositories, etc.) and/or run the Data Flow Server behind a proxy,
you need to specify those properties as command line arguments when starting the Data Flow Server. For example:
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-local-{project-version}.jar --maven.localRepository=mylocal
--maven.remote-repositories.repo1.url=https://repo1
--maven.remote-repositories.repo1.auth.username=user1
--maven.remote-repositories.repo1.auth.password=pass1
--maven.remote-repositories.repo2.url=https://repo2 --maven.proxy.host=proxy1
--maven.proxy.port=9010 --maven.proxy.auth.username=proxyuser1
--maven.proxy.auth.password=proxypass1
----
+
By default, the protocol is set to `http`. You can omit the auth properties if the proxy doesn't need a username and password.
+
By default, the maven `localRepository` is set to `${user.home}/.m2/repository/`,
and `https://repo.spring.io/libs-snapshot` will be the only remote repository. Like in the above example, the remote
repositories can be specified along with their authentication (if needed). If the remote repositories are behind a proxy,
then the proxy properties can be specified as above.
+
If you want to pass these properties as environment properties, then you need to use `SPRING_APPLICATION_JSON` to set
these properties and pass `SPRING_APPLICATION_JSON` as environment variable as below:
+
[source,bash]
----
$ SPRING_APPLICATION_JSON='{ "maven": { "local-repository": null,
"remote-repositories": { "repo1": { "url": "https://repo1", "auth": { "username": "repo1user", "password": "repo1pass" } }, "repo2": { "url": "https://repo2" } },
"proxy": { "host": "proxyhost", "port": 9018, "auth": { "username": "proxyuser", "password": "proxypass" } } } }' java -jar spring-cloud-dataflow-server-local-{project-version}.jar
----
+
. Launch the shell:
+
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar
----
+
If the Data Flow Server and shell are not running on the same host, point the shell to the Data Flow server:
+
[source,bash]
----
server-unknown:>dataflow config server https://dataflow-server.cfapps.io
Successfully targeted https://dataflow-server.cfapps.io
dataflow:>
----
+
By default, the application registry will be empty. If you would like to register all out-of-the-box stream applications built with the Kafka binder in bulk, you can with the following command. For more details, review how to <<streams.adoc#spring-cloud-dataflow-register-apps, register applications>>.
+
[source,bash,subs=attributes]
----
$ dataflow:>app import --uri https://bit.ly/stream-applications-kafka-maven
----
+
. You can now use the shell commands to list available applications (source/processors/sink) and create streams. For example:
+
[source,bash]
----
dataflow:> stream create --name httptest --definition "http --server.port=9000 | log" --deploy
----
+
NOTE: You will need to wait a little while until the apps are actually deployed successfully
before posting data.  Look in the log file of the Data Flow server for the location of the log
files for the `http` and `log` applications.  Tail the log file for each application to verify
the application has started.
+
Now post some data
[source,bash]
----
dataflow:> http post --target http://localhost:9000 --data "hello world"
----
Look to see if `hello world` ended up in log files for the `log` application.

[TIP]
====
In case you encounter unexpected errors when executing shell commands, you can
retrieve more detailed error information by setting the exception logging level
to `WARNING` in `logback.xml`:

[source,xml]
----
<logger name="org.springframework.shell.core.JLineShellComponent.exceptions" level="WARNING"/>
----

====

[[configuring-rdbms]]
== RDBMS configuration

The JDBC drivers for *MySQL* (via MariaDB driver), *HSQLDB*, *PostgreSQL* along with embedded *H2* are available out of the box.
If you are using any other RDBMS, then the corresponding JDBC driver jar needs to be on the classpath of the server.

The RDBMS properties can be passed as command-line arguments to the Data Flow Server.

For instance,
If you are using *MySQL*:

[source,bash]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar \
    --spring.datasource.url=jdbc:mysql:<db-info> \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver &
----

For *PostgreSQL*:

[source,bash]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar \
    --spring.datasource.url=jdbc:postgresql:<db-info> \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=org.postgresql.Driver &
----

For *HSQLDB*:

[source,bash]
----
java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar \
    --spring.datasource.url=jdbc:hsqldb:<db-info> \
    --spring.datasource.username=SA \
    --spring.datasource.driver-class-name=org.hsqldb.jdbc.JDBCDriver &
----

[[getting-started-security]]
== Security

By default, the Data Flow server is unsecured and runs on an unencrypted HTTP connection.
You can secure your REST endpoints, as well as the Data Flow Dashboard by enabling HTTPS
and requiring clients to authenticate.

[[getting-started-security-enabling-https]]
=== Enabling HTTPS

By default, the dashboard, management, and health endpoints use HTTP as a transport.
You can switch to HTTPS easily, by adding a certificate to your configuration in
`application.yml`.

[source,yaml]
----
server:
  port: 8443                                         # <1>
  ssl:
    key-alias: yourKeyAlias                          # <2>
    key-store: path/to/keystore                      # <3>
    key-store-password: yourKeyStorePassword         # <4>
    key-password: yourKeyPassword                    # <5>
    trust-store: path/to/trust-store                 # <6>
    trust-store-password: yourTrustStorePassword     # <7>
----

<1> As the default port is `9393`, you may choose to change the port to a more common HTTPs-typical port.
<2> The alias (or name) under which the key is stored in the keystore.
<3> The path to the keystore file. Classpath resources may also be specified, by using the classpath prefix: `classpath:path/to/keystore`
<4> The password of the keystore.
<5> The password of the key.
<6> The path to the truststore file. Classpath resources may also be specified, by using the classpath prefix: `classpath:path/to/trust-store`
<7> The password of the trust store.

NOTE: If HTTPS is enabled, it will completely replace HTTP as the protocol over
which the REST endpoints and the Data Flow Dashboard interact. Plain HTTP requests
will fail - therefore, make sure that you configure your Shell accordingly.

==== Using Self-Signed Certificates

For testing purposes or during development it might be convenient to create self-signed certificates.
To get started, execute the following command to create a certificate:

[source,bash]
----
$ keytool -genkey -alias dataflow -keyalg RSA -keystore dataflow.keystore \
          -validity 3650 -storetype JKS \
          -dname "CN=localhost, OU=Spring, O=Pivotal, L=Kailua-Kona, ST=HI, C=US"  # <1>
          -keypass dataflow -storepass dataflow
----

<1> _CN_ is the only important parameter here. It should match the domain you are trying to access, e.g. `localhost`.

Then add the following to your `application.yml` file:

[source,yaml]
----
server:
  port: 8443
  ssl:
    enabled: true
    key-alias: dataflow
    key-store: "/your/path/to/dataflow.keystore"
    key-store-type: jks
    key-store-password: dataflow
    key-password: dataflow
----

This is all that's needed for the Data Flow Server. Once you start the server,
you should be able to access it via https://localhost:8443/[https://localhost:8443/]. As this is a self-signed
certificate, you will hit a warning in your browser, that you need to ignore.

This issue also is relevant for the Data Flow Shell. Therefore additional steps are
necessary to make the Shell work with self-signed certificates. First, we need to
export the previously created certificate from the keystore:

[source,bash]
----
$ keytool -export -alias dataflow -keystore dataflow.keystore -file dataflow_cert -storepass dataflow
----

Next, we need to create a truststore which the Shell will use:

[source,bash]
----
$ keytool -importcert -keystore dataflow.truststore -alias dataflow -storepass dataflow -file dataflow_cert -noprompt
----

Now, you are ready to launch the Data Flow Shell using the following JVM arguments:

[source,bash,subs=attributes]
----
$ java -Djavax.net.ssl.trustStorePassword=dataflow \
       -Djavax.net.ssl.trustStore=/path/to/dataflow.truststore \
       -Djavax.net.ssl.trustStoreType=jks \
       -jar spring-cloud-dataflow-shell-{project-version}.jar
----

[TIP]
====
In case you run into trouble establishing a connection via SSL, you can enable additional
logging by using and setting the `javax.net.debug` JVM argument to `ssl`.
====

Don't forget to target the Data Flow Server with:

[source,bash]
----
dataflow:> dataflow config server https://localhost:8443/
----

[[getting-started-security-enabling-authentication]]
=== Enabling Authentication

By default, the REST endpoints (administration, management and health), as well
as the Dashboard UI do not require authenticated access. However, authentication can
be provided via https://oauth.net/2/[OAuth 2.0], thus allowing you to also integrate Spring Cloud
Data Flow into Single Sign On (SSO) environments. The following 2 OAuth2 Grant Types will be used:

* _Authorization Code_ - Used for the GUI (Browser) integration. You will be redirected to your OAuth Service for authentication
* _Password_ - Used by the shell (And the REST integration), so you can login using username and password

The REST endpoints are secured via Basic Authentication but will use the Password
Grand Type under the covers to authenticate with your OAuth2 service.

NOTE: When authentication is set up, it is strongly recommended to enable HTTPS
as well, especially in production environments.

You can turn on authentication by adding the following to `application.yml` or via
environment variables:

[source,yaml]
----
security:
  basic:
    enabled: true                                                     # <1>
    realm: Spring Cloud Data Flow                                     # <2>
  oauth2:                                                             # <3>
    client:
      client-id: myclient
      client-secret: mysecret
      access-token-uri: http://127.0.0.1:9999/oauth/token
      user-authorization-uri: http://127.0.0.1:9999/oauth/authorize
    resource:
      user-info-uri: http://127.0.0.1:9999/me
----

<1> Must be set to `true` for security to be enabled.
<2> The realm for Basic authentication
<3> OAuth Configuration Section

NOTE: As of version 1.0 Spring Cloud Data Flow does not provide finer-grained authorization. Thus, once you are logged in, you have full access to all functionality.

You can verify that basic authentication is working properly using _curl_:

[source,bash]
----
$ curl -u myusername:mypassword http://localhost:9393/
----

As a result you should see a list of available REST endpoints.

=== Authentication using the Spring Cloud Data Flow Shell

If your OAuth2 provider supports the _Password_ Grant Type you can start the
_Data Flow Shell_ with:

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar \
  --dataflow.uri=http://localhost:9393 \
  --dataflow.username=my_username --dataflow.password=my_password
----

NOTE: Keep in mind that when authentication for Spring Cloud Data Flow is enabled,
the underlying OAuth2 provider *must* support the _Password_ OAuth2 Grant Type,
if you want to use the Shell.

From within the Data Flow Shell you can also provide credentials using:

[source,bash]
----
dataflow config server --uri http://localhost:9393 --username my_username --password my_password
----

Once successfully targeted, you should see the following output:

[source,bash]
----
dataflow:>dataflow config info
dataflow config info

╔═══════════╤═══════════════════════════════════════╗
║Credentials│[username='my_username, password=****']║
╠═══════════╪═══════════════════════════════════════╣
║Result     │                                       ║
║Target     │http://localhost:9393                  ║
╚═══════════╧═══════════════════════════════════════╝
----
=== Authentication Examples

==== Local OAuth2 Server

With https://projects.spring.io/spring-security-oauth/[Spring Security OAuth] you
can easily create your own OAuth2 Server with the following 2 simple annotations:

* @EnableResourceServer
* @EnableAuthorizationServer

A working example application can be found at:

https://github.com/ghillert/oauth-test-server/[https://github.com/ghillert/oauth-test-server/]

Simply clone the project, built and start it. Furthermore configure Spring Cloud
Data Flow with the respective _Client Id_ and _Client Secret_.

==== Authentication using GitHub

If you rather like to use an existing OAuth2 provider, here is an example for GitHub.
First you need to **Register a new application** under your GitHub account at:

https://github.com/settings/developers[https://github.com/settings/developers]

When running a default version of Spring Cloud Data Flow locally, your GitHub configuration
should look like the following:

.Register an OAuth Application for GitHub
image::{dataflow-asciidoc}/images/dataflow-security-github.png[Register an OAuth Application for GitHub , scaledwidth="100%"]

NOTE: For the _Authorization callback URL_ you will enter Spring Cloud Data Flow's Login URL, e.g. `http://localhost:9393/login`.

Configure Spring Cloud Data Flow with the GitHub relevant Client Id and Secret:

[source,yaml]
----
security:
  basic:
    enabled: true
  oauth2:
    client:
      client-id: your-github-client-id
      client-secret: your-github-client-secret
      access-token-uri: https://github.com/login/oauth/access_token
      user-authorization-uri: https://github.com/login/oauth/authorize
    resource:
      user-info-uri: https://api.github.com/user
----

IMPORTANT: GitHub does not support the OAuth2 password grant type. As such you cannot use the Spring Cloud Data Flow Shell in conjunction with GitHub.
