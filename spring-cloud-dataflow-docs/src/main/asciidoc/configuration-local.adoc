[[configuration-local]]
== Configuration - Local

[partintro]
--
This section covers how to configure Spring Cloud Data Flow Server's features, such as which relational database to use and security.
It also covers how to configure Spring Cloud Data Flow's shell features.
--

[[configuration-local-enable-disable-specific-features]]
=== Feature Toggles

Spring Cloud Data Flow Server offers specific set of features that can be enabled/disabled when launching. These features include all the lifecycle operations and REST endpoints (server and client implementations, including the shell and the UI) for:

* Streams (requires Skipper)
* Tasks
* Task Scheduler

One can enable and disable these features by setting the following boolean properties when launching the Data Flow server:

* `spring.cloud.dataflow.features.streams-enabled`
* `spring.cloud.dataflow.features.tasks-enabled`
* `spring.cloud.dataflow.features.schedules-enabled`

By default, stream (requires Skipper), and tasks are enabled and Task Scheduler is disabled by default.

The REST `/about` endpoint provides information on the features that have been enabled and disabled.

[[configuration-local-rdbms]]
=== Database

A relational database is used to store stream and task definitions as well as the state of executed tasks.
Spring Cloud Data Flow provides schemas for *H2*, *MySQL*, *Oracle*, *PostgreSQL*, *Db2*, and *SQL Server*. The schema is automatically created when the server starts.

By default, Spring Cloud Data Flow offers an embedded instance of the *H2* database. The *H2* database is good
for development purposes but is not recommended for production use.

NOTE: *H2* database is not supported as an external mode.

The JDBC drivers for *MySQL* (through the MariaDB driver), *PostgreSQL*, *SQL Server*, and embedded *H2* are available without additional configuration.
If you are using any other database, then you need to put the corresponding JDBC driver jar on the classpath of the server.

The database properties can be passed as environment variables or command-line arguments to the Data Flow Server.

==== MySQL

The following example shows how to define a MySQL database connection using MariaDB driver.

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-{project-version}.jar \
    --spring.datasource.url=jdbc:mysql://localhost:3306/mydb \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver
----

MySQL versions up to _5.7_ can be used with a MariaDB driver. Starting from version _8.0_ MySQL's own driver has to be used.

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-{project-version}.jar \
    --spring.datasource.url=jdbc:mysql://localhost:3306/mydb \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=com.mysql.jdbc.Driver
----

NOTE: Due to licensing restrictions we're unable to bundle MySQL driver. You need to add it to
      server's classpath yourself.

==== MariaDB

The following example shows how to define a MariaDB database connection with command Line arguments

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-{project-version}.jar \
    --spring.datasource.url=jdbc:mariadb://localhost:3306/mydb?useMysqlMetadata=true \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver
----

Starting with MariaDB v2.4.1 connector release, it is required to also add `useMysqlMetadata=true`
to the JDBC URL. This is a required workaround until when MySQL and MariaDB entirely switch as two
different databases.

MariaDB version _10.3_ introduced a support for real database sequences which is yet another breaking
change while toolings around these databases fully support MySQL and MariaDB as a separate database
types. Workaround is to use older hibernate dialect which doesn't try to use sequences.

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-{project-version}.jar \
    --spring.datasource.url=jdbc:mariadb://localhost:3306/mydb?useMysqlMetadata=true \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MariaDB102Dialect \
    --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver
----

==== PostgreSQL

The following example shows how to define a PostgreSQL database connection with command line arguments:

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-{project-version}.jar \
    --spring.datasource.url=jdbc:postgresql://localhost:5432/mydb \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=org.postgresql.Driver
----

==== SQL Server

The following example shows how to define a SQL Server database connection with command line arguments:

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-{project-version}.jar \
    --spring.datasource.url='jdbc:sqlserver://localhost:1433;databaseName=mydb' \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=com.microsoft.sqlserver.jdbc.SQLServerDriver
----

==== Db2

The following example shows how to define a Db2 database connection with command line arguments:

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-{project-version}.jar \
    --spring.datasource.url=jdbc:db2://localhost:50000/mydb \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=com.ibm.db2.jcc.DB2Driver
----

NOTE: Due to licensing restrictions we're unable to bundle Db2 driver. You need to add it to
      server's classpath yourself.

==== Oracle

The following example shows how to define a Oracle database connection with command line arguments:

[source,bash,subs=attributes]
----
java -jar spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-{project-version}.jar \
    --spring.datasource.url=jdbc:oracle:thin:@localhost:1521/MYDB \
    --spring.datasource.username=<user> \
    --spring.datasource.password=<password> \
    --spring.datasource.driver-class-name=oracle.jdbc.OracleDriver
----

NOTE: Due to licensing restrictions we're unable to bundle Oracle driver. You need to add it to
      server's classpath yourself.

==== Adding a Custom JDBC Driver
To add a custom driver for the database (for example, Oracle), you should rebuild the Data Flow Server and add the dependency to the Maven `pom.xml` file.
You need to modify the maven `pom.xml` of `spring-cloud-dataflow-server` module.
There are GA release tags in GitHub repository, so you can switch to desired GA tags to add the drivers on the production-ready codebase.

To add a custom JDBC driver dependency for the Spring Cloud Data Flow server:

. Select the tag that corresponds to the version of the server you want to rebuild and clone the github repository.
. Edit the spring-cloud-dataflow-server/pom.xml and, in the `dependencies` section, add the dependency for the database driver required.  In the following example , an Oracle driver has been chosen:

[source, xml]
----
<dependencies>
...
  <dependency>
    <groupId>com.oracle.jdbc</groupId>
    <artifactId>ojdbc8</artifactId>
    <version>12.2.0.1</version>
  </dependency>
...
</dependencies>
----

[start=3]
. Build the application as described in <<appendix-building.adoc#building, Building Spring Cloud Data Flow>>

You can also provide default values when rebuilding the server by adding the necessary properties to the dataflow-server.yml file,
as shown in the following example for PostgreSQL:

[source]
----
spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/mydb
    username: myuser
    password: mypass
    driver-class-name:org.postgresql.Driver
----

[start=4]
. Alternatively, you can build a custom Spring CLoud Data Flow server with your build files.
There are examples of a custom server builds in our https://github.com/spring-cloud/spring-cloud-dataflow-samples/tree/master/custom-dataflow-builds[samples repo] if there is a need to add a driver jars.

[[configuration-local-deployer]]
=== Deployer Properties
You can use the following configuration properties of the https://github.com/spring-cloud/spring-cloud-deployer-local[Local deployer] to customize how Streams and Tasks are deployed.
When deploying using the Data Flow shell, you can use the syntax `deployer.<appName>.local.<deployerPropertyName>`. See below for an example shell usage.
These properties are also used when configuring <<configuration-local-tasks,Local Task Platforms>> in the Data Flow server and local platforms in Skipper for deploying Streams.

[width="100%",frame="topbot",options="header"]
|===
|Deployer Property Name | Description | Default Value

|workingDirectoriesRoot
|Directory in which all created processes will run and create log files.
|java.io.tmpdir

|envVarsToInherit
|Array of regular expression patterns for environment variables that are passed to launched applications.
| <"TMP", "LANG", "LANGUAGE", "LC_.\*", "PATH", "SPRING_APPLICATION_JSON"> on windows and <"TMP", "LANG", "LANGUAGE", "LC_.*", "PATH"> on Unix

|deleteFilesOnExit
|Whether to delete created files and directories on JVM exit.
|true

|javaCmd
|Command to run java
|java

|shutdownTimeout
|Max number of seconds to wait for app shutdown.
|30

|javaOpts
|The Java Options to pass to the JVM, e.g -Dtest=foo
|<none>

|inheritLogging
|allow logging to be redirected to the output stream of the process that triggered child process.
|false

|debugPort
|Port for remote debugging
|<none>

|===

As an example, to set Java options for the time application in the `ticktock` stream, use the following stream deployment properties.
[source,bash]
----
dataflow:> stream create --name ticktock --definition "time --server.port=9000 | log"
dataflow:> stream deploy --name ticktock --properties "deployer.time.local.javaOpts=-Xmx2048m -Dtest=foo"
----

As a convenience, you can set the `deployer.memory` property to set the Java option `-Xmx`, as shown in the following example:

[source,bash]
----
dataflow:> stream deploy --name ticktock --properties "deployer.time.memory=2048m"
----

At deployment time, if you specify an `-Xmx` option in the `deployer.<app>.local.javaOpts` property in addition to a value of the `deployer.<app>.local.memory` option, the value in the `javaOpts` property has precedence.  Also, the `javaOpts` property set when deploying the application has precedence over the Data Flow Server's `spring.cloud.deployer.local.javaOpts` property.

[[configuration-local-logging]]
=== Logging

Spring Cloud Data Flow `local` server is automatically configured to use `RollingFileAppender` for logging.
The logging configuration is located on the classpath contained in a file named `logback-spring.xml`.

By default, the log file is configured to use:

```
<property name="LOG_FILE" value="${LOG_FILE:-${LOG_PATH:-${LOG_TEMP:-${java.io.tmpdir:-/tmp}}}/spring-cloud-dataflow-server}"/>

```

with the logback configuration for the `RollingPolicy`:

----

<appender name="FILE"
			  class="ch.qos.logback.core.rolling.RollingFileAppender">
		<file>${LOG_FILE}.log</file>
		<rollingPolicy
				class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
			<!-- daily rolling -->
			<fileNamePattern>${LOG_FILE}.${LOG_FILE_ROLLING_FILE_NAME_PATTERN:-%d{yyyy-MM-dd}}.%i.gz</fileNamePattern>
			<maxFileSize>${LOG_FILE_MAX_SIZE:-100MB}</maxFileSize>
			<maxHistory>${LOG_FILE_MAX_HISTORY:-30}</maxHistory>
			<totalSizeCap>${LOG_FILE_TOTAL_SIZE_CAP:-500MB}</totalSizeCap>
		</rollingPolicy>
		<encoder>
			<pattern>${FILE_LOG_PATTERN}</pattern>
		</encoder>
	</appender>

----

To check the `java.io.tmpdir` for the current Spring Cloud Data Flow Server `local` server,

```
jinfo <pid> | grep "java.io.tmpdir"

```

If you want to change or override any of the properties `LOG_FILE`, `LOG_PATH`, `LOG_TEMP`, `LOG_FILE_MAX_SIZE`, `LOG_FILE_MAX_HISTORY` and `LOG_FILE_TOTAL_SIZE_CAP`, please set them as system properties.

[[configuration-local-streams]]
=== Streams
Data Flow Server delegates to the Skipper server the management of the Stream's lifecycle.  Set the configuration property `spring.cloud.skipper.client.serverUri` to the location of Skipper, e.g.

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-server-{project-version}.jar --spring.cloud.skipper.client.serverUri=https://192.51.100.1:7577/api
----

The configuration of show streams are deployed and to which platforms, is done by configuration of `platform accounts` on the Skipper server.
See the documentation on https://docs.spring.io/spring-cloud-skipper/docs/current/reference/htmlsingle/#platforms[platforms] for more information.


[[configuration-local-tasks]]
=== Tasks
The Data Flow server is responsible for deploying Tasks.
Tasks that are launched by Data Flow write their state to the same database that is used by the Data Flow server.
For Tasks which are Spring Batch Jobs, the job and step execution data is also stored in this database.
As with streams launched by Skipper, Tasks can be launched to multiple platforms.
If no platform is defined, a platform named `default` is created using the default values of the class https://github.com/spring-cloud/spring-cloud-deployer-local/blob/master/spring-cloud-deployer-local/src/main/java/org/springframework/cloud/deployer/spi/local/LocalDeployerProperties.java[LocalDeployerProperties], which is summarized in the table <<configuration-local-deployer,Local Deployer Properties>>

To configure new platform accounts for the local platform, provide an entry under the `spring.cloud.dataflow.task.platform.local` section in your `application.yaml` file for via another Spring Boot supported mechanism.
In the following example, two local platform accounts named `localDev` and  `localDevDebug` are created.
The keys such as `shutdownTimeout` and `javaOpts` are local deployer properties.

[source,yaml]
----
spring:
  cloud:
    dataflow:
      task:
        platform:
          local:
            accounts:
              localDev:
                shutdownTimeout: 60
                javaOpts: "-Dtest=foo -Xmx1024m"
              localDevDebug:
                javaOpts: "-Xdebug -Xmx2048m"

----

TIP: By defining one platform as `default` allows you to skip using `platformName` where its use would otherwise be required.

When launching a task, pass the value of the platform account name using the task launch option `--platformName`  If you do not pass a value for `platformName`, the value `default` will be used.

NOTE: When deploying a task to multiple platforms, the configuration of the task needs to connect to the same database as the Data Flow Server.

You can configure the Data Flow server that is running locally to deploy tasks to Cloud Foundry or Kubernetes.  See the sections on <<configuration-cloudfoundry-tasks,Cloud Foundry Task Platform Configuration>> and <<configuration-kubernetes-tasks,Kubernetes Task Platform Configuration>> for more information.


[[configuration-local-security]]
=== Security

By default, the Data Flow server is unsecured and runs on an unencrypted HTTP connection.
You can secure your REST endpoints as well as the Data Flow Dashboard by enabling HTTPS
and requiring clients to authenticate using https://oauth.net/2/[OAuth 2.0].

[NOTE]
====
By default, the REST endpoints (administration, management, and health) as well as the Dashboard UI do not require authenticated access.
====

While you can theoretically choose any OAuth provider in conjunction with
Spring Cloud Data Flow, we recommend using the
https://github.com/cloudfoundry/uaa[CloudFoundry User Account and Authentication (UAA) Server].

Not only is the UAA OpenID certified and is used by Cloud Foundry but it can
also be used in local stand-alone deployment scenarios. Furthermore, the UAA not
only provides its own user store, but also provides comprehensive LDAP integration.

[[configuration-local-security-enabling-https]]
==== Enabling HTTPS

By default, the dashboard, management, and health endpoints use HTTP as a transport.
You can switch to HTTPS by adding a certificate to your configuration in
`application.yml`, as shown in the following example:

[source,yaml]
----
server:
  port: 8443                                         # <1>
  ssl:
    key-alias: yourKeyAlias                          # <2>
    key-store: path/to/keystore                      # <3>
    key-store-password: yourKeyStorePassword         # <4>
    key-password: yourKeyPassword                    # <5>
    trust-store: path/to/trust-store                 # <6>
    trust-store-password: yourTrustStorePassword     # <7>
----

<1> As the default port is `9393`, you may choose to change the port to a more common HTTPs-typical port.
<2> The alias (or name) under which the key is stored in the keystore.
<3> The path to the keystore file. Classpath resources may also be specified, by using the classpath prefix - for example: `classpath:path/to/keystore`.
<4> The password of the keystore.
<5> The password of the key.
<6> The path to the truststore file. Classpath resources may also be specified, by using the classpath prefix - for example: `classpath:path/to/trust-store`
<7> The password of the trust store.

NOTE: If HTTPS is enabled, it completely replaces HTTP as the protocol over
which the REST endpoints and the Data Flow Dashboard interact. Plain HTTP requests
will fail. Therefore, make sure that you configure your Shell accordingly.

[[configuration-security-self-signed-certificates]]
===== Using Self-Signed Certificates

For testing purposes or during development, it might be convenient to create self-signed certificates.
To get started, execute the following command to create a certificate:

[source,bash]
----
$ keytool -genkey -alias dataflow -keyalg RSA -keystore dataflow.keystore \
          -validity 3650 -storetype JKS \
          -dname "CN=localhost, OU=Spring, O=Pivotal, L=Kailua-Kona, ST=HI, C=US"  # <1>
          -keypass dataflow -storepass dataflow
----

<1> `CN` is the important parameter here. It should match the domain you are trying to access - for example, `localhost`.

Then add the following lines to your `application.yml` file:

[source,yaml]
----
server:
  port: 8443
  ssl:
    enabled: true
    key-alias: dataflow
    key-store: "/your/path/to/dataflow.keystore"
    key-store-type: jks
    key-store-password: dataflow
    key-password: dataflow
----

This is all that is needed for the Data Flow Server. Once you start the server,
you should be able to access it at `https://localhost:8443/`.
As this is a self-signed certificate, you should hit a warning in your browser, which
you need to ignore.

[[configuration-security-self-signed-certificates-shell]]
===== Self-Signed Certificates and the Shell

By default, self-signed certificates are an issue for the shell, and additional steps
are necessary to make the shell work with self-signed certificates. Two options
are available:

* Add the self-signed certificate to the JVM truststore.
* Skip certificate validation.

====== Adding the Self-signed Certificate to the JVM Truststore

In order to use the JVM truststore option, we need to
export the previously created certificate from the keystore, as follows:

[source,bash]
----
$ keytool -export -alias dataflow -keystore dataflow.keystore -file dataflow_cert -storepass dataflow
----

Next, we need to create a truststore which the shell can use, as follows:

[source,bash]
----
$ keytool -importcert -keystore dataflow.truststore -alias dataflow -storepass dataflow -file dataflow_cert -noprompt
----

Now, you are ready to launch the Data Flow Shell by using the following JVM arguments:

[source,bash,subs=attributes]
----
$ java -Djavax.net.ssl.trustStorePassword=dataflow \
       -Djavax.net.ssl.trustStore=/path/to/dataflow.truststore \
       -Djavax.net.ssl.trustStoreType=jks \
       -jar spring-cloud-dataflow-shell-{project-version}.jar
----

[TIP]
====
In case you run into trouble establishing a connection over SSL, you can enable additional
logging by using and setting the `javax.net.debug` JVM argument to `ssl`.
====

Do not forget to target the Data Flow Server with the following:

[source,bash]
----
dataflow:> dataflow config server https://localhost:8443/
----

====== Skipping Certificate Validation

Alternatively, you can also bypass the certification validation by providing the
optional command-line parameter `--dataflow.skip-ssl-validation=true`.

If you set this command-line parameter, the shell accepts any (self-signed) SSL
certificate.

[WARNING]
====
If possible, you should avoid using this option. Disabling the trust manager
defeats the purpose of SSL and makes you vulnerable to man-in-the-middle attacks.
====

[[configuration-security-oauth2]]
==== Authentication using OAuth 2.0

In order to support authentication and authorization, Spring Cloud Data
Flow is using https://oauth.net/2/[OAuth 2.0] and https://openid.net/connect/[OpenID Connect].
It lets you integrate Spring Cloud Data Flow into Single Sign On (SSO)
environments.

NOTE: As of Spring Cloud Data Flow 2.0, OAuth2 is the only mechanism
for providing authentication and authorization.

The following OAuth2 Grant Types are used:

* *Authorization Code*: Used for the GUI (browser) integration. Visitors are redirected to your OAuth Service for authentication
* *Password*: Used by the shell (and the REST integration), so visitors can log in with username and password
* *Client Credentials*: Retrieve an access token directly from your OAuth provider and pass it to the Data Flow server by using the Authorization HTTP header

NOTE: Currently, Spring Cloud Data Flow uses opaque tokens and not transparent
tokens (JWT).

The REST endpoints can be accessed in two ways:

* *Basic authentication*, which uses the _Password Grant Type_ under the covers to authenticate with your OAuth2 service
* *Access token*, which uses the Client _Credentials Grant Type_ under the covers

NOTE: When authentication is set up, it is strongly recommended to enable HTTPS
as well, especially in production environments.

You can turn on OAuth2 authentication by adding the following to `application.yml` or by setting
environment variables. The following example shows the minimal setup needed for
https://github.com/cloudfoundry/uaa[CloudFoundry User Account and Authentication (UAA) Server]:

[source,yaml]
----
spring:
  security:
    oauth2:                                                           # <1>
      client:
        registration:
          uaa:                                                        # <2>
            client-id: myclient
            client-secret: mysecret
            redirect-uri: '{baseUrl}/login/oauth2/code/{registrationId}'
            authorization-grant-type: authorization_code
            scope:
            - openid                                                  # <3>
        provider:
          uaa:
            jwk-set-uri: http://uaa.local:8080/uaa/token_keys
            token-uri: http://uaa.local:8080/uaa/oauth/token
            user-info-uri: http://uaa.local:8080/uaa/userinfo    # <4>
            user-name-attribute: user_name                            # <5>
            authorization-uri: http://uaa.local:8080/uaa/oauth/authorize
      resourceserver:
        opaquetoken:
          introspection-uri: http://uaa.local:8080/uaa/introspect # <6>
          client-id: dataflow
          client-secret: dataflow
----

<1> Providing this property activates OAuth2 security
<2> The provider id. It is possible to specify more than 1 provider
<3> As the UAA is an OpenID provider, you must at least specify the `openid` scope.
    If your provider also provides additional scopes to control the role assignments,
    you must specify those scopes here as well
<4> OpenID endpoint. Used to retrieve user information such as the username. Mandatory.
<5> The JSON property of the response that contains the username
<6> Used to introspect and validate a directly passed-in token. Mandatory.

You can verify that basic authentication is working properly by using curl, as follows:

[source,bash]
----
curl -u myusername:mypassword http://localhost:9393/ -H 'Accept: application/json'
----

As a result, you should see a list of available REST endpoints.

IMPORTANT: Please be aware that when accessing the Root URL with a web browser and
enabled security, you are redirected to the Dashboard UI. In order to see the
list of REST endpoints, specify the `application/json` Accept header. Also be sure
to add the `Accept` header using tools such as
https://chrome.google.com/webstore/detail/postman/fhbjgbiflinjbdggehcddcbncdddomop?hl=en[Postman] (Chrome)
or https://addons.mozilla.org/en-GB/firefox/addon/restclient/[RESTClient] (Firefox).

Besides Basic Authentication, you can also provide an Access Token in order to
access the REST Api. In order to make that happen, you would retrieve an
OAuth2 Access Token from your OAuth2 provider first and then pass that Access Token to
the REST Api using the *Authorization* Http header:

```
$ curl -H "Authorization: Bearer <ACCESS_TOKEN>" http://localhost:9393/ -H 'Accept: application/json'
```

[[configuration-security-customizing-authorization]]
==== Customizing Authorization

The preceding content deals with mostly with authentication - that is, how to assess
the identity of the user. In this section we want to discuss the available
*authorization* options - that is, who can do what.

The authorization rules are defined in `dataflow-server-defaults.yml` (part of
the Spring Cloud Data Flow Core module).

Because the determination of security roles is environment-specific,
Spring Cloud Data Flow, assigns all roles to authenticated OAuth2
users by default. The `DefaultDataflowAuthoritiesExtractor` class is used for that purpose.

Alternatively, Spring Cloud Data Flow can map OAuth2 scopes to Data Flow roles by
setting the boolean property `map-oauth-scopes` for your provider to `true` (False is the default).
For example, if your provider's id is `uaa`, the property would be
`spring.cloud.dataflow.security.authorization.provider-role-mappings.uaa.map-oauth-scopes`.

For more details, please see the chapter on <<configuration-security-role-mapping>>.

Lastly, you can customize the role mapping behavior by providing your own Spring bean definition that
extends Spring Cloud Data Flow's `AuthorityMapper` interface. In that case,
the custom bean definition takes precedence over the default one provided by
Spring Cloud Data Flow.

The default scheme uses seven roles to protect the xref:api-guide[REST endpoints]
that Spring Cloud Data Flow exposes:

* *ROLE_CREATE* for anything that involves creating, e.g. creating streams or tasks
* *ROLE_DEPLOY* for deploying streams or launching tasks
* *ROLE_DESTROY* for anything that involves deleting streams, tasks etc.
* *ROLE_MANAGE* for boot management endpoints
* *ROLE_MODIFY* for anything that involves mutating the state of the system
* *ROLE_SCHEDULE* for scheduling related operation (e.g. schedule a task execution)
* *ROLE_VIEW* for anything that relates to retrieving state

As mentioned earlier, all authorization-related default settings are specified
in `dataflow-server-defaults.yml`, which is part of the Spring Cloud Data Flow Core
Module. Nonetheless, you can override those settings, if desired - for example,
in `application.yml`. The configuration takes the form of a YAML list (as some
rules may have precedence over others). Consequently, you need to copy and paste
the whole list and tailor it to your needs (as there is no way to merge lists).

NOTE: Always refer to your version of the `application.yml` file, as the following snippet may be outdated.

The default rules are as follows:

[source,yaml]
----
spring:
  cloud:
    dataflow:
      security:
        authorization:
          enabled: true
          loginUrl: "/"
          permit-all-paths: "/authenticate,/security/info,/assets/**,/dashboard/logout-success-oauth.html,/favicon.ico"
          rules:
            # About

            - GET    /about                          => hasRole('ROLE_VIEW')

            # Audit

            - GET /audit-records                     => hasRole('ROLE_VIEW')
            - GET /audit-records/**                  => hasRole('ROLE_VIEW')

            # Boot Endpoints

            - GET /management/**                  => hasRole('ROLE_MANAGE')

            # Apps

            - GET    /apps                           => hasRole('ROLE_VIEW')
            - GET    /apps/**                        => hasRole('ROLE_VIEW')
            - DELETE /apps/**                        => hasRole('ROLE_DESTROY')
            - POST   /apps                           => hasRole('ROLE_CREATE')
            - POST   /apps/**                        => hasRole('ROLE_CREATE')
            - PUT    /apps/**                        => hasRole('ROLE_MODIFY')

            # Completions

            - GET /completions/**                    => hasRole('ROLE_VIEW')

            # Job Executions & Batch Job Execution Steps && Job Step Execution Progress

            - GET    /jobs/executions                => hasRole('ROLE_VIEW')
            - PUT    /jobs/executions/**             => hasRole('ROLE_MODIFY')
            - GET    /jobs/executions/**             => hasRole('ROLE_VIEW')
            - GET    /jobs/thinexecutions            => hasRole('ROLE_VIEW')

            # Batch Job Instances

            - GET    /jobs/instances                 => hasRole('ROLE_VIEW')
            - GET    /jobs/instances/*               => hasRole('ROLE_VIEW')

            # Running Applications

            - GET    /runtime/streams                => hasRole('ROLE_VIEW')
            - GET    /runtime/apps                   => hasRole('ROLE_VIEW')
            - GET    /runtime/apps/**                => hasRole('ROLE_VIEW')

            # Stream Definitions

            - GET    /streams/definitions            => hasRole('ROLE_VIEW')
            - GET    /streams/definitions/*          => hasRole('ROLE_VIEW')
            - GET    /streams/definitions/*/related  => hasRole('ROLE_VIEW')
            - POST   /streams/definitions            => hasRole('ROLE_CREATE')
            - DELETE /streams/definitions/*          => hasRole('ROLE_DESTROY')
            - DELETE /streams/definitions            => hasRole('ROLE_DESTROY')

            # Stream Deployments

            - DELETE /streams/deployments/*          => hasRole('ROLE_DEPLOY')
            - DELETE /streams/deployments            => hasRole('ROLE_DEPLOY')
            - POST   /streams/deployments/**         => hasRole('ROLE_MODIFY')
            - GET    /streams/deployments/**         => hasRole('ROLE_VIEW')

            # Stream Validations

            - GET /streams/validation/               => hasRole('ROLE_VIEW')
            - GET /streams/validation/*              => hasRole('ROLE_VIEW')

            # Stream Logs
            - GET /streams/logs/*                    => hasRole('ROLE_VIEW')

            # Task Definitions

            - POST   /tasks/definitions              => hasRole('ROLE_CREATE')
            - DELETE /tasks/definitions/*            => hasRole('ROLE_DESTROY')
            - GET    /tasks/definitions              => hasRole('ROLE_VIEW')
            - GET    /tasks/definitions/*            => hasRole('ROLE_VIEW')

            # Task Executions

            - GET    /tasks/executions               => hasRole('ROLE_VIEW')
            - GET    /tasks/executions/*             => hasRole('ROLE_VIEW')
            - POST   /tasks/executions               => hasRole('ROLE_DEPLOY')
            - POST   /tasks/executions/*             => hasRole('ROLE_DEPLOY')
            - DELETE /tasks/executions/*             => hasRole('ROLE_DESTROY')

            # Task Schedules

            - GET    /tasks/schedules                => hasRole('ROLE_VIEW')
            - GET    /tasks/schedules/*              => hasRole('ROLE_VIEW')
            - GET    /tasks/schedules/instances      => hasRole('ROLE_VIEW')
            - GET    /tasks/schedules/instances/*    => hasRole('ROLE_VIEW')
            - POST   /tasks/schedules                => hasRole('ROLE_SCHEDULE')
            - DELETE /tasks/schedules/*              => hasRole('ROLE_SCHEDULE')

            # Task Platform Account List */

            - GET    /tasks/platforms                => hasRole('ROLE_VIEW')

            # Task Validations

            - GET    /tasks/validation/               => hasRole('ROLE_VIEW')
            - GET    /tasks/validation/*              => hasRole('ROLE_VIEW')

            # Task Logs
            - GET /tasks/logs/*                       => hasRole('ROLE_VIEW')

            # Tools

            - POST   /tools/**                       => hasRole('ROLE_VIEW')

----

The format of each line is the following:
----
HTTP_METHOD URL_PATTERN '=>' SECURITY_ATTRIBUTE
----

where

* HTTP_METHOD is one http method, capital case
* URL_PATTERN is an Ant style URL pattern
* SECURITY_ATTRIBUTE is a SpEL expression.  See https://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#el-access[Expression-Based Access Control].
* Each of those separated by one or several blank characters (spaces, tabs, and so on)

Be mindful that the above is indeed a YAML list, not a map (thus the use of '-' dashes
at the start of each line) that lives under the `spring.cloud.dataflow.security.authorization.rules` key.

[[configuration-security-authorization-local-shell-and-dashboard]]
===== Authorization - Shell and Dashboard Behavior

When security is enabled, the dashboard and the shell are role-aware,
meaning that, depending on the assigned roles, not all functionality may be visible.

For instance, shell commands for which the user does not have the necessary roles
are marked as unavailable.

[IMPORTANT]
====
Currently, the shell's `help` command lists commands that are unavailable.
Please track the following issue: https://github.com/spring-projects/spring-shell/issues/115
====

Similarly, for the Dashboard, the UI does not show pages or page elements for
which the user is not authorized.

[[configuration-security-securing-management-endpoints]]
===== Securing the Spring Boot Management Endpoints

When security is enabled, the
{spring-boot-docs-reference}/html/production-ready-monitoring.html[Spring Boot HTTP Management Endpoints]
are secured the same way as the other REST endpoints. The management REST endpoints
are available under `/management` and require the `MANAGEMENT` role.

The default configuration in `dataflow-server-defaults.yml` has the following configuration:

[source,yaml]
----
management:
  endpoints:
    web:
      base-path: /management
  security:
    roles: MANAGE
----

IMPORTANT: Currently, please refrain from customizing the default management path.

[[configuration-security-uaa-authentication]]
==== Setting up UAA Authentication

For local deployment scenarios, we recommend using the https://github.com/cloudfoundry/uaa[CloudFoundry User
Account and Authentication (UAA) Server], which is https://openid.net/certification/[OpenID certified].
While the UAA is used by https://www.cloudfoundry.org/[Cloud Foundry],
it is also a fully featured stand alone OAuth2 server with enterprise features such as
https://github.com/cloudfoundry/uaa/blob/develop/docs/UAA-LDAP.md[LDAP integration].

===== Requirements

Checkout, Build and Run UAA:

- Make sure you use Java 8
- https://git-scm.com/[Git] installed
- You need the https://github.com/cloudfoundry/cf-uaac[CloudFoundry UAA Command Line Client] installed
- Use a different host name for UAA when running on the same machine, e.g. `http://uaa/`

In case you run into issues installing _uaac_, you may have to set the `GEM_HOME` environment
variable:

[source,bash]
----
export GEM_HOME="$HOME/.gem"
----

and add `~/.gem/gems/cf-uaac-4.2.0/bin` to your path.

===== Prepare UAA for JWT

As the UAA is an OpenID provider it uses JSON Web Tokens (JWT) it needs to have
a private key for signing those JWTs:

[source,bash]
----
openssl genrsa -out signingkey.pem 2048
openssl rsa -in signingkey.pem -pubout -out verificationkey.pem
export JWT_TOKEN_SIGNING_KEY=$(cat signingkey.pem)
export JWT_TOKEN_VERIFICATION_KEY=$(cat verificationkey.pem)
----

Later, once the UAA is started you can see the keys when accessing `http://uaa:8080/uaa/token_keys`

===== Download + Start UAA

[source,bash]
----
git clone https://github.com/pivotal/uaa-bundled.git
cd uaa-bundled
./mvnw clean install
java -jar target/uaa-bundled-1.0.0.BUILD-SNAPSHOT.jar
----

The configuration of the UAA is driven by either a Yaml file `uaa.yml` or you can script the configuration
using the UAA Command Line Client:

[source,bash]
----
uaac target http://uaa:8080/uaa
uaac token client get admin -s adminsecret
uaac client add dataflow \
  --name dataflow \
  --secret dataflow \
  --scope cloud_controller.read,cloud_controller.write,openid,password.write,scim.userids,foo.create,foo.view \
  --authorized_grant_types password,authorization_code,client_credentials,refresh_token \
  --authorities uaa.resource,dataflow.create,dataflow.deploy,dataflow.destroy,dataflow.manage,dataflow.modify,dataflow.schedule,dataflow.view,foo.view,foo.create\
  --redirect_uri http://localhost:9393/login \
  --autoapprove openid \

uaac group add "foo.view"
uaac group add "foo.create"

uaac user add springrocks -p mysecret --emails springrocks@someplace.com
uaac user add vieweronly -p mysecret --emails mrviewer@someplace.com

uaac member add "foo.view" springrocks
uaac member add "foo.create" springrocks
uaac member add "foo.view" vieweronly
----

This script will set up the dataflow client as well as 2 users:

- User _springrocks_ will have both scopes `foo.view` and `foo.create`
- User _vieweronly_ will only have one scope `foo.view`

Once added, you can quickly double-check that the UAA has the users created:
[source,bash]
----
curl -v -d"username=springrocks&password=mysecret&client_id=dataflow&grant_type=password" -u "dataflow:dataflow" http://uaa:8080/uaa/oauth/token -d 'token_format=opaque'
----

This should produce output similar to the following:

[source,bash]
----
*   Trying 127.0.0.1...
* TCP_NODELAY set
* Connected to uaa (127.0.0.1) port 8080 (#0)
* Server auth using Basic with user 'dataflow'
> POST /uaa/oauth/token HTTP/1.1
> Host: uaa:8080
> Authorization: Basic ZGF0YWZsb3c6ZGF0YWZsb3c=
> User-Agent: curl/7.54.0
> Accept: */*
> Content-Length: 97
> Content-Type: application/x-www-form-urlencoded
>
* upload completely sent off: 97 out of 97 bytes
< HTTP/1.1 200
< Cache-Control: no-store
< Pragma: no-cache
< X-XSS-Protection: 1; mode=block
< X-Frame-Options: DENY
< X-Content-Type-Options: nosniff
< Content-Type: application/json;charset=UTF-8
< Transfer-Encoding: chunked
< Date: Thu, 31 Oct 2019 21:22:59 GMT
<
* Connection #0 to host uaa left intact
{"access_token":"0329c8ecdf594ee78c271e022138be9d","token_type":"bearer","id_token":"eyJhbGciOiJSUzI1NiIsImprdSI6Imh0dHBzOi8vbG9jYWxob3N0OjgwODAvdWFhL3Rva2VuX2tleXMiLCJraWQiOiJsZWdhY3ktdG9rZW4ta2V5IiwidHlwIjoiSldUIn0.eyJzdWIiOiJlZTg4MDg4Ny00MWM2LTRkMWQtYjcyZC1hOTQ4MmFmNGViYTQiLCJhdWQiOlsiZGF0YWZsb3ciXSwiaXNzIjoiaHR0cDovL2xvY2FsaG9zdDo4MDkwL3VhYS9vYXV0aC90b2tlbiIsImV4cCI6MTU3MjYwMDE3OSwiaWF0IjoxNTcyNTU2OTc5LCJhbXIiOlsicHdkIl0sImF6cCI6ImRhdGFmbG93Iiwic2NvcGUiOlsib3BlbmlkIl0sImVtYWlsIjoic3ByaW5ncm9ja3NAc29tZXBsYWNlLmNvbSIsInppZCI6InVhYSIsIm9yaWdpbiI6InVhYSIsImp0aSI6IjAzMjljOGVjZGY1OTRlZTc4YzI3MWUwMjIxMzhiZTlkIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImNsaWVudF9pZCI6ImRhdGFmbG93IiwiY2lkIjoiZGF0YWZsb3ciLCJncmFudF90eXBlIjoicGFzc3dvcmQiLCJ1c2VyX25hbWUiOiJzcHJpbmdyb2NrcyIsInJldl9zaWciOiJlOTkyMDQxNSIsInVzZXJfaWQiOiJlZTg4MDg4Ny00MWM2LTRkMWQtYjcyZC1hOTQ4MmFmNGViYTQiLCJhdXRoX3RpbWUiOjE1NzI1NTY5Nzl9.bqYvicyCPB5cIIu_2HEe5_c7nSGXKw7B8-reTvyYjOQ2qXSMq7gzS4LCCQ-CMcb4IirlDaFlQtZJSDE-_UsM33-ThmtFdx--TujvTR1u2nzot4Pq5A_ThmhhcCB21x6-RNNAJl9X9uUcT3gKfKVs3gjE0tm2K1vZfOkiGhjseIbwht2vBx0MnHteJpVW6U0pyCWG_tpBjrNBSj9yLoQZcqrtxYrWvPHaa9ljxfvaIsOnCZBGT7I552O1VRHWMj1lwNmRNZy5koJFPF7SbhiTM8eLkZVNdR3GEiofpzLCfoQXrr52YbiqjkYT94t3wz5C6u1JtBtgc2vq60HmR45bvg","refresh_token":"6ee95d017ada408697f2d19b04f7aa6c-r","expires_in":43199,"scope":"scim.userids openid foo.create cloud_controller.read password.write cloud_controller.write foo.view","jti":"0329c8ecdf594ee78c271e022138be9d"}
----

Using `token_format` parameter you can requested token to be either:

- opaque
- jwt

===== Start Skipper

[source,bash]
----
git clone https://github.com/spring-cloud/spring-cloud-skipper.git
cd spring-cloud/spring-cloud-skipper
./mvnw clean package -DskipTests=true
java -jar spring-cloud-skipper-server/target/spring-cloud-skipper-server-2.2.0.BUILD-SNAPSHOT.jar
----

===== Start Spring Cloud Data Flow

[source,bash]
----
git clone https://github.com/spring-cloud/spring-cloud-dataflow.git
cd spring-cloud-dataflow
./mvnw clean package -DskipTests=true
cd ..
----

Create a yaml file scdf.yml with the following contents:

[source,yaml]
----
spring:
  cloud:
    dataflow:
      security:
        authorization:
          provider-role-mappings:
            uaa:
              map-oauth-scopes: true
              role-mappings:
                ROLE_CREATE: foo.create
                ROLE_DEPLOY: foo.create
                ROLE_DESTROY: foo.create
                ROLE_MANAGE: foo.create
                ROLE_MODIFY: foo.create
                ROLE_SCHEDULE: foo.create
                ROLE_VIEW: foo.view
  security:
    oauth2:
      client:
        registration:
          uaa:
            redirect-uri: '{baseUrl}/login/oauth2/code/{registrationId}'
            authorization-grant-type: authorization_code
            client-id: dataflow
            client-secret: dataflow
            scope:                                                       <1>
            - openid
            - foo.create
            - foo.view
        provider:
          uaa:
            jwk-set-uri: http://uaa:8080/uaa/token_keys
            token-uri: http://uaa:8080/uaa/oauth/token
            user-info-uri: http://uaa:8080/uaa/userinfo                  <2>
            user-name-attribute: user_name
            authorization-uri: http://uaa:8080/uaa/oauth/authorize
      resourceserver:
        opaquetoken:                                                     <3>
          introspection-uri: http://uaa:8080/uaa/introspect
          client-id: dataflow
          client-secret: dataflow
----

<1> If you use scopes to identify roles, please make sure to also request
    the relevant scopes, e.g `dataflow.view`, `dataflow.create` and don't forget to request the `openid` scope
<2> Used to retrieve profile information, e.g. username for display purposes (mandatory)
<3> Used for token introspection and validation (mandatory)

The `introspection-uri` property is especially important when passing an externally retrieved (opaque)
OAuth Access Token to Spring Cloud Data Flow. In that case Spring Cloud Data Flow will take the OAuth Access,
and use the UAA's https://docs.cloudfoundry.org/api/uaa/version/74.4.0/index.html#introspect-token[Introspect Token Endpoint]
to not only check the validity of the token but also retrieve the associated OAuth scopes from the UAA

Finally startup Spring Cloud Data Flow:

[source,bash]
----
java -jar spring-cloud-dataflow/spring-cloud-dataflow-server/target/spring-cloud-dataflow-server-2.3.0.BUILD-SNAPSHOT.jar --spring.config.additional-location=scdf.yml
----

[[configuration-security-role-mapping]]
===== Role Mappings

By default all roles are assigned to users that login to Spring Cloud Data Flow.
However, you can set the property:

`spring.cloud.dataflow.security.authorization.provider-role-mappings.uaa.map-oauth-scopes: true`

This will instruct the underlying `DefaultAuthoritiesExtractor` to map
OAuth scopes to the respective authorities. The following scopes are supported:

* Scope `dataflow.create` maps to the `CREATE` role
* Scope `dataflow.deploy` maps to the `DEPLOY` role
* Scope `dataflow.destroy` maps to the `DESTROY` role
* Scope `dataflow.manage` maps to the `MANAGE` role
* Scope `dataflow.modify` maps to the `MODIFY` role
* Scope `dataflow.schedule` maps to the `SCHEDULE` role
* Scope `dataflow.view` maps to the `VIEW` role

Additionally you can also map arbitrary scopes to each of the Data Flow roles:

[source,yaml]
----
spring:
  cloud:
    dataflow:
      security:
        authorization:
          provider-role-mappings:
            uaa:
              map-oauth-scopes: true                                    # <1>
              role-mappings:
                ROLE_CREATE: dataflow.create                            # <2>
                ROLE_DEPLOY: dataflow.deploy
                ROLE_DESTROY: dataflow.destoy
                ROLE_MANAGE: dataflow.manage
                ROLE_MODIFY: dataflow.modify
                ROLE_SCHEDULE: dataflow.schedule
                ROLE_VIEW: dataflow.view
----

<1> Enables explicit mapping support from OAuth scopes to Data Flow roles
<2> When role mapping support is enabled, you must provide a mapping for
all 7 Spring Cloud Data Flow roles *ROLE_CREATE*, *ROLE_DEPLOY*, *ROLE_DESTROY*, *ROLE_MANAGE*, *ROLE_MODIFY*, *ROLE_SCHEDULE*, *ROLE_VIEW*.

[TIP]
====
You can assign an OAuth scope to multiple Spring Cloud Data Flow roles, giving you flexible regarding the granularity of your authorization configuration.
====

[[configuration-security-ldap-authentication]]
==== LDAP Authentication

LDAP Authentication (Lightweight Directory Access Protocol) is indirectly
provided by Spring Cloud Data Flow using the UAA. The UAA itself provides
https://github.com/cloudfoundry/uaa/blob/develop/docs/UAA-LDAP.md[comprehensive LDAP support].

[IMPORTANT]
====
While you may use your own OAuth2 authentication server, the LDAP support
documented here requires using the UAA as authentication server. For any
other provider, please consult the documentation for that particular provider.
====

The UAA supports authentication against an LDAP (Lightweight Directory Access Protocol)
server using the following modes:

* https://github.com/cloudfoundry/uaa/blob/develop/docs/UAA-LDAP.md#ldap-search-and-bind[Direct bind]
* https://github.com/cloudfoundry/uaa/blob/develop/docs/UAA-LDAP.md#ldap-bind[Search and bind]
* https://github.com/cloudfoundry/uaa/blob/develop/docs/UAA-LDAP.md#ldap-search-and-compare[Search and Compare]

[NOTE]
====
When integrating with an external identity provider such as LDAP, authentication
within the UAA becomes *chained*. UAA first attempts to authenticate with
a user's credentials against the UAA user store before the external provider,
LDAP. For more information, see
https://github.com/cloudfoundry/uaa/blob/develop/docs/UAA-LDAP.md#chained-authentication[Chained Authentication]
in the _User Account and Authentication LDAP Integration_ GitHub documentation.
====

[[configuration-security-ldap-role-mapping]]
===== LDAP Role Mapping

The OAuth2 authentication server (UAA), provides comprehensive support
for https://github.com/cloudfoundry/uaa/blob/develop/docs/UAA-LDAP.md#scopes[mapping LDAP groups to OAuth scopes].

The following options exist:

* `ldap/ldap-groups-null.xml` No groups will be mapped
* `ldap/ldap-groups-as-scopes.xml` Group names will be retrieved from an LDAP attribute. E.g. `CN`
* `ldap/ldap-groups-map-to-scopes.xml` Groups will be mapped to UAA groups using the external_group_mapping table

These values are specified via the configuration property `ldap.groups.file controls`. Under the covers
these values reference a Spring XML configuration file.

[TIP]
====
During test and development it might be necessary to make frequent changes
to LDAP groups and users and see those reflected in the UAA. However, user
information is cached for the duration of the login. The following script
helps to retrieve the updated information quickly:

[source,bash]
----
#!/bin/bash
uaac token delete --all
uaac target http://localhost:8080/uaa
uaac token owner get cf <username> -s "" -p  <password>
uaac token client get admin -s adminsecret
uaac user get <username>
----
====

[[configuration-security-ldap-uaa-example]]
===== LDAP Security and UAA Example Application

In order to get up and running quickly and to help you understand the security architecture, we
provide the https://github.com/spring-cloud/spring-cloud-dataflow-samples/tree/master/security-ldap-uaa-example[LDAP Security and UAA Example]
on GitHub.

[IMPORTANT]
====
This is solely a demo/example application and shall not be used in production.
====

The setup consists of:

* Spring Cloud Data Flow Server
* Skipper Server
* CloudFoundry User Account and Authentication (UAA) Server
* Lightweight Directory Access Protocol (LDAP) Server (provided by https://directory.apache.org/[Apache Directory Server] (ApacheDS))

Ultimately, as part of this example, you will learn how to configure and launch
a Composed Task using this security setup.

[[configuration-security-spring-security-oauth2-example]]
==== Spring Security OAuth2 Resource/Authorization Server Sample

For local testing and development, you may also use the Resource and Authorization
Server support provided by
https://projects.spring.io/spring-security-oauth/[Spring Security OAuth]. It
allows you to easily create your own (very basic) OAuth2 Server with the following simple annotations:

* `@EnableResourceServer`
* `@EnableAuthorizationServer`

NOTE: In fact the UAA uses Spring Security OAuth2 under the covers, thus the basic endpoints
are the same.

A working example application can be found at:
https://github.com/ghillert/oauth-test-server/[https://github.com/ghillert/oauth-test-server/]

Clone the project and configure Spring Cloud Data Flow with the respective Client ID and Client Secret:

[source,yaml]
----
security:
  oauth2:
    client:
      client-id: myclient
      client-secret: mysecret
      access-token-uri: http://127.0.0.1:9999/oauth/token
      user-authorization-uri: http://127.0.0.1:9999/oauth/authorize
    resource:
      user-info-uri: http://127.0.0.1:9999/me
      token-info-uri: http://127.0.0.1:9999/oauth/check_token
----

IMPORTANT: This sample application is not intended for production use

[[configuration-security-shell-authentication]]
==== Data Flow Shell Authentication

When using the Shell, the credentials can either be provided via username and password
or by specifying a _credentials-provider_ command. If your OAuth2 provider supports
the _Password_ Grant Type you can start the _Data Flow Shell_ with:

[source,bash,subs=attributes+]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar         \
  --dataflow.uri=http://localhost:9393                                \   # <1>
  --dataflow.username=my_username                                     \   # <2>
  --dataflow.password=my_password                                     \   # <3>
  --skip-ssl-validation  true                                         \   # <4>
----

<1> Optional, defaults to http://localhost:9393.
<2> Mandatory.
<3> If the password is not provided, the user is prompted for it.
<4> Optional, defaults to `false`, ignores certificate errors (when using self-signed certificates). Use cautiously!


NOTE: Keep in mind that when authentication for Spring Cloud Data Flow is enabled,
the underlying OAuth2 provider *must* support the _Password_ OAuth2 Grant Type
if you want to use the Shell via username/password authentication.

From within the Data Flow Shell you can also provide credentials by using the following command:

[source,bash]
----
server-unknown:>dataflow config server                                \
  --uri  http://localhost:9393                                        \   # <1>
  --username myuser                                                   \   # <2>
  --password mysecret                                                 \   # <3>
  --skip-ssl-validation  true                                         \   # <4>
----

<1> Optional, defaults to http://localhost:9393.
<2> Mandatory..
<3> If security is enabled, and the password is not provided, the user is prompted for it.
<4> Optional, ignores certificate errors (when using self-signed certificates). Use cautiously!

The following image shows a typical shell command to connect to and authenticate a Data
Flow Server:

.Target and Authenticate with the Data Flow Server from within the Shell
image::{dataflow-asciidoc}/images/dataflow-security-shell-target.png[Target and Authenticate with the Data Flow Server from within the Shell, scaledwidth="100%"]

Once successfully targeted, you should see the following output:

[source,bash]
----
dataflow:>dataflow config info
dataflow config info


Credentials[username='my_username, password=****']

Result                                            
Target     http://localhost:9393                  

----

Alternatively, you can specify the _credentials-provider_ command in order to
pass-in a bearer token directly, instead of providing a username and password.
This works from within the shell or by providing the
`--dataflow.credentials-provider-command` command-line argument when starting the Shell.

[IMPORTANT]
====
When using the _credentials-provider_ command, please be aware that your
specified command *must* return a _Bearer token_ (Access Token prefixed with _Bearer_).
For instance, in Unix environments the following simplistic command can be used:

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{project-version}.jar \
  --dataflow.uri=http://localhost:9393 \
  --dataflow.credentials-provider-command="echo Bearer 123456789"
----

====

=== About Configuration
The Spring Cloud Data Flow About Restful API result contains a display name,
version, and, if specified, a URL for each of the major dependencies that
comprise Spring Cloud Data Flow.  The result (if enabled) also contains the
sha1 and or sha256 checksum values for the shell dependency. The information
that is returned for each of the dependencies is configurable by setting the following
properties:

* spring.cloud.dataflow.version-info.spring-cloud-dataflow-core.name: the
name to be used for the core.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-core.version:
the version to be used for the core.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-dashboard.name: the
name to be used for the dashboard.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-dashboard.version:
the version to be used for the dashboard.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-implementation.name: the
name to be used for the implementation.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-implementation.version:
the version to be used for the implementation.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.name: the
name to be used for the shell.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.version:
the version to be used for the shell.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.url:
the URL to be used for downloading the shell dependency.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha1: the sha1
checksum value that is returned with the shell dependency info.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha256:
the sha256 checksum value that is returned with the shell dependency info.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha1-url:
if the `spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha1`
is not specified, SCDF uses the contents of the file specified at this URL for the checksum.
* spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha256-url:
if the `spring.cloud.dataflow.version-info.spring-cloud-dataflow-shell.checksum-sha256`
is not specified, SCDF uses the contents of the file specified at this URL for the checksum.

==== Enabling Shell Checksum values
By default, checksum values are not displayed for the shell dependency. If
you need this feature enabled, set the
`spring.cloud.dataflow.version-info.dependency-fetch.enabled` property to true.

==== Reserved Values for URLs
There are reserved values (surrounded by curly braces) that you can insert into
the URL that will make sure that the links are up to date:

* repository: if using a build-snapshot, milestone, or release candidate of
Data Flow, the repository refers to the repo-spring-io repository. Otherwise, it
refers to Maven Central.
* version: Inserts the version of the jar/pom.

For example,
`https://myrepository/org/springframework/cloud/spring-cloud-dataflow-shell/\{version}/spring-cloud-dataflow-shell-\{version}.jar`
produces
`https://myrepository/org/springframework/cloud/spring-cloud-dataflow-shell/1.2.3.RELEASE/spring-cloud-dataflow-shell-1.2.3.RELEASE.jar`
if you were using the 1.2.3.RELEASE version of the Spring Cloud Data Flow Shell
