[[dashboard]]
= Dashboard

[partintro]
--
This section describes how to use the dashboard of Spring Cloud Data Flow.
--

[[dashboard-introduction]]
== Introduction

Spring Cloud Data Flow provides a browser-based GUI called the dashboard to manage the following information:

* *Apps*: The Apps tab lists all available applications and provides the controls to register/unregister them.
* *Runtime*: The Runtime tab provides the list of all running applications.
* *Streams*: The Streams tab lets you list, design, create, deploy, and destroy Stream Definitions.
ifndef::omit-tasks-docs[]
* *Tasks*: The Tasks tab lets you list, create, launch, schedule and, destroy Task Definitions.
endif::omit-tasks-docs[]
* *Jobs*: The Jobs tab lets you perform batch job related functions.

Upon starting Spring Cloud Data Flow, the dashboard is available at:

`http://<host>:<port>/dashboard`

For example, if Spring Cloud Data Flow is running locally, the dashboard is available at `\http://localhost:9393/dashboard`.

If you have enabled https, then the dashboard will be located at `\https://localhost:9393/dashboard`.
If you have enabled security, a login form is available at `\http://localhost:9393/dashboard/#/login`.

NOTE: The default Dashboard server port is `9393`.

The following image shows the opening page of the Spring Cloud Data Flow dashboard:

.The Spring Cloud Data Flow Dashboard
image::{dataflow-asciidoc}/images/dataflow-dashboard-about.png[The Spring Cloud Data Flow Dashboard, scaledwidth="100%"]



[[dashboard-apps]]
== Apps

The Apps section of the dashboard lists all the available applications and provides the controls to register and unregister them (if applicable).
It is possible to import a number of applications at once by using the Bulk Import Applications action.

The following image shows a typical list of available apps within the dashboard:

.List of Available Applications
image::{dataflow-asciidoc}/images/dataflow-available-apps-list.png[List of available applications, scaledwidth="100%"]



=== Bulk Import of Applications

The Bulk Import Applications page provides numerous options for defining and importing a set of applications all at once.
For bulk import, the application definitions are expected to be expressed in a properties style, as follows:

`<type>.<name> = <coordinates>`

The following examples show a typical application definitions:

[source,subs=properties]
----
task.timestamp=maven://org.springframework.cloud.task.app:timestamp-task:1.2.0.RELEASE
processor.transform=maven://org.springframework.cloud.stream.app:transform-processor-rabbit:1.2.0.RELEASE
----

At the top of the bulk import page, a URI can be specified that points to a properties file stored elsewhere, it should contain properties formatted as shown in the previous example.
Alternatively, by using the textbox labeled "`Apps as Properties`", you can directly list each property string. Finally, if the properties are stored in a local file, the "`Select Properties File`" option opens a local file browser to select the file.
After setting your definitions through one of these routes, click Import.

The following image shows the Bulk Import Applications page:

.Bulk Import Applications
image::{dataflow-asciidoc}/images/dataflow-bulk-import-applications.png[Bulk Import Applications, scaledwidth="100%"]



[[dashboard-runtime]]
== Runtime

The Runtime section of the Dashboard application shows the list of all running applications.
For each runtime app, the state of the deployment and the number of deployed instances is shown.
A list of the used deployment properties is available by clicking on the App Id.

The following image shows an example of the Runtime tab in use:

.List of Running Applications
image::{dataflow-asciidoc}/images/dataflow-runtime.png[List of running applications, scaledwidth="100%"]



[[dashboard-streams]]
== Streams

The Streams tab has two child tabs: Definitions and Create Stream. The following topics describe how to work with each one:

* <<dashboard-stream-definitions>>
* <<dashboard-flo-streams-designer>>
* <<dashboard-stream-deploy>>



[[dashboard-stream-definitions]]
=== Working with Stream Definitions

The Streams section of the Dashboard includes the Definitions tab that provides a listing of Stream definitions.
There you have the option to deploy or undeploy those stream definitions.
Additionally, you can remove the definition by clicking on Destroy.
Each row includes an arrow on the left, which you can click to see a visual representation of the definition.
Hovering over the boxes in the visual representation shows more details about the apps, including any options passed to them.

In the following screenshot, the `timer` stream has been expanded to show the visual representation:

.List of Stream Definitions
image::{dataflow-asciidoc}/images/dataflow-streams-list-definitions.png[List of Stream Definitions, scaledwidth="100%"]

If you click the details button, the view changes to show a visual representation of that stream and any related streams.
In the preceding example, if you click details for the `timer` stream, the view changes to the following view, which clearly shows the relationship between the three streams (two of them are tapping into the `timer` stream):

.Stream Details Page
image::{dataflow-asciidoc}/images/dataflow-stream-details.png[Stream Details Page, scaledwidth="100%"]



[[dashboard-flo-streams-designer]]
=== Creating a Stream

The Streams section of the Dashboard includes the Create Stream tab, which makes available the https://github.com/spring-projects/spring-flo[Spring Flo] designer: a canvas application that offers an interactive graphical interface for creating data pipelines.

In this tab, you can:

* Create, manage, and visualize stream pipelines using DSL, a graphical canvas, or both
* Write pipelines via DSL with content-assist and auto-complete
* Use auto-adjustment and grid-layout capabilities in the GUI for simpler and interactive organization of pipelines

You should watch this https://www.youtube.com/watch?v=78CgV46OstI[screencast] that highlights some of the "Flo for Spring Cloud Data Flow" capabilities.
The Spring Flo https://github.com/spring-projects/spring-flo/wiki[wiki] includes more detailed content on core Flo capabilities.

The following image shows the Flo designer in use:

.Flo for Spring Cloud Data Flow
image::{dataflow-asciidoc}/images/dataflow-flo-create-stream.png[Flo for Spring Cloud Data Flo, scaledwidth="100%"]



[[dashboard-stream-deploy]]
=== Deploying a Stream

The stream deploy page includes tabs that provide different ways to setup the deployment properties and deploy the stream.
The following screenshots show the stream deploy page for `foobar` (`time | log`).

You can define deployments properties using:

* Form builder tab: a builder which help you to define deployment properties (deployer, application properties...)
* Free text tab: a free text area (key/value pairs)

You can switch between the both views, the form builder provides a more stronger validation of the inputs.

.The following image shows the form builder
image::{dataflow-asciidoc}/images/dataflow-stream-deploy-builder.png[Form builder, scaledwidth="100%"]

.The following image shows the same properties in the free text
image::{dataflow-asciidoc}/images/dataflow-stream-deploy-freetext.png[Free text, scaledwidth="100%"]



[dashboard-flo-streams-designer-fanin-fanout]]
=== Creating Fan-In/Fan-Out Streams

In chapter <<spring-cloud-dataflow-stream-dsl-fanin-fanout>> you learned how we can support fan-in and fan-out use cases using <<spring-cloud-dataflow-stream-dsl-named-destinations,named destinations>>.
The UI provides dedicated support for named destinations as well:

.Flo for Spring Cloud Data Flow
image::{dataflow-asciidoc}/images/dataflow-flo-create-stream-fanin-fanout.png[Fan-in and Fan-out example, scaledwidth="100%"]

In this example we have data from an _HTTP Source_ and a _JDBC Source_ that is being sent to the
_sharedData_ channel which represents a *Fan-in* use case.
On the other end we have a _Cassandra Sink_ and a _File Sink_ subscribed to the _sharedData_ channel which represents a *Fan-out* use case.

=== Creating a Tap Stream

Creating Taps using the Dashboard is straightforward.
Let's say you have stream consisting of an _HTTP Source_ and a _File Sink_ and you would like to tap into the stream
to also send data to a _JDBC Sink_.
In order to create the tap stream simply connect the output connector of the _HTTP Source_ to the _JDBC Sink_.
The connection will be displayed as a dotted line, indicating that you created a tap stream.

.Creating a Tap Stream
image::{dataflow-asciidoc}/images/dataflow-flo-create-tap-stream.png[Tap stream example, scaledwidth="100%"]

The primary stream (_HTTP Source_ to _File Sink_) will be automatically named, in case you did not provide a name for the stream, yet.
When creating tap streams, the primary stream must always be explicitly named.
In the picture above, the primary stream was named _HTTP_INGEST_.

Using the Dashboard, you can also switch the primary stream to become the secondary tap stream.

.Change Primary Stream to Secondary Tap Stream
image::{dataflow-asciidoc}/images/dataflow-flo-tap-stream-switch-to-primary-stream.png[Switch tap stream to primary stream, scaledwidth="100%"]

Simply hover over the existing primary stream, the line between _HTTP Source_ and _File Sink_.
Several control icons will appear, and by clicking on the icon labeled _Switch to/from tap_,
you change the primary stream into a tap stream.
Do the same for the tap stream and switch it to a primary stream.

.End Result of Switching the Primary Stream
image::{dataflow-asciidoc}/images/dataflow-flo-tap-stream-switch-to-primary-stream-result.png[End result of switching the tap stream to a primary stream, scaledwidth="100%"]
=======
TIP: When interacting directly with <<spring-cloud-dataflow-stream-dsl-named-destinations,named destinations>>,
there can be "n" combinations (Inputs/Outputs). This allows you to create complex topologies involving a
wide variety of data sources and destinations.
=======

ifndef::omit-tasks-docs[]
[[dashboard-tasks]]
== Tasks

The Tasks section of the Dashboard currently has three tabs:

* <<dashboard-tasks-apps>>
* <<dashboard-task-definition>>
* <<dashboard-tasks-executions>>
* <<dashboard-task-scheduling>>

[[dashboard-tasks-apps]]
=== Apps

Each app encapsulates a unit of work into a reusable component.
Within the Data Flow runtime environment, apps let users create definitions for streams as well as tasks.
Consequently, the Apps tab within the Tasks section lets users create task definitions.

TIP: You can also use this tab to create Batch Jobs.

The following image shows a typical list of task apps:

.List of Task Apps
image::{dataflow-asciidoc}/images/dataflow-task-apps-list.png[List of Task Apps, scaledwidth="100%"]

On this screen, you can perform the following actions:

* View details, such as the task app options.
* Create a task definition from the respective app.

==== View Task App Details

On this page you can view the details of a selected task app, including the list of available options (properties) for that app.

==== Create a Task Definition

image::{dataflow-asciidoc}/images/dataflow-task-definition-create.png[List of Task Apps, scaledwidth="100%"]

At a minimum, you must provide a name for the new definition.
You also have the option to specify various properties that are used during the deployment of the app.

NOTE: Each parameter is included only if the Include checkbox is selected.

[[dashboard-task-definition]]
=== Definitions

This page lists the Data Flow task definitions and provides actions to launch or destroy those tasks.
It also provides a shortcut operation to define one or more tasks with simple textual input, indicated by
the Bulk Define Tasks button.

The following image shows the Definitions page:

.List of Task Definitions
image::{dataflow-asciidoc}/images/dataflow-task-definitions-list.png[List of Task Definitions, scaledwidth="100%"]

==== Creating Composed Task Definitions

The dashboard includes the Create Composed Task tab, which provides an interactive graphical interface for creating composed tasks.

In this tab, you can:

* Create and visualize composed tasks using DSL, a graphical canvas, or both.
* Use auto-adjustment and grid-layout capabilities in the GUI for simpler and interactive organization of the composed task.

On the Create Composed Task screen, you can define one or more task parameters by entering both the parameter key and the parameter value.

NOTE: Task parameters are not typed.

The following image shows the composed task designer:

.Composed Task Designer
image::{dataflow-asciidoc}/images/dataflow-ctr-flo-tab.png[Composed Task Designer, scaledwidth="100%"]



==== Launching Tasks

Once the task definition has been created, the tasks can be launched through the dashboard.
To do so, click the Definitions tab and select the task you want to launch by pressing `Launch`.



[[dashboard-tasks-executions]]
=== Executions

The Executions tab shows the current running and completed tasks.

The following image shows the Executions tab:

.List of Task Executions
image::{dataflow-asciidoc}/images/dataflow-task-executions-list.png[List of Task Executions, scaledwidth="100%"]

[[dashboard-tasks-execution-detail]]
=== Execution Detail
For each task execution on the Executions page, a user can retrieve detailed information about a specific execution by clicking the information icon located to the right of the task execution.

image::{dataflow-asciidoc}/images/dataflow-task-execution-detail.png[List of Task Executions, scaledwidth="100%"]


On this screen the user can view not only the information from the Task Executions page but also:

* Task Arguments
* External Execution Id
* Batch Job Indicator (indicates if the task execution contained Spring Batch jobs.)
* Job Execution Ids links (Clicking the Job Execution Id will take you to the <<dashboard-job-executions-details>> for that Job Execution Id.)
* Task Execution Duration
* Task Execution Exit Message

[[dashboard-jobs]]
== Jobs

The Jobs section of the Dashboard lets you inspect batch jobs.
The main section of the screen provides a list of job executions.
Batch jobs are tasks that each execute one or more batch jobs.
Each job execution has a reference to the task execution ID (in the Task Id column).

The list of Job Executions also shows the state of the underlying Job Definition.
Thus, if the underlying definition has been deleted, "`No definition found`" appears in the Status column.

You can take the following actions for each job:

* Restart (for failed jobs).
* Stop (for running jobs).
* View execution details.

Note: Clicking the stop button actually sends a stop request to the running job, which may not immediately stop.

The following image shows the Jobs page:

.List of Job Executions
image::{dataflow-asciidoc}/images/dataflow-job-executions-list.png[List of Job Executions, scaledwidth="100%"]



[[dashboard-job-executions-details]]
=== Job Execution Details

After having launched a batch job, the Job Execution Details page will show information about the job.

The following image shows the Job Execution Details page:

.Job Execution Details
image::{dataflow-asciidoc}/images/dataflow-jobs-job-execution-details.png[Job Execution Details, scaledwidth="100%"]

The Job Execution Details page contains a list of the executed steps.
You can further drill into the details of each step's execution by clicking the magnifying glass icon.



[[dashboard-job-executions-steps]]
=== Step Execution Details

The Step Execution Details page provides information about an individual step within a job.

The following image shows the Step Execution Details page:

.Step Execution Details
image::{dataflow-asciidoc}/images/dataflow-step-execution-history.png[Step Execution History, scaledwidth="100%"]

On the top of the page, you can see a progress indicator the respective step, with the option to refresh the indicator.
A link is provided to view the step execution history.

The Step Execution Details screen provides a complete list of all Step Execution Context key/value pairs.

IMPORTANT: For exceptions, the Exit Description field contains additional error information.
However, this field can have a maximum of 2500 characters.
Therefore, in the case of long exception stack traces, trimming of error messages may occur.
When that happens, refer to the server log files for further details.



[[dashboard-job-executions-steps-progress]]
=== Step Execution Progress

On this screen, you can see a progress bar indicator in regards to the execution
of the current step. Under the Step Execution History, you can also view various
metrics associated with the selected step, such as duration, read counts, write
counts, and others.

endif::omit-tasks-docs[]

[[dashboard-task-scheduling]]
== Scheduling

=== Creating or deleting a Schedule from the Task Definition's page
From the Task Definitions page a user can create or delete a schedule for a specific task definition.

.Task Definitions with Schedule controls
image::{dataflow-asciidoc}/images/dataflow-scheduling-task-definition.png[Task Definitions with Schedule Controls, scaledwidth="50%"]

On this screen you can perform the following actions:

* The user can click the clock icon and this will take you to the Schedule Creation screen.
* The user can click the clock icon with the `x` to the upper right to delete the schedule(s) associated with the task definition.

=== Creating a Schedule
.Create Schedule for Task Execution
image::{dataflow-asciidoc}/images/dataflow-scheduling-create.png[Create Schedule for Task Execution, scaledwidth="50%"]

Once the user clicks the clock icon on the Task Definition screen, Spring Cloud Data Flow will take the user to the Schedule Creation screen.
On this screen a user can establish the schedule name, the cron expression as well as establish the properties and arguments to be used when the task is launched by this schedule.

=== Listing Available Schedules

.List Available Schedules
image::{dataflow-asciidoc}/images/dataflow-scheduling-list.png[List Available Schedules, scaledwidth="50%"]

On this screen you can perform the following actions:

* Delete a schedule
* Get details for a schedule

[[dashboard-auditing]]
== Auditing

The Auditing page of the Dashboard gives you access to recorded audit events. Currently audit event
are recorded for:

* Streams
  - Create
  - Delete
  - Deploy
  - Undeploy
* Tasks
  - Create
  - Delete
  - Launch
* Scheduling of Tasks
  - Create Schedule
  - Delete Schedule

.List Overview of Audit Records
image::{dataflow-asciidoc}/images/dataflow-audit-records-list.png[List Available Audit Records, scaledwidth="100%"]

By clicking on the _Show Details_ icon, you can obtain further details regarding
the auditing details:

.List Details of an Audit Record
image::{dataflow-asciidoc}/images/dataflow-audit-records-details.png[Details of a single Audit Record, scaledwidth="100%"]

Generally, auditing provides the following information:

* When was the record created?
* Username that triggered the audit event (if security is enabled)
* Audit operation (Schedule, Stresm, Task)
* Performed action (Create, Delete, Deploy, Rollback, Undeploy, Update)
* Correlation Id, e.g. Stream/Task name
* Audit Data

The written value of the property _Audit Data_ depends on the performed _Audit Operation_ and the _ActionType_.
For example, when a Schedule is being created, the name of the task definition,
task definition properties, deployment properties, as well as command line arguments are written
to the persistence store.

Sensitive information is sanitized prior to saving the Audit Record, in an _best-effort-manner_.
Any of the following keys are being detected and its sensitive values are
masked:

- password
- secret
- key
- token
- .\*credentials.*
- vcap_services

