[[architecture]]
= Architecture

[[arch-intro]]
== Introduction

Spring Cloud Data Flow simplifies the development and deployment of applications focused on data processing use cases.
The major concepts of the architecture are Applications, the Data Flow Server, and the target runtime.

ifdef::omit-tasks-docs[]
Applications are long-lived Stream applications where an unbounded amount of data is consumed or produced through messaging middleware.
endif::omit-tasks-docs[]
ifndef::omit-tasks-docs[]
Applications come in two flavors:

* Long-lived applications.  These applications can be part of a `Stream` in which case an unbounded amount of data is consumed or produced through messaging middleware.  Alternatively, they can be arbitrary applications that are long lived and do not necessarily use messaging middleware.
* Short-lived `Task` applications that process a finite set of data and then terminate.
endif::omit-tasks-docs[]

Depending on the runtime, applications can be packaged in two ways:

* Spring Boot uber-jar that is hosted in a maven repository, file, or HTTP(S).
* Docker image.

The runtime is the place where applications execute.
The target runtimes for applications are platforms that you may already be using for other application deployments.

The supported platforms are:

* Cloud Foundry
* Kubernetes
* Local Server

NOTE: The local server is supported in production for Task deployment as a replacement for the Spring Batch Admin project.  The local server is not supported in production for Stream deployments.

There is a deployer Service Provider Interface (SPI) that lets you extend Data Flow to deploy onto other runtimes.
There are community implementations

* https://github.com/donovanmuller/spring-cloud-dataflow-server-nomad[HashiCorp Nomad]
* https://github.com/donovanmuller/spring-cloud-dataflow-server-openshift[OpenShift]
* https://github.com/trustedchoice/spring-cloud-dataflow-server-mesos[Apache Mesos]

The https://github.com/spring-cloud/spring-cloud-dataflow-server-yarn[Apache YARN] implementation has reached end-of-line status.
Let us know at https://gitter.im/spring-cloud/spring-cloud-dataflow[Gitter] if youare interested in forking the project to continue developing and maintaining it.

The Spring Cloud Data Flow Server delegates the deployment and runtime status of stream applications to the Spring Cloud Skipper Server.
Later has the capabilities to update and rollback applications in a Stream at runtime as well as the ability to deploy applications to multiple platforms.

The Data Flow server is also responsible for:

* Interpreting and executing a stream DSL that describes the logical flow of data through multiple long-lived applications.
* Launching a long-lived task application.
* Interpreting and executing a composed task DSL that describes the logical flow of data through multiple short-lived applications.
* Applying a deployment manifest that describes the mapping of applications onto the runtime - for example, to set the initial number of instances, memory requirements, and data partitioning.
* Providing the runtime status of deployed applications.

As an example, the stream DSL to describe the flow of data from an HTTP source to an Apache Cassandra sink would be written using a Unix pipes and filter syntax " `http | cassandra` ".
Each name in the DSL is mapped to an application that can that Maven or Docker repositories.
You can also register an application to an `http` location.
Many source, processor, and sink applications for common use cases (such as JDBC, HDFS, HTTP, and router) are provided by the Spring Cloud Data Flow team.
The pipe symbol represents the communication between the two applications through messaging middleware.
The messaging middleware that you can use is based on what the https://cloud.spring.io/spring-cloud-stream/#binder-implementations[Spring Cloud Stream project] supports, namely:

* https://github.com/spring-cloud/spring-cloud-stream-binder-rabbit[RabbitMQ]
* https://github.com/spring-cloud/spring-cloud-stream-binder-kafka[Apache Kafka / Kafka Streams]
* https://github.com/spring-cloud/spring-cloud-stream-binder-aws-kinesis[Amazon Kinesis]
* https://github.com/spring-cloud/spring-cloud-gcp/tree/master/spring-cloud-gcp-pubsub-stream-binder[Google PubSub (partner maintained)]
* https://github.com/SolaceProducts/spring-cloud-stream-binder-solace[Solace PubSub+ (partner maintained)]
* https://github.com/Microsoft/spring-cloud-azure/tree/master/spring-cloud-azure-eventhub-stream-binder[Azure Event Hubs (partner maintained)]

<<applications,Pre-built Stream applications>> are available for RabbitMQ and Apache Kafka.
You can easily modify these pre-built application using the https://start-scs.cfapps.io/[Spring Intializr for Stream applications].
The Data Flow server is responsible for creating the destinations/topics that correspond to each pipe symbol and configure each application to produce or consume from the topics so the desired flow of data is achieved.

The interaction of the main components for the three types of DSLs are shown in the following images.

.The Spring Cloud Data Flow Stream DSL Architecure
image::{dataflow-asciidoc}/images/arch-stream-dsl.png[The Spring Cloud Data Flow Streaming Application Architecture, scaledwidth="60%"]

In this architecture, the Stream DSL is POSTed to the Data Flow Server.
Based on the mapping of DSL application names to Maven and Docker artifacts, the http-source and cassandra-sink applications are deployed on the target runtime.
Data that is posted to the HTTP application will then be stored in Cassandra.
The https://docs.spring.io/spring-cloud-dataflow-samples/docs/current/reference/html/_streaming.html#spring-cloud-data-flow-samples-http-cassandra-overview[Samples Repository] shows this use case in full detail.

The architecture for the Application DSL is similar.


.The Spring Cloud Data Flow Application DSL Architecure
image::{dataflow-asciidoc}/images/arch-app-dsl.png[The Spring Cloud Data Flow Application DSL Architecture, scaledwidth="60%"]

In this architecture, the Application DSL describes a group of related applications is POSTed to the Data Flow Server.
Instead of a pipe symbol separating the applications, a double pipe symbol is used.
As compared to the case of a single pipe, when a double pipe symbol is used the Data Flow server does not configure the applications to connect to each other over a single messaging middleware destination.
Instead, deployment properties (not shown) set by the user are used to configure applications to have multiple producing or consuming messaging middleware destinations.
In this example, the `baristaApp` sends messages on either the `hotDrinks` destination or the `coldDrinks` destination.
It is also possible to deploy applications using the Application DSL that do not involve the use of messaging middleware.


The architecture for the Task DSL does not involve the Skipper server.

.The Spring Cloud Data Flow Task DSL Architecure
image::{dataflow-asciidoc}/images/arch-task-dsl.png[The Spring Cloud Data Flow Task DSL Architecture, scaledwidth="60%"]

In this architecture, the Task DSL is POSTed to the Data Flow Server.
It describes a sequence of steps to execute with the double ampersand symbol denoting the step separation.

In all cases, the target platform can be either Cloud Foundry or Kubernetes.
Similarly, the Data Flow Server and Skipper Server can run on either Cloud Foundry or Kubernetes.

[[arch-microservice-style]]
== Microservice Architectural Style

The Data Flow Server deploys applications onto the target runtime that conform to the microservice architectural style.  For example, a stream represents a high-level application that consists of multiple small microservice applications each running in their own process.  Each microservice application can be scaled up or down independently of the other and each has its own versioning lifecycle.  Using Data Flow with Skipper enables you to independently upgrade or rollback each application at runtime.

ifdef::omit-tasks-docs[]
Streaming based microservice applications build upon Spring Boot as the foundational library.
endif::omit-tasks-docs[]
ifndef::omit-tasks-docs[]
Both Streaming and Task-based microservice applications build upon Spring Boot as the foundational library.
endif::omit-tasks-docs[]
This gives all microservice applications functionality such as health checks, security, configurable logging, monitoring, and management functionality, as well as executable JAR packaging.

It is important to emphasize that these microservice applications are 'just apps' that you can run by yourself by using `java -jar` and passing in appropriate configuration properties.  We provide many common microservice applications for common operations so you need not start from scratch when addressing common use cases that build upon the rich ecosystem of Spring Projects, such as Spring Integration, Spring Data, and Spring Batch.  Creating your own microservice application is similar to creating other Spring Boot applications. You can start by using the https://start.spring.io[Spring Initializr web site] to create the basic scaffolding of either a Stream or Task-based microservice.

In addition to passing the appropriate application properties to each applications, the Data Flow server is responsible for preparing the target platform's infrastructure so that the applications can be deployed.  For example, in Cloud Foundry, it would bind specified services to the applications and execute the `cf push` command for each application.  For Kubernetes, it would create the replication controller, service, and load balancer.

The Data Flow Server helps simplify the deployment of multiple, relatated, applications onto a target runtime, setting up necessary input and output topics, partitions, and metrics functionality.  However, one could also opt to deploy each of the microservice applications manually and not use Data Flow at all. This approach might be more appropriate to start out with for small scale deployments, gradually adopting the convenience and consistency of Data Flow as you develop more applications.
ifdef::omit-tasks-docs[]
Manual deployment of Stream-based microservices is also a useful educational exercise that can help you better understand some of the automatic application configuration and platform targeting steps that the Data Flow Server provides.
endif::omit-tasks-docs[]
ifndef::omit-tasks-docs[]
Manual deployment of Stream- and Task-based microservices is also a useful educational exercise that can help you better understand some of the automatic application configuration and platform targeting steps that the Data Flow Server provides.
endif::omit-tasks-docs[]

[[arch-comparison]]
=== Comparison to Other Platform Architectures

Spring Cloud Data Flow’s architectural style is different than other Stream and Batch processing platforms.  For example in Apache Spark, Apache Flink, and Google Cloud Dataflow, applications run on a dedicated compute engine cluster.  The nature of the compute engine gives these platforms a richer environment for performing complex calculations on the data as compared to Spring Cloud Data Flow, but it introduces the complexity of another execution environment that is often not needed when creating data-centric applications.  That does not mean you cannot do real-time data computations when using Spring Cloud Data Flow. Spring Cloud Stream also supports using Reactive APIs such as https://docs.spring.io/spring-cloud-stream/docs/current/reference/htmlsingle/#_reactive_programming_support[Project Reactor and RxJava] which can be useful for creating functional style applications that contain time-sliding-window and moving-average functionality.  Similarly, Spring Cloud Stream also supports the development of applications in that use the https://docs.spring.io/spring-cloud-stream/docs/current/reference/htmlsingle/#_kafka_streams_binding_capabilities_of_spring_cloud_stream[Kafka Streams] API.

Apache Storm, Hortonworks DataFlow, and Spring Cloud Data Flow’s predecessor, Spring XD, use a dedicated application execution cluster, unique to each product, that determines where your code should run on the cluster and performs health checks to ensure that long-lived applications are restarted if they fail.  Often, framework-specific interfaces are required in order to correctly “plug in” to the cluster’s execution framework.

As we discovered during the evolution of Spring XD, the rise of multiple container frameworks in 2015 made creating our own runtime a duplication of effort.  There is no reason to build your own resource management mechanics when there are multiple runtime platforms that offer this functionality already.  Taking these considerations into account is what made us shift to the current architecture, where we delegate the execution to popular runtimes, which you may already be using for other purposes.  This is an advantage in that it reduces the cognitive distance for creating and managing data-centric applications as many of the same skills used for deploying other end-user/web applications are applicable.


[[arch-data-flow-server]]
== Data Flow Server

The Data Flow Server provides the following functionality:

* <<arch-data-flow-server-endpoints>>
* <<arch-data-flow-server-security>>

[[arch-data-flow-server-endpoints]]
=== Endpoints

The Data Flow Server uses an embedded servlet container and exposes REST endpoints for creating, deploying, undeploying, and destroying streams and tasks, querying runtime state and the like. The Data Flow Server is implemented by using Spring’s MVC framework and the link:https://github.com/spring-projects/spring-hateoas[Spring HATEOAS] library to create REST representations that follow the HATEOAS principle, as shown in the following image:

.The Spring Cloud Data Flow Server
image::{dataflow-asciidoc}/images/dataflow-server-arch.png[The Spring Cloud Data Flow Server Architecture, scaledwidth="100%"]

[NOTE] The Data Flow Server that deploys applications to the  local machine is not intended to be used in production for streaming use cases but for the development and testing of stream based applications.
The local Data Flow  *is* intended to be used in production for batch use cases as a replacement for the Spring Batch Admin project.
Both streaming and batch use cases are intended to be used in production when deploying to Cloud Foundry or Kuberenetes.

[[arch-data-flow-server-security]]
=== Security

The Data Flow Server executable jars use https://oauth.net/2/[OAuth 2.0] authentication
to secure the relevant REST endpoints. These can be accessed using either
_Basic Authentication_ or using _OAuth2 Access Tokens_. As OAuth provider we recommend the
https://github.com/cloudfoundry/uaa[CloudFoundry User Account and Authentication (UAA) Server]
which also provides comprehensive LDAP support. Please refer to the
<<configuration-local-security,security section>> for more information.

[[arch-streams]]
== Streams

[[arch-streams-topologies]]
=== Topologies
The Stream DSL describes linear sequences of data flowing through the system.  For example, in the stream definition `http | transformer | cassandra`, each pipe symbol connects the application on the left to the one on the right.  Named channels can be used for routing and to fan in/fan out data to multiple messaging destinations.

The concept of a <<spring-cloud-dataflow-stream-dsl-tap,tap>> can be used to ‘listen’ to the data that is flowing across any of the pipe symbols. "Taps" are just other streams that use an input any one of the "pipes" in a target stream and have an independent life cycle from the target stream.

[[arch-streams-concurrency]]
=== Concurrency
For an application that consumes events, Spring Cloud Stream exposes a concurrency setting that controls the size of a thread pool used for dispatching incoming messages.  See the {spring-cloud-stream-docs}#_consumer_properties[Consumer properties] documentation for more information.

[[arch-streams-partitioning]]
=== Partitioning
A common pattern in stream processing is to partition the data as it moves from one application to the next.  Partitioning is a critical concept in stateful processing, for either performance or consistency reasons, to ensure that all related data is processed together. For example, in a time-windowed average calculation example, it is important that all measurements from any given sensor are processed by the same application instance.  Alternatively, you may want to cache some data related to the incoming events so that it can be enriched without making a remote procedure call to retrieve the related data.

Spring Cloud Data Flow supports partitioning by configuring Spring Cloud Stream's output and input bindings.  Spring Cloud Stream provides a common abstraction for implementing partitioned processing use cases in a uniform fashion across different types of middleware.  Partitioning can thus be used whether the broker itself is naturally partitioned (for example, Kafka topics) or not (RabbitMQ).  The following image shows how data could be partitioned into two buckets, such that each instance of the average processor application consumes a unique set of data.

.Spring Cloud Stream Partitioning
image::{dataflow-asciidoc}/images/stream-partitioning.png[Stream Partitioning Architecture, scaledwidth="50%"]

To use a simple partitioning strategy in Spring Cloud Data Flow, you need only set the instance count for each application in the stream and a `partitionKeyExpression` producer property when deploying the stream.  The `partitionKeyExpression` identifies what part of the message is used as the key to partition data in the underlying middleware.  An `ingest` stream can be defined as `http | averageprocessor | cassandra`. (Note that the Cassandra sink is not shown in the diagram above.)  Suppose the payload being sent to the HTTP source was in JSON format and had a field called `sensorId`. For example, consider the case of deploying the stream with the shell command `stream deploy ingest --propertiesFile ingestStream.properties` where the contents of the `ingestStream.properties` file are as follows:

[source,bash]
----
deployer.http.count=3
deployer.averageprocessor.count=2
app.http.producer.partitionKeyExpression=payload.sensorId
----
The result is to deploy the stream such that all the input and output destinations are configured for data to flow through the applications but also ensure that a unique set of data is always delivered to each `averageprocessor` instance.  In this case, the default algorithm is to evaluate `payload.sensorId % partitionCount` where the `partitionCount` is the application count in the case of RabbitMQ and the partition count of the topic in the case of Kafka.

Please refer to <<passing_stream_partition_properties>> for additional strategies to partition streams during deployment and how they map onto the underlying {spring-cloud-stream-docs}#_partitioning[Spring Cloud Stream Partitioning properties].

Also note that you cannot currently scale partitioned streams.  Read <<arch-runtime-scaling>> for more information.

[[arch-streams-delivery]]
=== Message Delivery Guarantees

Streams are composed of applications that use the Spring Cloud Stream library as the basis for communicating with the underlying messaging middleware product.  Spring Cloud Stream also provides an opinionated configuration of middleware from several vendors, in particular providing {spring-cloud-stream-docs}#_persistent_publish_subscribe_support[persistent publish-subscribe semantics].

The {spring-cloud-stream-docs}#_binders[Binder abstraction] in Spring Cloud Stream is what connects the application to the middleware.  There are several configuration properties of the binder that are portable across all binder implementations and some that are specific to the middleware.

For consumer applications, there is a retry policy for exceptions generated during message handling. The retry policy is configured by using the {spring-cloud-stream-docs}#_consumer_properties[common consumer properties] `maxAttempts`, `backOffInitialInterval`, `backOffMaxInterval`, and `backOffMultiplier`.  The default values of these properties retry the callback method invocation 3 times and wait one second for the first retry.  A backoff multiplier of 2 is used for the second and third attempts.

When the number of retry attempts has exceeded the `maxAttempts` value, the exception and the failed message become the payload of a message and are sent to the application's error channel. By default, the default message handler for this error channel logs the message. You can change the default behavior in your application by creating your own message handler that subscribes to the error channel.

Spring Cloud Stream also supports a configuration option for both Kafka and RabbitMQ binder implementations that sends the failed message and stack trace to a dead letter queue.  The dead letter queue is a destination and its nature depends on the messaging middleware (for example, in the case of Kafka, it is a dedicated topic).  To enable this for RabbitMQ set the `republishtoDlq` and `autoBindDlq` {spring-cloud-stream-docs}#_rabbitmq_consumer_properties[consumer properties] and the `autoBindDlq` {spring-cloud-stream-docs}#_rabbit_producer_properties[producer property] to true when deploying the stream.  To always apply these producer and consumer properties when deploying streams, configure them as <<spring-cloud-dataflow-global-properties,common application properties>> when starting the Data Flow server.

Additional messaging delivery guarantees are those provided by the underlying messaging middleware that is chosen for the application for both producing and consuming applications.  Refer to the Kafka {spring-cloud-stream-docs}#_kafka_consumer_properties[Consumer] and {spring-cloud-stream-docs}#_kafka_producer_properties[Producer] and Rabbit {spring-cloud-stream-docs}#_rabbitmq_consumer_properties[Consumer] and {spring-cloud-stream-docs}#_rabbit_producer_properties[Producer] documentation for more details.  You can find extensive declarative support for all the native QOS options.


[[arch-streaming-apps]]
== Stream Programming Models

While Spring Boot provides the foundation for creating DevOps-friendly microservice applications, other libraries in the Spring ecosystem help create Stream-based microservice applications.  The most important of these is Spring Cloud Stream.

The essence of the Spring Cloud Stream programming model is to provide an easy way to describe multiple inputs and outputs of an application that communicate over messaging middleware. These input and outputs map onto Kafka topics or Rabbit exchanges and queues as well as the KStream/KTable programming model.  Common application configuration for a Source that generates data, a Processor that consumes and produces data, and a Sink that consumes data is provided as part of the library.

[[arch-streaming-imperative-programming]]
=== Imperative Programming Model

Spring Cloud Stream is most closely integrated with Spring Integration’s imperative "one event at a time" programming model.  This means you write code that handles a single event callback, as shown in the following example,

[source,java]
----
@EnableBinding(Sink.class)
public class LoggingSink {

    @StreamListener(Sink.INPUT)
    public void log(String message) {
        System.out.println(message);
    }
}
----

In this case, the `String` payload of a message coming on the input channel is handed to the `log` method.  The `@EnableBinding` annotation is used to tie the input channel to the external middleware.

[[arch-streaming-functional-programming]]
=== Functional Programming Model

However, Spring Cloud Stream can support other programming styles, such as reactive APIs, where incoming and outgoing data is handled as continuous data flows and how each individual message should be handled is defined. With many reactive AOIs, you can also use operators that describe functional transformations from inbound to outbound data flows.
Here is an example:

[source,java]
----
@EnableBinding(Processor.class)
public static class UppercaseTransformer {

  @StreamListener
  @Output(Processor.OUTPUT)
  public Flux<String> receive(@Input(Processor.INPUT) Flux<String> input) {
    return input.map(s -> s.toUpperCase());
  }
}
----


[[arch-application-versioning]]
== Application Versioning
Application versioning within a Stream is now supported when using Data Flow together with Skipper.  You can update application and deployment properties as well as the version of the application.
Rolling back to a previous application version is also supported.

ifndef::omit-tasks-docs[]
[[arch-task]]
== Task Programming Model

The Spring Cloud Task programming model provides:

* Persistence of the Task’s lifecycle events and exit code status.
* Lifecycle hooks to execute code before or after a task execution.
* The ability to emit task events to a stream (as a source) during the task lifecycle.
* Integration with Spring Batch Jobs.

See the <<spring-cloud-dataflow-task,Tasks>> section for more information.

endif::omit-tasks-docs[]

[[arch-runtime]]
== Runtime

The Data Flow Server relies on the target platform for the following runtime functionality:

* <<arch-runtime-fault-tolerance>>
* <<arch-runtime-resource-management>>

[[arch-runtime-fault-tolerance]]
=== Fault Tolerance

The target runtimes supported by Data Flow all have the ability to restart a long-lived application. Spring Cloud Data Flow sets up health probes are required by the runtime environment when deploying the application.
You also have the ability to customize the health probes.

The collective state of all applications that make up the stream is used to determine the state of the stream. If an application fails, the state of the stream changes from ‘deployed’ to ‘partial’.

[[arch-runtime-resource-management]]
=== Resource Management
Each target runtime lets you control the amount of memory, disk, and CPU allocated to each application. These are passed as properties in the deployment manifest by using key names that are unique to each runtime. Refer to each platform's server documentation for more information.

[[arch-runtime-scaling]]
=== Scaling at Runtime

When deploying a stream, you can set the instance count for each individual application that makes up the stream.
Once the stream is deployed, each target runtime lets you control the target number of instances for each individual application.
Using the APIs, UIs, or command line tools for each runtime, you can scale up or down the number of instances as required.

Currently, scaling at runtime is not supported with the Kafka binder, as well as with partitioned streams, for which the suggested workaround is redeploying the stream with an updated number of instances.
Both cases require a static consumer to be set up, based on information about the total instance count and current instance index.


