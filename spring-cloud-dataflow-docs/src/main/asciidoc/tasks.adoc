[[spring-cloud-dataflow-task]]
= Tasks

[partintro]
--
This section goes into more detail about how you can work with https://cloud.spring.io/spring-cloud-task/[Spring Cloud Task].
It covers topics such as creating and running task applications.

If you are just starting out with Spring Cloud Data Flow, you should probably read the "`<<getting-started.adoc#getting-started, Getting Started>>`" guide before diving into this section.
--

[[spring-cloud-dataflow-task-intro]]
== Introduction

A task executes a process on demand.
In the case of Spring Cloud Task, a task is a https://projects.spring.io/spring-boot/[Spring Boot] application that is annotated with `@EnableTask`.
A user launches a task that performs a certain process, and, once complete, the task ends.  Unlike a stream where a stream definition can have at most one deployment a single task definition can be launched multiple times simultaneously.
An example of a task would be a Spring Boot application that exports data from a JDBC repository to an HDFS instance.
Tasks record the start time and the end time as well as the boot exit code in a relational database.
The task implementation is based on the https://cloud.spring.io/spring-cloud-task/[Spring Cloud Task] project.

=== Application properties

Each application takes properties to customize its behavior.  As an example, the `timestamp` task `format` setting establishes a output format that is different from the default value.

`dataflow:> task create --definition "timestamp --format=\"yyyy\"" --name printTimeStamp`

This `timestamp` property is actually the same as the `timestamp.format` property specified by the timestamp application.
Data Flow adds the ability to use the shorthand form `format` instead of `timestamp.format`.
One may also specify the longhand version as well, as shown in the following example:

`dataflow:> task create --definition "timestamp --timestamp.format=\"yyyy\"" --name printTimeStamp`

This shorthand behavior is discussed more in the section on <<spring-cloud-dataflow-stream-app-whitelisting>>.
If you have <<spring-cloud-dataflow-stream-app-metadata-artifact, registered application property metadata>> you can use tab completion in the shell after typing `--` to get a list of candidate property names.

The shell provides tab completion for application properties. The shell command `app info <appType>:<appName>` provides additional documentation for all the supported properties.

NOTE: The supported Task `<appType>` is task.


== The Lifecycle of a Task

Before we dive deeper into the details of creating Tasks, we need to understand the typical lifecycle for tasks in the context of Spring Cloud Data Flow:

. <<spring-cloud-dataflow-create-task-apps>>
. <<spring-cloud-dataflow-register-task-apps>>
. <<spring-cloud-dataflow-create-task-definition>>
. <<spring-cloud-dataflow-task-launch>>
. <<spring-cloud-dataflow-task-review-executions>>
. <<spring-cloud-dataflow-task-definition-destroying>>

[[spring-cloud-dataflow-create-task-apps]]
=== Creating a Task Application

While Spring Cloud Task does provide a number of out-of-the-box applications (at https://github.com/spring-cloud-task-app-starters[spring-cloud-task-app-starters]), most task applications require custom development.
  To create a custom task application:

.  Use the https://start.spring.io[Spring Initializer] to create a new project, making sure to select the following starters:
.. `Cloud Task`: This dependency is the `spring-cloud-starter-task`.
.. `JDBC`: This dependency is the `spring-jdbc` starter.
. Within your new project, create a new class to serve as your main class, as follows:
+
[source,java]
----
@EnableTask
@SpringBootApplication
public class MyTask {

    public static void main(String[] args) {
		SpringApplication.run(MyTask.class, args);
	}
}
----
+
. With this class, you need one or more `CommandLineRunner` or `ApplicationRunner` implementations within your application.  You can either implement your own or use the ones provided by Spring Boot (there is one for running batch jobs, for example).
. Packaging your application with Spring Boot into an über jar is done through the standard https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started-first-application.html#getting-started-first-application-executable-jar[Spring Boot conventions].
The packaged application can be registered and deployed as noted below.



==== Task Database Configuration

CAUTION: When launching a task application, be sure that the database driver that is being used by Spring Cloud Data Flow is also a dependency on the task application.
For example, if your Spring Cloud Data Flow is set to use Postgresql, be sure that the task application also has Postgresql as a dependency.

TIP: When you run tasks externally (that is, from the command line) and you want Spring Cloud Data Flow to show the TaskExecutions in its UI, be sure that common datasource settings are shared among the both.
By default, Spring Cloud Task uses a local H2 instance, and the execution is recorded to the database used by Spring Cloud Data Flow.



[[spring-cloud-dataflow-register-task-apps]]
=== Registering a Task Application

You can register a Task App with the App Registry by using the Spring Cloud Data Flow Shell `app register` command.
You must provide a unique name and a URI that can be resolved to the app artifact. For the type, specify "task".
The following listing shows three examples:

[source,bash]
----
dataflow:>app register --name task1 --type task --uri maven://com.example:mytask:1.0.2

dataflow:>app register --name task2 --type task --uri file:///Users/example/mytask-1.0.2.jar

dataflow:>app register --name task3 --type task --uri https://example.com/mytask-1.0.2.jar
----

When providing a URI with the `maven` scheme, the format should conform to the following:

`maven://<groupId>:<artifactId>[:<extension>[:<classifier>]]:<version>`

If you would like to register multiple apps at one time, you can store them in a properties file where the keys are formatted as `<type>.<name>` and the values are the URIs.
For example, the followinng listing would be a valid properties file:

[source]
task.foo=file:///tmp/foo.jar
task.bar=file:///tmp/bar.jar


Then you can use the `app import` command and provide the location of the properties file by using the  `--uri` option, as follows:

`app import --uri file:///tmp/task-apps.properties`

For convenience, we have the static files with application-URIs (for both maven and docker) available for all the out-of-the-box Task app-starters.
You can point to this file and import all the application-URIs in bulk.
Otherwise, as explained earlier in this chapter, you can register them individually or have your own custom property file with only the required application-URIs in it.
It is recommended, however, to have a "`focused`" list of desired application-URIs in a custom property file.

The following table lists the available static property files:

[width="100%",frame="topbot",options="header"]
|======================
|Artifact Type |Stable Release |SNAPSHOT Release
|Maven   | https://bit.ly/Clark-GA-task-applications-maven | https://bit.ly/Clark-BUILD-SNAPSHOT-task-applications-maven
|Docker  | https://bit.ly/Clark-GA-task-applications-docker | https://bit.ly/Clark-BUILD-SNAPSHOT-task-applications-docker
|======================

For example, if you would like to register all out-of-the-box task applications in bulk, you can do so with the following command:

`dataflow:>app import --uri https://bit.ly/Clark-GA-task-applications-maven`

You can also pass the `--local` option (which is `TRUE` by default) to indicate whether the properties file location should be resolved within the shell process itself.
If the location should be resolved from the Data Flow Server process, specify `--local false`.

When using either `app register` or `app import`, if a task app is already registered with
the provided name, it is not overridden by default. If you would like to override the
pre-existing task app, then include the `--force` option.

[NOTE]
In some cases, the Resource is resolved on the server side.
In other cases, the URI is passed to a runtime container instance where it is resolved.
Consult the specific documentation of each Data Flow Server for more detail.



[[spring-cloud-dataflow-create-task-definition]]
=== Creating a Task Definition

You can create a task Definition from a task app by providing a definition name as well as
properties that apply to the task execution.  Creating a task definition can be done through
the RESTful API or the shell.  To create a task definition by using the shell, use the
`task create` command to create the task definition, as shown in the following example:

[source]
dataflow:>task create mytask --definition "timestamp --format=\"yyyy\""
Created new task 'mytask'

A listing of the current task definitions can be obtained through the RESTful API or the shell.
To get the task definition list by using the shell, use the `task list` command.

[[spring-cloud-dataflow-task-launch]]
=== Launching a Task
An adhoc task can be launched through the RESTful API or the shell.
To launch an ad-hoc task through the shell, use the `task launch` command, as shown in the following example:

[source]
dataflow:>task launch mytask
 Launched task 'mytask'

When a task is launched, any properties that need to be passed as command line arguments to the task application can be set when launching the task, as follows:

`dataflow:>task launch mytask --arguments "--server.port=8080,--custom=value"`

Additional properties meant for a `TaskLauncher` itself can be passed in by using a `--properties` option.
The format of this option is a comma-separated string of properties prefixed with `app.<task definition name>.<property>`.
Properties are passed to `TaskLauncher` as application properties.
It is up to an implementation to choose how those are passed into an actual task application.
If the property is prefixed with `deployer` instead of `app`, it is passed to `TaskLauncher` as a deployment property and its meaning may be `TaskLauncher` implementation specific.

`dataflow:>task launch mytask --properties "deployer.timestamp.custom1=value1,app.timestamp.custom2=value2"`

==== Common application properties

In addition to configuration through DSL, Spring Cloud Data Flow provides a mechanism for setting common properties to all the task applications that are launched by it.
This can be done by adding properties prefixed with `spring.cloud.dataflow.applicationProperties.task` when starting the server.
When doing so, the server passes all the properties, without the prefix, to the instances it launches.

For example, all the launched applications can be configured to use the properties `prop1` and `prop2` by launching the Data Flow server with the following options:

[source]
--spring.cloud.dataflow.applicationProperties.task.prop1=value1
--spring.cloud.dataflow.applicationProperties.task.prop2=value2

This causes the properties, `prop1=value1` and `prop2=value2`, to be passed to all the launched applications.

[NOTE]
Properties configured by using this mechanism have lower precedence than task deployment properties.
They are overridden if a property with the same key is specified at task launch time (for example, `app.trigger.prop2`
overrides the common property).



[[spring-cloud-dataflow-task-review-executions]]
=== Reviewing Task Executions
Once the task is launched, the state of the task is stored in a relational DB.  The state
includes:

* Task Name
* Start Time
* End Time
* Exit Code
* Exit Message
* Last Updated Time
* Parameters

A user can check the status of their task executions through the RESTful API or the shell.
To display the latest task executions through the shell, use the `task execution list` command.

To get a list of task executions for just one task definition, add `--name` and
the task definition name, for example `task execution list --name foo`.  To retrieve full
details for a task execution use the `task execution status` command with the id of the task execution,
for example `task execution status --id 549`.



[[spring-cloud-dataflow-task-definition-destroying]]
=== Destroying a Task Definition
Destroying a Task Definition removes the definition from the definition repository.
This can be done through the RESTful API or the shell.
To destroy a task through the shell, use the `task destroy` command, as shown in the following example:

[source]
dataflow:>task destroy mytask
 Destroyed task 'mytask'

The task execution information for previously launched tasks for the definition remains in the task repository.

NOTE: This does not stop any currently executing tasks for this definition. Instead, it removes the task definition from the database.



[[spring-cloud-dataflow-task-events]]
== Subscribing to Task/Batch Events

You can also tap into various task and batch events when the task is launched.
If the task is enabled to generate task or batch events (with the additional dependencies `spring-cloud-task-stream` and, in the case of Kafka as the binder, `spring-cloud-stream-binder-kafka`), those events are published during the task lifecycle.
By default, the destination names for those published events on the broker (Rabbit, Kafka, and others) are the event names themselves (for instance: `task-events`, `job-execution-events`, and so on).

[source]
dataflow:>task create myTask --definition “myBatchJob"
dataflow:>task launch myTask
dataflow:>stream create task-event-subscriber1 --definition ":task-events > log" --deploy

You can control the destination name for those events by specifying explicit names when launching the task, as follows:

[source]
dataflow:>task launch myTask --properties "spring.cloud.stream.bindings.task-events.destination=myTaskEvents"
dataflow:>stream create task-event-subscriber2 --definition ":myTaskEvents > log" --deploy

The following table lists the default task and batch event and destination names on the broker:

.Task and Batch Event Destinations

[cols="2*"]
|===

|*Event*|*Destination*

|Task events
|`task-events`
|Job Execution events  |`job-execution-events`
|Step Execution events|`step-execution-events`
|Item Read events|`item-read-events`
|Item Process events|`item-process-events`
|Item Write events|`item-write-events`
|Skip events|`skip-events`
|===

[[spring-cloud-dataflow-composed-tasks]]
== Composed Tasks

Spring Cloud Data Flow lets a user create a directed graph where each node of the graph is a task application.
This is done by using the DSL for composed tasks.
A composed task can be created through the RESTful API, the Spring Cloud Data Flow Shell, or the Spring Cloud Data Flow UI.

=== Configuring the Composed Task Runner

Composed tasks are executed through a task application called the https://github.com/spring-cloud-task-app-starters/composed-task-runner[Composed Task Runner].



==== Registering the Composed Task Runner

By default, the Composed Task Runner application is not registered with Spring Cloud Data Flow.
Consequently, to launch composed tasks, we must first register the Composed
Task Runner as an application with Spring Cloud Data Flow, as follows:

`app register --name composed-task-runner --type task --uri maven://org.springframework.cloud.task.app:composedtaskrunner-task:<DESIRED_VERSION>`

You can also configure Spring Cloud Data Flow to use a different task definition name for the composed task runner.
This can be done by setting the `spring.cloud.dataflow.task.composedTaskRunnerName` property to the name of your choice.
You can then register the composed task runner application with the name you set by using that property.



==== Configuring the Composed Task Runner

The Composed Task Runner application has a `dataflow.server.uri` property that is used for validation and for launching child tasks.
This defaults to `http://localhost:9393`. If you run a distributed Spring Cloud Data Flow server, as you would if you deploy the server on Cloud Foundry, YARN, or Kubernetes, you need to provide the URI that can be used to access the server.
You can either provide this `dataflow.server.uri` property for the Composed Task Runner application when launching a composed task or you can provide a `spring.cloud.dataflow.server.uri` property for the Spring Cloud Data Flow server when it is started.
For the latter case, the `dataflow.server.uri` Composed Task Runner application property is automatically set when a composed task is launched.

In some cases, you may wish to execute an instance of the Composed Task Runner through the Task Launcher sink.
In that case, you must configure the Composed Task Runner to use the same datasource that the Spring Cloud Data Flow instance is using.
The datasource properties are set with the `TaskLaunchRequest` through the use of the `commandlineArguments` or the `environmentProperties` switches.
This is because the Composed Task Runner monitors the `task_executions` table to check the status of the tasks that it is running.
Using information from the table, it determines how it should navigate the graph.



=== The Lifecycle of a Composed Task

The lifecycle of a composed task has three parts:

* <<spring-cloud-data-flow-composed-task-creating>>
* <<spring-cloud-data-flow-composed-task-stopping>>
* <<spring-cloud-data-flow-composed-task-restarting>>



[[spring-cloud-data-flow-composed-task-creating]]
==== Creating a Composed Task

The DSL for the composed tasks is used when creating a task definition through the task create command, as shown in the following example:

[source]
dataflow:> app register --name timestamp --type task --uri maven://org.springframework.cloud.task.app:timestamp-task:<DESIRED_VERSION>
dataflow:> app register --name mytaskapp --type task --uri file:///home/tasks/mytask.jar
dataflow:> task create my-composed-task --definition "mytaskapp && timestamp"
dataflow:> task launch my-composed-task

In the preceding example, we assume that the applications to be used by our composed task have not been registered yet.
Consequently, in the first two steps, we register two task applications.
We then create our composed task definition by using the `task create` command.
The composed task DSL in the preceding example, when launched, runs mytaskapp and then runs the timestamp application.

But before we launch the `my-composed-task` definition, we can view what Spring Cloud Data Flow generated for us.
This can be done by executing the task list command, as shown (including its output) in the following example:

[source,bash,options="nowrap"]
----
dataflow:>task list
╔══════════════════════════╤══════════════════════╤═══════════╗
║        Task Name         │   Task Definition    │Task Status║
╠══════════════════════════╪══════════════════════╪═══════════╣
║my-composed-task          │mytaskapp && timestamp│unknown    ║
║my-composed-task-mytaskapp│mytaskapp             │unknown    ║
║my-composed-task-timestamp│timestamp             │unknown    ║
╚══════════════════════════╧══════════════════════╧═══════════╝
----

In the example, Spring Cloud Data Flow created three task definitions, one for each of the applications that makes up our composed task (`my-composed-task-mytaskapp` and `my-composed-task-timestamp`) as well as the composed task (`my-composed-task`) definition.
We also see that each of the generated names for the child tasks is made up of the name of the composed task and the name of the application, separated by a dash `-` (as in _my-composed-task_ `-` _mytaskapp_).



===== Task Application Parameters

The task applications that make up the composed task definition can also contain parameters, as shown in the following example:

`dataflow:> task create my-composed-task --definition "mytaskapp --displayMessage=hello && timestamp --format=YYYY"`



==== Launching a Composed Task
Launching a composed task is done the same way as launching a stand-alone task, as follows:

`task launch my-composed-task`

Once the task is launched, and assuming all the tasks complete successfully, you can see three task executions when executing a `task execution list`, as shown in the following example:

[source,bash,options="nowrap"]
----
dataflow:>task execution list
╔══════════════════════════╤═══╤════════════════════════════╤════════════════════════════╤═════════╗
║        Task Name         │ID │         Start Time         │          End Time          │Exit Code║
╠══════════════════════════╪═══╪════════════════════════════╪════════════════════════════╪═════════╣
║my-composed-task-timestamp│713│Wed Apr 12 16:43:07 EDT 2017│Wed Apr 12 16:43:07 EDT 2017│0        ║
║my-composed-task-mytaskapp│712│Wed Apr 12 16:42:57 EDT 2017│Wed Apr 12 16:42:57 EDT 2017│0        ║
║my-composed-task          │711│Wed Apr 12 16:42:55 EDT 2017│Wed Apr 12 16:43:15 EDT 2017│0        ║
╚══════════════════════════╧═══╧════════════════════════════╧════════════════════════════╧═════════╝
----

In the preceding example, we see that `my-compose-task` launched and that it also launched the other tasks in sequential order.
All of them executed successfully with `Exit Code` as `0`.

===== Passing properties to the child tasks

To set the properties for child tasks in a composed task graph at task launch time,
you would use the following format of `app.<composed task definition name>.<child task app name>.<property>`.
Using the following Composed Task definition as an example:

[source,bash]
----
dataflow:> task create my-composed-task --definition "mytaskapp  && mytimestamp"
----
To have mytaskapp display 'HELLO' and set the mytimestamp timestamp format to 'YYYY' for the Composed Task definition, you would use the following task launch format:
[source,bash]
----
task launch my-composed-task --properties "app.my-composed-task.mytaskapp.displayMessage=HELLO,app.my-composed-task.mytimestamp.timestamp.format=YYYY"
----

===== Exit Statuses

The following list shows how the Exit Status is set for each step (task) contained in the composed task following each step execution:

* If the `TaskExecution` has an `ExitMessage`, that is used as the `ExitStatus`.
* If no `ExitMessage` is present and the `ExitCode` is set to zero, then the `ExitStatus` for the step is `COMPLETED`.
* If no `ExitMessage` is present and the `ExitCode` is set to any non-zero number, the `ExitStatus` for the step is `FAILED`.



==== Destroying a Composed Task

The command used to destroy a stand-alone task is the same as the command used to destroy a composed task.
The only difference is that destroying a composed task also destroys the child tasks associated with it.
The following example shows the task list before and after using the `destroy` command:

[source,bash,options="nowrap"]
----
dataflow:>task list
╔══════════════════════════╤══════════════════════╤═══════════╗
║        Task Name         │   Task Definition    │Task Status║
╠══════════════════════════╪══════════════════════╪═══════════╣
║my-composed-task          │mytaskapp && timestamp│COMPLETED  ║
║my-composed-task-mytaskapp│mytaskapp             │COMPLETED  ║
║my-composed-task-timestamp│timestamp             │COMPLETED  ║
╚══════════════════════════╧══════════════════════╧═══════════╝
...
dataflow:>task destroy my-composed-task
dataflow:>task list
╔═════════╤═══════════════╤═══════════╗
║Task Name│Task Definition│Task Status║
╚═════════╧═══════════════╧═══════════╝
----



[[spring-cloud-data-flow-composed-task-stopping]]
==== Stopping a Composed Task
In cases where a composed task execution needs to be stopped, you can do so through the:

* RESTful API
* Spring Cloud Data Flow Dashboard

To stop a composed task through the dashboard, select the Jobs tab and click the Stop button next to the job execution that you want to stop.

The composed task run is stopped when the currently running child task completes.
The step associated with the child task that was running at the time that the composed task was stopped is marked as `STOPPED` as well as the composed task job execution.



[[spring-cloud-data-flow-composed-task-restarting]]
==== Restarting a Composed Task
In cases where a composed task fails during execution and the status of the composed task is `FAILED`, the task can be restarted.
You can do so through the:

* RESTful API
* The shell
* Spring Cloud Data Flow Dashboard


To restart a composed task through the shell, launch the task with the same parameters.
To restart a composed task through the dashboard, select the Jobs tab and click the Restart button next to the job execution that you want to restart.

NOTE: Restarting a Composed Task job that has been stopped (through the Spring Cloud Data Flow Dashboard or RESTful API) relaunches the `STOPPED` child task and then launches the remaining (unlaunched) child tasks in the specified order.



== Composed Tasks DSL

Composed tasks can be run in three ways:

* <<spring-cloud-data-flow-conditional-execution>>
* <<spring-cloud-data-flow-transitional-execution>>
* <<spring-cloud-data-flow-split-execution>>



[[spring-cloud-data-flow-conditional-execution]]
=== Conditional Execution

Conditional execution is expressed by using a double ampersand symbol (`&&`).
This lets each task in the sequence be launched only if the previous task
successfully completed, as shown in the following example:

`task create my-composed-task --definition "task1 && task2"`

When the composed task called `my-composed-task` is launched, it launches the task called `task1` and, if it completes successfully, then the task called `task2` is launched.
If `task1` fails, then `task2` does not launch.

You can also use the Spring Cloud Data Flow Dashboard to create your conditional execution, by using the designer to drag and drop applications that are required and connecting them together to create your directed graph, as shown in the following image:

.Conditional Execution
image::{dataflow-asciidoc}/images/dataflow-ctr-conditional-execution.png[Composed Task Conditional Execution, scaledwidth="50%"]

The preceding diagram is a screen capture of the directed graph as it being created by using the Spring Cloud Data Flow Dashboard.
You can see that are four components in the diagram that comprise a conditional execution:

* Start icon: All directed graphs start from this symbol.
There is only one.
* Task icon: Represents each task in the directed graph.
* End icon: Represents the termination of a directed graph.
* Solid line arrow: Represents the flow conditional execution flow between:
** Two applications.
** The start control node and an application.
** An application and the end control node.
* End icon: All directed graphs end at this symbol.

TIP: You can view a diagram of your directed graph by clicking the Detail button next to the composed task definition on the Definitions tab.



[[spring-cloud-data-flow-transitional-execution]]
=== Transitional Execution

The DSL supports fine- grained control over the transitions taken during the execution of the directed graph.
Transitions are specified by providing a condition for equality based on the exit status of the previous task.
A task transition is represented by the following symbol `-&gt;`.


==== Basic Transition

A basic transition would look like the following:

`task create my-transition-composed-task --definition "foo 'FAILED' -> bar 'COMPLETED' -> baz"`

In the preceding example, `foo` would launch, and, if it had an exit status of `FAILED`, the `bar` task would launch.
If the exit status of `foo` was `COMPLETED`, `baz` would launch.
All other statuses returned by `foo` have no effect, and the task would terminate normally.

Using the Spring Cloud Data Flow Dashboard to create the same " `basic transition` " would resemble the following image:

.Basic Transition
image::{dataflow-asciidoc}/images/dataflow-ctr-transition-basic.png[Composed Task Basic Transition, scaledwidth="50%"]

The preceding diagram is a screen capture of the directed graph as it being created in the Spring Cloud Data Flow Dashboard.
Notice that there are two different types of connectors:

* Dashed line: Represents transitions from the application to one of the possible destination applications.
* Solid line: Connects applications in a conditional execution or a connection between the application and a control node (start or end).

To create a transitional connector:

. When creating a transition, link the application to each possible destination by using the connector.
. Once complete, go to each connection and select it by clicking it.
. A bolt icon appears.
. Click that icon.
. Enter the exit status required for that connector.
. The solid line for that connector turns to a dashed line.



==== Transition With a Wildcard

Wildcards are supported for transitions by the DSL, as shown in the following:

`task create my-transition-composed-task --definition "foo 'FAILED' -> bar '*' -> baz"`

In the preceding example, `foo` would launch, and, if it had an exit status of `FAILED`, the `bar` task would launch.
For any exit status of `foo` other than `FAILED`, `baz` would launch.

Using the Spring Cloud Data Flow Dashboard to create the same "`transition with wildcard`" would resemble the following image:

.Basic Transition With Wildcard
image::{dataflow-asciidoc}/images/dataflow-ctr-transition-basic-wildcard.png[Composed Task Basic Transition with Wildcard, scaledwidth="50%"]



==== Transition With a Following Conditional Execution

A transition can be followed by a conditional execution so long as the wildcard
is not used, as shown in the following example:

`task create my-transition-conditional-execution-task --definition "foo 'FAILED' -> bar 'UNKNOWN' -> baz && qux && quux"`

In the preceding example, `foo` would launch, and, if it had an exit status of `FAILED`, the `bar` task would launch.
If `foo` had an exit status of `UNKNOWN`, `baz` would launch.
For any exit status of `foo` other than `FAILED` or `UNKNOWN`, `qux` would launch and, upon successful completion, `quux` would launch.

Using the Spring Cloud Data Flow Dashboard to create the same "`transition with conditional execution`" would resemble the following image:

.Transition With Conditional Execution
image::{dataflow-asciidoc}/images/dataflow-ctr-transition-conditional-execution.png[Composed Task Transition with Conditional Execution, scaledwidth="50%"]

NOTE: In this diagram we see the dashed line (transition) connecting the `foo` application to the target applications, but a solid line connecting the conditional executions between `foo`, `qux`, and  `quux`.



[[spring-cloud-data-flow-split-execution]]
=== Split Execution

Splits allow multiple tasks within a composed task to be run in parallel.
It is denoted by using angle brackets (`<>`) to group tasks and flows that are to be run in parallel.
These tasks and flows are separated by the double pipe `||` symbol, as shown in the following example:

`task create my-split-task --definition "<foo || bar || baz>"`

The preceding example above launches tasks `foo`, `bar` and `baz` in parallel.

Using the Spring Cloud Data Flow Dashboard to create the same "`split execution`" would resemble the following image:

.Split
image::{dataflow-asciidoc}/images/dataflow-ctr-split.png[Composed Task Split, scaledwidth="50%"]

With the task DSL, a user may also execute multiple split groups in succession, as shown in the following example:

`task create my-split-task --definition "<foo || bar || baz> && <qux || quux>"'

In the preceding example, tasks `foo`, `bar`, and `baz` are launched in parallel.
Once they all complete, then tasks `qux` and `quux` are launched in parallel.
Once they complete, the composed task ends.
However, if `foo`, `bar`, or `baz` fails, the split containing `qux` and `quux` does not launch.

Using the Spring Cloud Data Flow Dashboard to create the same "`split with multiple groups`" would resemble the following image:

.Split as a part of a conditional execution
image::{dataflow-asciidoc}/images/dataflow-ctr-multiple-splits.png[Composed Task Split, scaledwidth="50%"]

Notice that there is a `SYNC` control node that is inserted by the designer when
connecting two consecutive splits.

==== Split Containing Conditional Execution

A split can also have a conditional execution within the angle brackets, as shown in the following example:

`task create my-split-task --definition "<foo && bar || baz>"`

In the preceding example, we see that `foo` and `baz` are launched in parallel.
However, `bar` does not launch until `foo` completes successfully.

Using the Spring Cloud Data Flow Dashboard to create the same " `split containing conditional execution` " resembles the following image:

.Split with conditional execution
image::{dataflow-asciidoc}/images/dataflow-ctr-split-contains-conditional.png[Composed Task Split With Conditional Execution, scaledwidth="50%"]

[[spring-cloud-dataflow-launch-tasks-from-stream]]
== Launching Tasks from a Stream

You can launch a task from a stream by using one of the available `task-launcher` sinks. Currently the platforms supported by the `task-launcher` sinks are:

* https://github.com/spring-cloud-stream-app-starters/tasklauncher-local[local],
* https://github.com/spring-cloud-stream-app-starters/tasklauncher-cloudfoundry[Cloud Foundry]
* https://github.com/spring-cloud-stream-app-starters/tasklauncher-kubernetes[Kubernetes]
* https://github.com/spring-cloud-stream-app-starters/tasklauncher-yarn[Yarn]

CAUTION: `task-launcher-local` is meant for development purposes only.

A `task-launcher` sink expects a message containing a https://github.com/spring-cloud/spring-cloud-task/blob/master/spring-cloud-task-stream/src/main/java/org/springframework/cloud/task/launcher/TaskLaunchRequest.java[TaskLaunchRequest] object in its payload.
From the `TaskLaunchRequest` object, the `task-launcher` obtains the URI of the artifact to be launched, as well as the environment properties, command line arguments, deployment properties, and application name to be used by the task.

The https://github.com/spring-cloud-stream-app-starters/tasklauncher-local/blob/v1.2.0.RELEASE/spring-cloud-starter-stream-sink-task-launcher-local/README.adoc[task-launcher-local] can be added to the available sinks by executing the app register command, as follows (for the Rabbit Binder, in this case):

`app register --name task-launcher-local --type sink --uri maven://org.springframework.cloud.stream.app:task-launcher-local-sink-rabbit:jar:1.2.0.RELEASE`

In the case of a Maven-based task that is to be launched, the `task-launcher` application is responsible for downloading the artifact.
You *must* configure the `task-launcher` with the appropriate configuration of https://github.com/spring-cloud/spring-cloud-deployer/blob/master/spring-cloud-deployer-resource-maven/src/main/java/org/springframework/cloud/deployer/resource/maven/MavenProperties.java[Maven Properties], such as `--maven.remote-repositories.repo1.url=https://repo.spring.io/libs-milestone"` to resolve artifacts (in this case against a milestone repo).  Note that this repostory can be different than the one used to register the `task-launcher` application itself.



=== TriggerTask

One way to launch a task with the `task-launcher` is to use the https://github.com/spring-cloud-stream-app-starters/triggertask/blob/v1.2.0.RELEASE/spring-cloud-starter-stream-source-triggertask/README.adoc[triggertask] source.
The `triggertask` source emits a message with a `TaskLaunchRequest` object that contains the required launch information.
The `triggertask` can be added to the available sources by running the app register command, as follows (for the Rabbit Binder, in this case):

`app register --type source --name triggertask --uri maven://org.springframework.cloud.stream.app:triggertask-source-rabbit:1.2.0.RELEASE`

For example, to launch the timestamp task once every 60 seconds, the stream implementation would be as follows:

[source]
stream create foo --definition "triggertask --triggertask.uri=maven://org.springframework.cloud.task.app:timestamp-task:jar:1.2.0.RELEASE --trigger.fixed-delay=60 --triggertask.environment-properties=spring.datasource.url=jdbc:h2:tcp://localhost:19092/mem:dataflow,spring.datasource.username=sa | task-launcher-local --maven.remote-repositories.repo1.url=https://repo.spring.io/libs-release" --deploy

If you run `runtime apps`, you can find the log file for the task launcher sink.
By using the `tail` command on that file, you can find the log file for the launched tasks.
Setting of `triggertask.environment-properties` establishes the Data Flow Server's H2 Database as the database where the task executions will be recorded.
You can then see the list of task executions by using the shell command `task execution list`, as shown (with its output) in the following example:

[source,bash,options="nowrap"]
----
dataflow:>task execution list
╔════════════════════╤══╤════════════════════════════╤════════════════════════════╤═════════╗
║     Task Name      │ID│         Start Time         │          End Time          │Exit Code║
╠════════════════════╪══╪════════════════════════════╪════════════════════════════╪═════════╣
║timestamp-task_26176│4 │Tue May 02 12:13:49 EDT 2017│Tue May 02 12:13:49 EDT 2017│0        ║
║timestamp-task_32996│3 │Tue May 02 12:12:49 EDT 2017│Tue May 02 12:12:49 EDT 2017│0        ║
║timestamp-task_58971│2 │Tue May 02 12:11:50 EDT 2017│Tue May 02 12:11:50 EDT 2017│0        ║
║timestamp-task_13467│1 │Tue May 02 12:10:50 EDT 2017│Tue May 02 12:10:50 EDT 2017│0        ║
╚════════════════════╧══╧════════════════════════════╧════════════════════════════╧═════════╝
----



=== TaskLaunchRequest-transform

Another way to start a task with the `task-launcher` would be to create a stream by using the
https://github.com/spring-cloud-stream-app-starters/tasklaunchrequest-transform[Tasklaunchrequest-transform] processor to translate a message payload to a `TaskLaunchRequest`.

The `tasklaunchrequest-transform` can be added to the available processors by executing the app register command, as follows (for the Rabbit Binder, in this case):

`app register --type processor --name tasklaunchrequest-transform --uri maven://org.springframework.cloud.stream.app:tasklaunchrequest-transform-processor-rabbit:1.2.0.RELEASE`

The following example shows the creation of a task that includes the `tasklaunchrequest-transform`:

`stream create task-stream --definition "http --port=9000 | tasklaunchrequest-transform --uri=maven://org.springframework.cloud.task.app:timestamp-task:jar:1.2.0.RELEASE | task-launcher-local --maven.remote-repositories.repo1.url=https://repo.spring.io/libs-release"`



=== Launching a Composed Task From a Stream

A composed task can be launched with one of the `task-launcher` sinks as discussed <<spring-cloud-dataflow-launch-tasks-from-stream, here>>.
Since we use the `ComposedTaskRunner` directly, we need to set up the task definitions it uses prior to the creation of the composed task launching stream.
Suppose we wanted to create the following composed task definition: `AAA && BBB`.
The first step would be to create the task definitions, as shown in the following example:

[source]
----
task create AAA --definition "timestamp"
task create BBB --definition "timestamp"
----
Now that the task definitions we need for composed task definition are ready, we need to create a stream that launches `ComposedTaskRunner`.
So, in this case, we create a stream with

* A trigger that emits a message once every 30 seconds
* A transformer that creates a `TaskLaunchRequest` for each message received
* A `task-launcher-local` sink that launches a the `ComposedTaskRunner` on our local machine

The stream should resemble the following:

[source]
----
stream create ctr-stream --definition "time --fixed-delay=30 | tasklaunchrequest-transform --uri=maven://org.springframework.cloud.task.app:composedtaskrunner-task:<current release> --command-line-arguments='--graph=AAA&&BBB --increment-instance-enabled=true --spring.datasource.url=...' | task-launcher-local"
----
In the preceding example, we see that the `tasklaunchrequest-transform` is establishing two primary components:

* *uri*: The URI of the `ComposedTaskRunner` that is used
* *command-line-arguments*: To configure the `ComposedTaskRunner`

For now, we focus on the configuration that is required to launch the `ComposedTaskRunner`:

* *graph*: this is the graph that is to be executed by the `ComposedTaskRunner`.
In this case it is `AAA&&BBB`.
* *increment-instance-enabled*: This lets each execution of `ComposedTaskRunner` be unique.
`ComposedTaskRunner` is built by using https://projects.spring.io/spring-batch/[Spring Batch].
Thus, we want a new Job Instance for each launch of the `ComposedTaskRunner`.
To do this, we set `increment-instance-enabled` to be `true`.
* *spring.datasource.**: The datasource that is used by Spring Cloud
Data Flow, which lets the user track the tasks launched by the
`ComposedTaskRunner` and the state of the job execution.
Also, this is so that the `ComposedTaskRunner` can track the state of the tasks it launched and update its state.

NOTE: Releases of `ComposedTaskRunner` can be found
https://github.com/spring-cloud-task-app-starters/composed-task-runner/releases[here].

[[sharing-spring-cloud-dataflows-datastore-with-tasks]]
== Sharing Spring Cloud Data Flow's Datastore with Tasks
As discussed in the <<spring-cloud-dataflow-task, Tasks>> documentation Spring
Cloud Data Flow allows a user to view Spring Cloud Task App executions. So in
this section we will discuss what is required by a Task Application and Spring
Cloud Data Flow to share the task execution information.

[a-common-datastore-dependency]
=== A Common DataStore Dependency
Spring Cloud Data Flow supports many database <<appendix-migration-guide.adoc#rdbms, types>> out-of-the-box,
so all the user typically has to do is declare the `spring_datasource_*` environment variables
to establish what data store Spring Cloud Data Flow will need.
So whatever database you decide to use for Spring Cloud Data Flow make sure that the your task also
includes that database dependency in its `pom.xml` or `gradle.build` file.  If the database dependency
that is used by Spring Cloud Data Flow is not present in the Task Application, the task will fail
and the task execution will not be recorded.

[a-common-datastore]
=== A Common Data Store
Spring Cloud Data Flow and your task application must access the same datastore instance.
This is so that the task executions recorded by the task application can be read by Spring Cloud Data Flow to list them in the Shell and Dashboard views.
Also the task app must have read  & write privileges to the task data tables that are used by Spring Cloud Data Flow.

Given the understanding of Datasource dependency between Task apps and Spring Cloud Data Flow, let's review how to apply them in various Task orchestration scenarios.

[datasource-simple-task-launch]
==== Simple Task Launch
When launching a task from Spring Cloud Data Flow, Data Flow adds its datasource
properties (`spring.datasource.url`, `spring.datasource.driverClassName`, `spring.datasource.username`, `spring.datasource.password`)
to the app properties of the task being launched.  Thus a task application
will record its task execution information to the Spring Cloud Data Flow repository.

[datasource-task-launcher-sink]
==== Task Launcher Sink
The https://github.com/spring-cloud-stream-app-starters/tasklauncher-local[Task Launcher Sink] allows tasks to be launched via a stream as discussed <<spring-cloud-dataflow-launch-tasks-from-stream, here>>.
Since tasks launched by the Task Launcher Sink may not want their task executions
recorded to the same datastore as Spring Cloud Data Flow,
each https://docs.spring.io/spring-cloud-task/docs/current/apidocs/org/springframework/cloud/task/launcher/TaskLaunchRequest.html[TaskLaunchRequest]
received by the Task Launcher Sink must have the required datasource information established as app properties or command line arguments.
Both https://github.com/spring-cloud-stream-app-starters/tasklaunchrequest-transform/blob/master/spring-cloud-starter-stream-processor-tasklaunchrequest-transform/README.adoc[TaskLaunchRequest-Transform]
and https://github.com/spring-cloud-stream-app-starters/triggertask/blob/master/spring-cloud-starter-stream-source-triggertask/README.adoc[TriggerTask Source] are examples
of how a source and a processor allow a user to set the datasource properties via the app properties or command line arguments.

==== Composed Task Runner
Spring Cloud Data Flow allows a user to create a directed graph where each node
of the graph is a task application and this is done via the
https://github.com/spring-cloud-task-app-starters/composed-task-runner/blob/master/spring-cloud-starter-task-composedtaskrunner/README.adoc[Composed Task Runner].
In this case the rules that applied to a <<datasource-simple-task-launch, Simple Task Launch>>
or <<datasource-task-launcher-sink, Task Launcher Sink>> apply to the composed task runner as well.
All child apps must also have access to the datastore that is being used by the composed task runner
Also, All child apps must have the same database dependency as the composed task runner enumerated in their `pom.xml` or `gradle.build` file.

==== Launching a task externally from Spring Cloud Data Flow
Users may wish to launch Spring Cloud Task applications via another method (scheduler for example) but still track the task execution via Spring Cloud Data Flow.
This can be done so long as the task applications observe the rules specified <<a-common-datastore-dependency, here>> and <<a-common-datastore, here>>.

NOTE: If a user wishes to use Spring Cloud Data Flow to view their
https://projects.spring.io/spring-batch/[Spring Batch] jobs, the user must make sure that
their batch application use the `@EnableTask` annotation and follow the rules enumerated <<a-common-datastore-dependency, here>> and <<a-common-datastore, here>>.
More information is available https://github.com/spring-projects/spring-batch-admin/blob/master/MIGRATION.md[here].

