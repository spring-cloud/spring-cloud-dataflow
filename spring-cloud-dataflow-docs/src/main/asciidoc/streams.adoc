[[streams]]
= Streams

[partintro]
--
In this section you will learn all about Streams and how to use them with Spring Cloud Data Flow.
--

[[spring-cloud-dataflow-stream-intro]]
== Introduction

In Spring Cloud Data Flow, a basic stream defines the ingestion of event data from a _source_ to a _sink_ that passes through any number of _processors_. Streams are composed of http://cloud.spring.io/spring-cloud-stream/[Spring Cloud Stream] applications and the deployment of stream definitions is done via the Data Flow Server (REST API). The xref:getting-started#getting-started[Getting Started] section shows you how to start the server and how to start and use the Spring Cloud Data Flow shell.

A high level DSL is used to create stream definitions. The DSL to define a stream that has an http source and a file sink (with no processors) is shown below

```
http | file
```
The DSL mimics UNIX pipes and filters syntax. Default values for ports and filenames are used in this example but can be overridden using `--` options, such as

```
http --server.port=8091 | file --directory=/tmp/httpdata/
```
To create these stream definitions you use the shell or make an HTTP POST request to the Spring Cloud Data Flow Server.  For more information on making HTTP request directly to the server, consult the <<api-guide, REST API Guide>>.

== Stream DSL

In the example above, we connected a source to a sink using the pipe symbol `|`. You can also pass properties to the source and sink configurations. The property names will depend on the individual app implementations, but as an example, the `http` source app exposes a `server.port` setting and it allows you to change the data ingestion port from the default value. To create the stream using port 8000, we would use
```
dataflow:> stream create --definition "http --server.port=8000 | log" --name myhttpstream
```
The shell provides tab completion for application properties and also the shell command `app info <appType>:<appName>` provides additional documentation for all the supported properties.

NOTE: Supported Stream <appType>'s are: source, processor, and sink

[[spring-cloud-dataflow-register-apps]]
== Register a Stream App

Register a Stream App with the App Registry using the Spring Cloud Data Flow Shell
`app register` command. You must provide a unique name, application type, and a URI that can be
resolved to the app artifact. For the type, specify "source", "processor", or "sink".
Here are a few examples:

```
dataflow:>app register --name mysource --type source --uri maven://com.example:mysource:0.0.1-SNAPSHOT

dataflow:>app register --name myprocessor --type processor --uri file:///Users/example/myprocessor-1.2.3.jar

dataflow:>app register --name mysink --type sink --uri http://example.com/mysink-2.0.1.jar
```

When providing a URI with the `maven` scheme, the format should conform to the following:

```
maven://<groupId>:<artifactId>[:<extension>[:<classifier>]]:<version>
```

For example, if you would like to register the snapshot versions of the `http` and `log`
applications built with the RabbitMQ binder, you could do the following:

```
dataflow:>app register --name http --type source --uri maven://org.springframework.cloud.stream.app:http-source-rabbit:1.1.2.BUILD-SNAPSHOT
dataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.1.2.BUILD-SNAPSHOT
```

If you would like to register multiple apps at one time, you can store them in a properties file
where the keys are formatted as `<type>.<name>` and the values are the URIs.

For example, if you would like to register the snapshot versions of the `http` and `log`
applications built with the RabbitMQ binder, you could have the following in a properties file [_eg: stream-apps.properties_]:

```
source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.1.2.BUILD-SNAPSHOT
sink.log=maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.1.2.BUILD-SNAPSHOT
```

Then to import the apps in bulk, use the `app import` command and provide the location of the properties file via `--uri`:

```
dataflow:>app import --uri file:///<YOUR_FILE_LOCATION>/stream-apps.properties
```

For convenience, we have the static files with application-URIs (for both maven and docker) available 
for all the out-of-the-box stream and task/batch app-starters. You can point to this file and import
all the application-URIs in bulk. Otherwise, as explained in previous paragraphs, you can register them individually or have your own custom property file with only the required application-URIs in it. It is recommended, however, to have a "focused" list of desired application-URIs in a custom property file.

List of available Stream Application Starters:

[width="100%",frame="topbot",options="header"]
|======================
|Artifact Type |Stable Release |SNAPSHOT Release
|RabbitMQ + Maven |link:http://bit.ly/Avogadro-GA-stream-applications-rabbit-maven[http://bit.ly/Avogadro-GA-stream-applications-rabbit-maven] |link:http://bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-rabbit-maven[http://bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-rabbit-maven]
|RabbitMQ + Docker|link:http://bit.ly/Avogadro-GA-stream-applications-rabbit-docker[http://bit.ly/Avogadro-GA-stream-applications-rabbit-docker] | N/A ]
|Kafka 0.9 + Maven 	  |link:http://bit.ly/Avogadro-GA-stream-applications-kafka-09-maven[http://bit.ly/Avogadro-GA-stream-applications-kafka-09-maven] |link:http://bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-kafka-09-maven[http://bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-kafka-09-maven]
|Kafka 0.9 + Docker   |link:http://bit.ly/Avogadro-GA-stream-applications-kafka-09-docker[http://bit.ly/Avogadro-GA-stream-applications-kafka-09-docker] | N/A ]
|Kafka 0.10 + Maven 	  |link:http://bit.ly/Avogadro-GA-stream-applications-kafka-10-maven[http://bit.ly/Avogadro-GA-stream-applications-kafka-10-maven] |link:http://bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-kafka-10-maven[http://bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-kafka-10-maven]
|Kafka 0.10 + Docker   |http://bit.ly/Avogadro-GA-stream-applications-kafka-10-docker[http://bit.ly/Avogadro-GA-stream-applications-kafka-10-docker] | N/A ]
|======================

List of available Task Application Starters:

[width="100%",frame="topbot",options="header"]
|======================
|Artifact Type |Stable Release |SNAPSHOT Release
|Maven   |http://bit.ly/Addison-GA-task-applications-maven[http://bit.ly/Addison-GA-task-applications-maven] |link:http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-maven[http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-maven]
|Docker  |link:http://bit.ly/Addison-GA-task-applications-docker[http://bit.ly/Addison-GA-task-applications-docker] | N/A ]
|======================

You can find more information about the available task starters look the http://cloud.spring.io/spring-cloud-task-app-starters/[Task App Starters Project Page] and
related reference documentation.  For more information about the available stream starters look at the http://cloud.spring.io/spring-cloud-stream-app-starters/[Stream App Starters Project Page]
and related reference documentation.

As an example, if you would like to register all out-of-the-box stream applications built with the RabbitMQ binder in bulk, you can with
the following command.

```
dataflow:>app import --uri http://bit.ly/Avogadro-GA-stream-applications-rabbit-maven
```

You can also pass the `--local` option (which is TRUE by default) to indicate whether the
properties file location should be resolved within the shell process itself. If the location should
be resolved from the Data Flow Server process, specify `--local false`.

When using either `app register` or `app import`, if a stream app is already registered with
the provided name and type, it will not be overridden by default. If you would like to override the
pre-existing stream app, then include the `--force` option.

[NOTE]
In some cases the Resource is resolved on the server side, whereas in others the
URI will be passed to a runtime container instance where it is resolved. Consult
the specific documentation of each Data Flow Server for more detail.

[[spring-cloud-dataflow-stream-app-whitelisting]]
=== Whitelisting application properties

Stream applications are Spring Boot applications which are aware of many <<spring-cloud-dataflow-global-properties>>, e.g. `server.port` but also families of properties such as those with the prefix `spring.jmx` and `logging`.  When creating your own application it is desirable to whitelist properties so that the shell and the UI can display them first as primary properties when presenting options via TAB completion or in drop-down boxes.

To whitelist application properties create a file named `spring-configuration-metadata-whitelist.properties` in the `META-INF` resource directory.  There are two property keys that can be used inside this file. The first key is named `configuration-properties.classes`.  The value is a comma separated list of fully qualified `@ConfigurationProperty` class names.  The second key is `configuration-properties.names` whose value is a comma separated list of property names.  This can contain the full name of property, such as `server.port` or a partial name to whitelist a category of property names, e.g. `spring.jmx`.

The link:https://github.com/spring-cloud-stream-app-starters[Spring Cloud Stream application starters] are a good place to look for examples of usage.  Here is a simple example of the file sink's `spring-configuration-metadata-whitelist.properties` file

```
configuration-properties.classes=org.springframework.cloud.stream.app.file.sink.FileSinkProperties
```

If we also wanted to add `server.port` to be white listed, then it would look like this:

```
configuration-properties.classes=org.springframework.cloud.stream.app.file.sink.FileSinkProperties
configuration-properties.names=server.port
```

[IMPORTANT]
====
Make sure to add 'spring-boot-configuration-processor' as an optional dependency to generate configuration metadata file for the properties.

[source,xml]
----
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-configuration-processor</artifactId>
    <optional>true</optional>
</dependency>
----
====

[[custom-applications]]
== Creating custom applications

While there are out of the box source, processor, sink applications available, one can extend these applications or write a custom link:https://github.com/spring-cloud/spring-cloud-stream[Spring Cloud Stream] application.

The process of creating Spring Cloud Stream applications via Spring Initializr is detailed in the Spring Cloud Stream {spring-cloud-stream-docs}#_getting_started[documentation].
It is possible to include multiple binders to an application.
If doing so, refer the instructions in <<passing_producer_consumer_properties>> on how to configure them.

For supporting property whitelisting, Spring Cloud Stream applications running in Spring Cloud Data Flow may include the Spring Boot `configuration-processor` as an optional dependency, as in the following example.

[source,xml]
----
<dependencies>
  <!-- other dependencies -->
  <dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-configuration-processor</artifactId>
    <optional>true</optional>
  </dependency>
</dependencies>

----

[NOTE]
====
Make sure that the `spring-boot-maven-plugin` is included in the POM. 
The plugin is necesary for creating the executable jar that will be registered with Spring Cloud Data Flow.
Spring Initialzr will include the plugin in the generated POM.
====

Once a custom application has been created, it can be registered as described in <<spring-cloud-dataflow-register-appss>>.


[[spring-cloud-dataflow-create-stream]]
== Creating a Stream

The Spring Cloud Data Flow Server exposes a full RESTful API for managing the lifecycle of stream definitions, but the easiest way to use is it is via the Spring Cloud Data Flow shell. Start the shell as described in the xref:Getting-Started#getting-started[Getting Started] section.

New streams are created by with the help of stream definitions. The definitions are built from a simple DSL. For example, let's walk through what happens if we execute the following shell command:

```
dataflow:> stream create --definition "time | log" --name ticktock
```
This defines a stream named `ticktock` based off the DSL expression `time | log`.  The DSL uses the "pipe" symbol `|`, to connect a source to a sink.

Then to deploy the stream execute the following shell command (or alternatively add the `--deploy` flag when creating the stream so that this step is not needed):

```
dataflow:> stream deploy --name ticktock
```
The Data Flow Server resolves `time` and `log` to maven coordinates and uses those to launch the `time` and `log` applications of the stream.

```
2016-06-01 09:41:21.728  INFO 79016 --- [nio-9393-exec-6] o.s.c.d.spi.local.LocalAppDeployer       : deploying app ticktock.log instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481708/ticktock.log
2016-06-01 09:41:21.914  INFO 79016 --- [nio-9393-exec-6] o.s.c.d.spi.local.LocalAppDeployer       : deploying app ticktock.time instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481910/ticktock.time
```

In this example, the time source simply sends the current time as a message each second, and the log sink outputs it using the logging framework.
You can tail the `stdout` log (which has an "_<instance>" suffix). The log files are located within the directory displayed in the Data Flow Server's log output, as shown above.

```
$ tail -f /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481708/ticktock.log/stdout_0.log
2016-06-01 09:45:11.250  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:11
2016-06-01 09:45:12.250  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:12
2016-06-01 09:45:13.251  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:13
```
=== Application properties

Application properties are the properties associated with each application in the stream. When the application is deployed, the application properties are applied to the application via
command line arguments or environment variables based on the underlying deployment implementation.

==== Passing application properties when creating a stream

The following stream

[source,bash]
----
dataflow:> stream create --definition "time | log" --name ticktock
----

can have application properties defined at the time of stream creation.

The shell command `app info <appType>:<appName>` displays the white-listed application properties for the application.
For more info on the property white listing refer to <<spring-cloud-dataflow-stream-app-whitelisting>>

Below are the white listed properties for the app `time`:

[source,bash]
----
dataflow:> app info source:time
╔══════════════════════════════╤══════════════════════════════╤══════════════════════════════╤══════════════════════════════╗
║         Option Name          │         Description          │           Default            │             Type             ║
╠══════════════════════════════╪══════════════════════════════╪══════════════════════════════╪══════════════════════════════╣
║trigger.time-unit             │The TimeUnit to apply to delay│<none>                        │java.util.concurrent.TimeUnit ║
║                              │values.                       │                              │                              ║
║trigger.fixed-delay           │Fixed delay for periodic      │1                             │java.lang.Integer             ║
║                              │triggers.                     │                              │                              ║
║trigger.cron                  │Cron expression value for the │<none>                        │java.lang.String              ║
║                              │Cron Trigger.                 │                              │                              ║
║trigger.initial-delay         │Initial delay for periodic    │0                             │java.lang.Integer             ║
║                              │triggers.                     │                              │                              ║
║trigger.max-messages          │Maximum messages per poll, -1 │1                             │java.lang.Long                ║
║                              │means infinity.               │                              │                              ║
║trigger.date-format           │Format for the date value.    │<none>                        │java.lang.String              ║
╚══════════════════════════════╧══════════════════════════════╧══════════════════════════════╧══════════════════════════════╝
----

Below are the white listed properties for the app `log`:

[source,bash]
----
dataflow:> app info sink:log
╔══════════════════════════════╤══════════════════════════════╤══════════════════════════════╤══════════════════════════════╗
║         Option Name          │         Description          │           Default            │             Type             ║
╠══════════════════════════════╪══════════════════════════════╪══════════════════════════════╪══════════════════════════════╣
║log.name                      │The name of the logger to use.│<none>                        │java.lang.String              ║
║log.level                     │The level at which to log     │<none>                        │org.springframework.integratio║
║                              │messages.                     │                              │n.handler.LoggingHandler$Level║
║log.expression                │A SpEL expression (against the│payload                       │java.lang.String              ║
║                              │incoming message) to evaluate │                              │                              ║
║                              │as the logged message.        │                              │                              ║
╚══════════════════════════════╧══════════════════════════════╧══════════════════════════════╧══════════════════════════════╝
----

The application properties for the `time` and `log` apps can be specified at the time of `stream` creation as follows:

[source,bash]
----
dataflow:> stream create --definition "time --fixed-delay=5 | log --level=WARN" --name ticktock
----

Note that the properties `fixed-delay` and `level` defined above for the apps `time` and `log` are the 'short-form' property names provided by the shell completion.
These 'short-form' property names are applicable only for the white-listed properties and in all other cases, only _fully qualified_ property names should be used.

=== Deployment properties

When deploying the stream, properties that control the deployment of the apps into the target platform are known as `deployment` properties.
For instance, one can specify how many instances need to be deployed for the specific application defined in the stream using the deployment property called `count`.

==== Passing instance count as deployment property

If you would like to have multiple instances of an application in the stream, you
can include a property with the deploy command:

[source,bash,subs=attributes]
----
dataflow:> stream deploy --name ticktock --properties "app.time.count=3"
----

Note that `count` is the *reserved* property name used by the underlying deployer. Hence, if the application also has a custom property named `count`, it is *not* supported
 when specified in 'short-form' form during stream _deployment_ as it could conflict with the _instance_ count deployer property. Instead, the `count` as a custom application property can be
 specified in its _fully qualified_ form (example: `app.foo.bar.count`) during stream _deployment_ or it can be specified using 'short-form' or _fully qualified_ form during the stream _creation_
 where it will be considered as an app property.

IMPORTANT: See <<spring-cloud-dataflow-stream-app-labels>>.

==== Inline vs file reference properties

When using the Spring Cloud Data Flow Shell, there are two ways to provide deployment
properties: either *inline* or via a *file reference*. Those two ways are exclusive
and documented below:

*Inline properties*::

  use the `--properties` shell option and list properties as a comma separated
  list of key=value pairs, like so:

[source,bash]
----
stream deploy foo
    --properties "app.transform.count=2,app.transform.producer.partitionKeyExpression=payload"
----

*Using a file reference*::

  use the `--propertiesFile` option and point it to a local Java `.properties` file or 
  `.yaml` or `.yml` file.  The file should be on the file system of the machine running the 
  shell.  If using a `.properties` file, normal rules apply (ISO 8859-1 encoding, `=`, `<space>` or
  `:` delimiter, etc.) although we recommend using `=` as a key-value pair delimiter
  for consistency.

[source,bash]
----
stream deploy foo --propertiesFile myprops.properties
----

where `myprops.properties` contains:

```
app.transform.count=2
app.transform.producer.partitionKeyExpression=payload
```

Both the above properties will be passed as deployment properties for the stream `foo` above.

==== Passing application properties when deploying a stream

The application properties can also be specified when deploying a stream. When specified during deployment, these application properties can either be specified as
 'short-form' property names (applicable for white-listed properties) or _fully qualified_ property names. The application properties should have the prefix "app.<appName/label>".

For example, the stream

[source,bash]
----
dataflow:> stream create --definition "time | log" --name ticktock
----

can be deployed with application properties using the 'short-form' property names:

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.fixed-delay=5,app.log.level=ERROR"
----

When using the app label,

[source,bash]
----
stream create ticktock --definition "a: time | b: log"
----

the application properties can be defined as:

[source,bash]
----
stream deploy ticktock --properties "app.a.fixed-delay=4,app.b.level=ERROR"
----

[[passing_producer_consumer_properties]]
==== Passing Spring Cloud Stream properties for the application
Spring Cloud Data Flow sets the `required` Spring Cloud Stream properties for the applications inside the stream. Most importantly, the `spring.cloud.stream.bindings.<input/output>.destination` is set internally for the apps to bind.

If someone wants to override any of the Spring Cloud Stream properties, they can be set via deployment properties.

For example, for the below stream

[source,bash]
----
dataflow:> stream create --definition "http | transform --expression=payload.getValue('hello').toUpperCase() | log" --name ticktock
----

if there are multiple binders available in the classpath for each of the applications and the binder is chosen for each deployment then the stream can be deployed with the specific Spring Cloud Stream properties as:

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.spring.cloud.stream.bindings.output.binder=kafka,app.transform.spring.cloud.stream.bindings.input.binder=kafka,app.transform.spring.cloud.stream.bindings.output.binder=rabbit,app.log.spring.cloud.stream.bindings.input.binder=rabbit"
----

NOTE: Overriding the destination names is not recommended as Spring Cloud Data Flow takes care of setting this internally.

==== Passing per-binding producer consumer properties
A Spring Cloud Stream application can have producer and consumer properties set `per-binding` basis.
While Spring Cloud Data Flow supports specifying short-hand notation for per binding producer properties such as `partitionKeyExpression`, `partitionKeyExtractorClass` as described in <<passing_stream_partition_properties>>, all the supported Spring Cloud Stream producer/consumer properties can be set as Spring Cloud Stream properties for the app directly as well.

The consumer properties can be set for the `inbound` channel name with the prefix `app.[app/label name].spring.cloud.stream.bindings.<channelName>.consumer.` and the producer properties can be set for the `outbound` channel name with the prefix `app.[app/label name].spring.cloud.stream.bindings.<channelName>.producer.`.
For example, the stream

[source,bash]
----
dataflow:> stream create --definition "time | log" --name ticktock
----

can be deployed with producer/consumer properties as:

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.spring.cloud.stream.bindings.output.producer.requiredGroups=myGroup,app.time.spring.cloud.stream.bindings.output.producer.headerMode=raw,app.log.spring.cloud.stream.bindings.input.consumer.concurrency=3,app.log.spring.cloud.stream.bindings.input.consumer.maxAttempts=5"
----

The `binder` specific producer/consumer properties can also be specified in a similar way.

For instance

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.spring.cloud.stream.rabbit.bindings.output.producer.autoBindDlq=true,app.log.spring.cloud.stream.rabbit.bindings.input.consumer.transacted=true"
----

[[passing_stream_partition_properties]]
==== Passing stream partition properties during stream deployment
A common pattern in stream processing is to partition the data as it is streamed.
This entails deploying multiple instances of a message consuming app and using
content-based routing so that messages with a given key (as determined at runtime)
are always routed to the same app instance. You can pass the partition properties during
stream deployment to declaratively configure a partitioning strategy to route each
message to a specific consumer instance.

See below for examples of deploying partitioned streams:

*app.[app/label name].producer.partitionKeyExtractorClass*::
  The class name of a PartitionKeyExtractorStrategy (default `null`)

*app.[app/label name].producer.partitionKeyExpression*::
  A SpEL expression, evaluated against the message, to determine the partition key;
  only applies if `partitionKeyExtractorClass` is null. If both are null, the app
  is not partitioned (default `null`)

*app.[app/label name].producer.partitionSelectorClass*::
  The class name of a PartitionSelectorStrategy (default `null`)

*app.[app/label name].producer.partitionSelectorExpression*::
  A SpEL expression, evaluated against the partition key, to determine the partition
  index to which the message will be routed. The final partition index will be the
  return value (an integer) modulo `[nextModule].count`. If both the class and
  expression are null, the underlying binder's default PartitionSelectorStrategy
  will be applied to the key (default `null`)

In summary, an app is partitioned if its count is > 1 and the previous app has a
`partitionKeyExtractorClass` or `partitionKeyExpression` (class takes precedence).
When a partition key is extracted, the partitioned app instance is determined by
invoking the `partitionSelectorClass`, if present, or the `partitionSelectorExpression % partitionCount`,
where `partitionCount` is application count in the case of RabbitMQ, and the underlying
partition count of the topic in the case of Kafka.

If neither a `partitionSelectorClass` nor a `partitionSelectorExpression` is
present the result is `key.hashCode() % partitionCount`.

[[passing_content_type_properties]]
==== Passing application content type properties
In a stream definition you can specify that the input or the output of an application need to be converted to a different type.
You can use the `inputType` and `outputType` properties to specify the content type for the incoming data and outgoing data, respectively.

For example, consider the following stream:

```
dataflow:>stream create tuple --definition "http | filter --inputType=application/x-spring-tuple
 --expression=payload.hasFieldName('hello') | transform --expression=payload.getValue('hello').toUpperCase()
 | log" --deploy
```

The `http` app is expected to send the data in JSON and the `filter` app receives the JSON data
and processes it as a Spring Tuple.
In order to do so, we use the `inputType` property on the filter app to convert the data into the expected Spring Tuple format.
The `transform` application processes the Tuple data and sends the processed data to the downstream `log` application.

When sending some data to the `http` application:

```
dataflow:>http post --data {"hello":"world","foo":"bar"} --contentType application/json --target http://localhost:<http-port>
```

At the log application you see the content as follows:

```
INFO 18745 --- [transform.tuple-1] log.sink                                 : WORLD
```

Depending on how applications are chained, the content type conversion can be specified either as via the `--outputType` in the upstream app or as an `--inputType` in the downstream app.
For instance, in the above stream, instead of specifying the `--inputType` on the 'transform' application to convert, the option `--outputType=application/x-spring-tuple` can also be specified on the 'http' application.

For the complete list of message conversion and message converters, please refer to Spring Cloud Stream {spring-cloud-stream-docs}#contenttypemanagement[documentation].

==== Overriding application properties during stream deployment

Application properties that are defined during deployment override the same properties defined during the stream creation.

For example, the following stream has application properties defined during stream creation:

[source,bash]
----
dataflow:> stream create --definition "time --fixed-delay=5 | log --level=WARN" --name ticktock
----

To override these application properties, one can specify the new property values during deployment:

[source,bash]
----
dataflow:>stream deploy ticktock --properties "app.time.fixed-delay=4,app.log.level=ERROR"
----

=== Deployment properties

When deploying the stream, properties that control the deployment of the apps into the target platform are known as `deployment` properties.
For instance, one can specify how many instances need to be deployed for the specific application defined in the stream using the deployment property called `count`.

==== Passing instance count as deployment property

If you would like to have multiple instances of an application in the stream, you
can include a property with the deploy command:

[source,bash,subs=attributes]
----
dataflow:> stream deploy --name ticktock --properties "app.time.count=3"
----

Note that `count` is the *reserved* property name used by the underlying deployer. Hence, if the application also has a custom property named `count`, it is *not* supported
 when specified in 'short-form' form during stream _deployment_ as it could conflict with the _instance_ count deployer property. Instead, the `count` as a custom application property can be
 specified in its _fully qualified_ form (example: `app.foo.bar.count`) during stream _deployment_ or it can be specified using 'short-form' or _fully qualified_ form during the stream _creation_
 where it will be considered as an app property.

IMPORTANT: See <<spring-cloud-dataflow-stream-app-labels>>.

==== Inline vs file reference properties

When using the Spring Cloud Data Flow Shell, there are two ways to provide deployment
properties: either *inline* or via a *file reference*. Those two ways are exclusive
and documented below:

*Inline properties*::

  use the `--properties` shell option and list properties as a comma separated
  list of key=value pairs, like so:

[source,bash]
----
stream deploy foo
    --properties "app.transform.count=2,app.transform.producer.partitionKeyExpression=payload"
----

*Using a file reference*::

  use the `--propertiesFile` option and point it to a local `.properties`, `.yaml` or `.yml` file
  (i.e. that lives in the filesystem of the machine running the shell). Being read
  as a `.properties` file, normal rules apply (ISO 8859-1 encoding, `=`, `<space>` or
  `:` delimiter, etc.) although we recommend using `=` as a key-value pair delimiter
  for consistency:

[source,bash]
----
stream deploy foo --propertiesFile myprops.properties
----

where `myprops.properties` contains:

```
app.transform.count=2
app.transform.producer.partitionKeyExpression=payload
```
Both the above properties will be passed as deployment properties for the stream `foo` above.

In case of using YAML as the format for the deployment properties, use the `.yaml` or `.yml` file extention when deploying the stream,

[source,bash]
----
stream deploy foo --propertiesFile myprops.yaml
----

where `myprops.yaml` contains:

```
app:
  transform:
    count: 2
    producer:
      partitionKeyExpression: payload
```

[[spring-cloud-dataflow-destroy-stream]]
== Destroying a Stream

You can delete a stream by issuing the `stream destroy` command from the shell:

```
dataflow:> stream destroy --name ticktock
```

If the stream was deployed, it will be undeployed before the stream definition is deleted.

[[spring-cloud-dataflow-deploy-undeploy-stream]]
== Deploying and Undeploying Streams

Often you will want to stop a stream, but retain the name and definition for future use. In that case you can `undeploy` the stream by name and issue the `deploy` command at a later time to restart it.
```
dataflow:> stream undeploy --name ticktock
dataflow:> stream deploy --name ticktock
```

[[spring-cloud-dataflow-stream-app-types]]
== Other Source and Sink Application Types

Let's try something a bit more complicated and swap out the `time` source for something else. Another supported source type is `http`, which accepts data for ingestion over HTTP POSTs. Note that the `http` source accepts data on a different port from the Data Flow Server (default 8080). By default the port is randomly assigned.

To create a stream using an `http` source, but still using the same `log` sink, we would change the original command above to

```
dataflow:> stream create --definition "http | log" --name myhttpstream --deploy
```
which will produce the following output from the server

```
2016-06-01 09:47:58.920  INFO 79016 --- [io-9393-exec-10] o.s.c.d.spi.local.LocalAppDeployer       : deploying app myhttpstream.log instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/myhttpstream-1464788878747/myhttpstream.log
2016-06-01 09:48:06.396  INFO 79016 --- [io-9393-exec-10] o.s.c.d.spi.local.LocalAppDeployer       : deploying app myhttpstream.http instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/myhttpstream-1464788886383/myhttpstream.http
```

Note that we don't see any other output this time until we actually post some data (using a shell command). In order to see the randomly assigned port on which the http source is listening, execute:

```
dataflow:> runtime apps
```
You should see that the corresponding http source has a `url` property containing the host and port information on which it is listening. You are now ready to post to that url, e.g.:
```
dataflow:> http post --target http://localhost:1234 --data "hello"
dataflow:> http post --target http://localhost:1234 --data "goodbye"
```
and the stream will then funnel the data from the http source to the output log implemented by the log sink

```
2016-06-01 09:50:22.121  INFO 79654 --- [  kafka-binder-] log.sink    : hello
2016-06-01 09:50:26.810  INFO 79654 --- [  kafka-binder-] log.sink    : goodbye
```

Of course, we could also change the sink implementation. You could pipe the output to a file (`file`), to hadoop (`hdfs`) or to any of the other sink apps which are available. You can also define your own apps.

[[spring-cloud-dataflow-simple-stream]]
== Simple Stream Processing

As an example of a simple processing step, we can transform the payload of the HTTP posted data to upper case using the stream definitions
```
http | transform --expression=payload.toUpperCase() | log
```
To create this stream enter the following command in the shell
```
dataflow:> stream create --definition "http | transform --expression=payload.toUpperCase() | log" --name mystream --deploy
```
Posting some data (using a shell command)
```
dataflow:> http post --target http://localhost:1234 --data "hello"
```
Will result in an uppercased 'HELLO' in the log

```
2016-06-01 09:54:37.749  INFO 80083 --- [  kafka-binder-] log.sink    : HELLO
```

[[spring-cloud-dataflow-stream-partitions]]
== Stateful Stream Processing

To demonstrate the data partitioning functionality, let's deploy the following stream with Kafka as the binder.

```
dataflow:>stream create --name words --definition "http --server.port=9900 | splitter --expression=payload.split(' ') | log"
Created new stream 'words'

dataflow:>stream deploy words --properties "app.splitter.producer.partitionKeyExpression=payload,app.log.count=2"
Deployed stream 'words'

dataflow:>http post --target http://localhost:9900 --data "How much wood would a woodchuck chuck if a woodchuck could chuck wood"
> POST (text/plain;Charset=UTF-8) http://localhost:9900 How much wood would a woodchuck chuck if a woodchuck could chuck wood
> 202 ACCEPTED
```

You'll see the following in the server logs.

```
2016-06-05 18:33:24.982  INFO 58039 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalAppDeployer       : deploying app words.log instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-694182453710731989/words-1465176804970/words.log
2016-06-05 18:33:24.988  INFO 58039 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalAppDeployer       : deploying app words.log instance 1
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-694182453710731989/words-1465176804970/words.log
```

Review the `words.log instance 0` logs:

```
2016-06-05 18:35:47.047  INFO 58638 --- [  kafka-binder-] log.sink                                 : How
2016-06-05 18:35:47.066  INFO 58638 --- [  kafka-binder-] log.sink                                 : chuck
2016-06-05 18:35:47.066  INFO 58638 --- [  kafka-binder-] log.sink                                 : chuck
```

Review the `words.log instance 1` logs:

```
2016-06-05 18:35:47.047  INFO 58639 --- [  kafka-binder-] log.sink                                 : much
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : wood
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : would
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : a
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : woodchuck
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : if
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : a
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : woodchuck
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : could
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : wood
```

This shows that payload splits that contain the same word are routed to the same application instance.

[[spring-cloud-dataflow-stream-tap-dsl]]
== Tap a Stream

Taps can be created at various producer endpoints in a stream. For a stream like this:

```
stream create --definition "http | step1: transform --expression=payload.toUpperCase() | step2: transform --expression=payload+'!' | log" --name mainstream --deploy

```
taps can be created at the output of `http`, `step1` and `step2`.

To create a stream that acts as a 'tap' on another stream requires to specify the `source destination name` for the tap stream. The syntax for source destination name is:

```
`:<streamName>.<label/appName>`
```
To create a tap at the output of `http` in the stream above, the source destination name is `mainstream.http`
To create a tap at the output of the first transform app in the stream above, the source destination name is `mainstream.step1`

The tap stream DSL looks like this:

```
stream create --definition ":mainstream.http > counter" --name tap_at_http --deploy

stream create --definition ":mainstream.step1 > jdbc" --name tap_at_step1_transformer --deploy
```

Note the colon (:) prefix before the destination names. The colon allows the parser to recognize this as a destination name instead of an app name.

[[spring-cloud-dataflow-stream-app-labels]]
== Using Labels in a Stream

When a stream is comprised of multiple apps with the same name, they must be qualified with labels:
```
stream create --definition "http | firstLabel: transform --expression=payload.toUpperCase() | secondLabel: transform --expression=payload+'!' | log" --name myStreamWithLabels --deploy
```

[[spring-cloud-dataflow-stream-explicit-destination-names]]
== Explicit Broker Destinations in a Stream

One can connect to a specific destination name located in the broker (Rabbit, Kafka etc.,) either at the `source` or at the `sink` position.

The following stream has the destination name at the `source` position:

```
stream create --definition ":myDestination > log" --name ingest_from_broker --deploy
```

This stream receives messages from the destination `myDestination` located at the broker and connects it to the `log` app.


The following stream has the destination name at the `sink` position:

```
stream create --definition "http > :myDestination" --name ingest_to_broker --deploy
```
This stream sends the messages from the `http` app to the destination `myDestination` located at the broker.

From the above streams, notice that the `http` and `log` apps are interacting with each other via the broker (through the destination `myDestination`) rather than having a pipe directly between `http` and `log` within a single stream.

It is also possible to connect two different destinations (`source` and `sink` positions) at the broker in a stream.

```
stream create --definition ":destination1 > :destination2" --name bridge_destinations --deploy
```

In the above stream, both the destinations (`destination1` and `destination2`) are located in the broker. The messages flow from the source destination to the sink destination via a `bridge` app that connects them.

[[spring-cloud-dataflow-stream-advanced]]
== Directed Graphs in a Stream

If directed graphs are needed instead of the simple linear streams described above, two features are relevant.

First, named destinations may be used as a way to combine the output from multiple streams or for multiple consumers to share the output from a single stream.
This can be done using the DSL syntax `http > :mydestination` or `:mydestination > log`.

Second, you may need to determine the output channel of a stream based on some information that is only known at runtime.
In that case, a router may be used in the sink position of a stream definition. For more information, refer to the Router Sink starter's
link:https://github.com/spring-cloud-stream-app-starters/router/tree/master/spring-cloud-starter-stream-sink-router[README].

[[spring-cloud-dataflow-global-properties]]
=== Common application properties

In addition to configuration via DSL, Spring Cloud Data Flow provides a mechanism for setting common properties to all the streaming applications that are launched by it.
This can be done by adding properties prefixed with `spring.cloud.dataflow.applicationProperties.stream` when starting the server.
When doing so, the server will pass all the properties, without the prefix, to the instances it launches.

For example, all the launched applications can be configured to use a specific Kafka broker by launching the configuration server with the following options:

```
--spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.kafka.binder.brokers=192.168.1.100:9092
--spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.kafka.binder.zkNodes=192.168.1.100:2181
```

This will cause the properties `spring.cloud.stream.kafka.binder.brokers` and `spring.cloud.stream.kafka.binder.zkNodes` to be passed to all the launched applications.

[NOTE]
Properties configured using this mechanism have lower precedence than stream deployment properties.
They will be overridden if a property with the same key is specified at stream deployment time (e.g. `app.http.spring.cloud.stream.kafka.binder.brokers` will override the common property).

[[spring-cloud-dataflow-stream-multi-binder]]
== Stream applications with multiple binder configurations

 In some cases, a stream can have its applications bound to multiple spring cloud stream binders when they are required to connect to different messaging
middleware configurations. In those cases, it is important to make sure the applications are configured appropriately with their binder
configurations. For example, let's consider the following stream:

```
http | transform --expression=payload.toUpperCase() | log
```

and in this stream, each application connects to messaging middleware in the following way:

```
Http source sends events to RabbitMQ (rabbit1)
Transform processor receives events from RabbitMQ (rabbit1) and sends the processed events into Kafka (kafka1)
Log sink receives events from Kafka (kafka1)
```
Here, `rabbit1` and `kafka1` are the binder names given in the spring cloud stream application properties.
Based on this setup, the applications will have the following binder(s) in their classpath with the appropriate configuration:

```
Http - Rabbit binder
Transform - Both Kafka and Rabbit binders
Log - Kafka binder
```
The spring-cloud-stream `binder` configuration properties can be set within the applications themselves.
If not, they can be passed via `deployment` properties when the stream is deployed.

For example,

```
dataflow:>stream create --definition "http | transform --expression=payload.toUpperCase() | log" --name mystream
```

```
dataflow:>stream deploy mystream --properties "app.http.spring.cloud.stream.bindings.output.binder=rabbit1,app.transform.spring.cloud.stream.bindings.input.binder=rabbit1,
app.transform.spring.cloud.stream.bindings.output.binder=kafka1,app.log.spring.cloud.stream.bindings.input.binder=kafka1"
```

One can override any of the binder configuration properties by specifying them via deployment properties.
